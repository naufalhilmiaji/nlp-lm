text
oh
how
the
headlines
blared:
chatbots
were
the
next
big
thing
our
hopes
were
sky
high
bright-eyed
and
bushy-tailed
the
industry
was
ripe
for
a
new
era
of
innovation:
it
was
time
to
start
socializing
with
machines
and
why
wouldn’t
they
be?
all
the
road
signs
pointed
towards
insane
success
at
the
mobile
world
congress
2017
chatbots
were
the
main
headliners
the
conference
organizers
cited
an
‘overwhelming
acceptance
at
the
event
of
the
inevitable
shift
of
focus
for
brands
and
corporates
to
chatbots’
in
fact
the
only
significant
question
around
chatbots
was
who
would
monopolize
the
field
not
whether
chatbots
would
take
off
in
the
first
place:
one
year
on
we
have
an
answer
to
that
question
no
because
there
isn’t
even
an
ecosystem
for
a
platform
to
dominate
chatbots
weren’t
the
first
technological
development
to
be
talked
up
in
grandiose
terms
and
then
slump
spectacularly
the
age-old
hype
cycle
unfolded
in
familiar
fashion
expectations
built
built
and
then
it
all
kind
of
fizzled
out
the
predicted
paradim
shift
didn’t
materialize
and
apps
are
tellingly
still
alive
and
well
we
look
back
at
our
breathless
optimism
and
turn
to
each
other
slightly
baffled:
is
that
it?
that
was
the
chatbot
revolution
we
were
promised?
digit’s
ethan
bloch
sums
up
the
general
consensus:
according
to
dave
feldman
vice
president
of
product
design
at
heap
chatbots
didn’t
just
take
on
one
difficult
problem
and
fail:
they
took
on
several
and
failed
all
of
them
bots
can
interface
with
users
in
different
ways
the
big
divide
is
text
vs
speech
in
the
beginning
of
computer
interfaces
was
the
written
word
users
had
to
type
commands
manually
into
a
machine
to
get
anything
done
then
graphical
user
interfaces
guis
came
along
and
saved
the
day
we
became
entranced
by
windows
mouse
clicks
icons
and
hey
we
eventually
got
color
too!
meanwhile
a
bunch
of
research
scientists
were
busily
developing
natural
language
nl
interfaces
to
databases
instead
of
having
to
learn
an
arcane
database
query
language
another
bunch
of
scientists
were
developing
speech-processing
software
so
that
you
could
just
speak
to
your
computer
rather
than
having
to
type
this
turned
out
to
be
a
whole
lot
more
difficult
than
anyone
originally
realised:
the
next
item
on
the
agenda
was
holding
a
two-way
dialog
with
a
machine
here’s
an
example
dialog
dating
back
to
the
1990s
with
vcr
setup
system:
pretty
cool
right?
the
system
takes
turns
in
collaborative
way
and
does
a
smart
job
of
figuring
out
what
the
user
wants
it
was
carefully
crafted
to
deal
with
conversations
involving
vcrs
and
could
only
operate
within
strict
limitations
modern
day
bots
whether
they
use
typed
or
spoken
input
have
to
face
all
these
challenges
but
also
work
in
an
efficient
and
scalable
way
on
a
variety
of
platforms
basically
we’re
still
trying
to
achieve
the
same
innovations
we
were
30
years
ago
here’s
where
i
think
we’re
going
wrong:
an
oversized
assumption
has
been
that
apps
are
‘over’
and
would
be
replaced
by
bots
by
pitting
two
such
disparate
concepts
against
one
another
instead
of
seeing
them
as
separate
entities
designed
to
serve
different
purposes
we
discouraged
bot
development
you
might
remember
a
similar
war
cry
when
apps
first
came
onto
the
scene
ten
years
ago:
but
do
you
remember
when
apps
replaced
the
internet?
it’s
said
that
a
new
product
or
service
needs
to
be
two
of
the
following:
better
cheaper
or
faster
are
chatbots
cheaper
or
faster
than
apps?
no
—
not
yet
at
least
whether
they’re
‘better’
is
subjective
but
i
think
it’s
fair
to
say
that
today’s
best
bot
isn’t
comparable
to
today’s
best
app
plus
nobody
thinks
that
using
lyft
is
too
complicated
or
that
it’s
too
hard
to
order
food
or
buy
a
dress
on
an
app
what
is
too
complicated
is
trying
to
complete
these
tasks
with
a
bot
—
and
having
the
bot
fail
a
great
bot
can
be
about
as
useful
as
an
average
app
when
it
comes
to
rich
sophisticated
multi-layered
apps
there’s
no
competition
that’s
because
machines
let
us
access
vast
and
complex
information
systems
and
the
early
graphical
information
systems
were
a
revolutionary
leap
forward
in
helping
us
locate
those
systems
modern-day
apps
benefit
from
decades
of
research
and
experimentation
why
would
we
throw
this
away?
but
if
we
swap
the
word
‘replace’
with
‘extend’
things
get
much
more
interesting
today’s
most
successful
bot
experiences
take
a
hybrid
approach
incorporating
chat
into
a
broader
strategy
that
encompasses
more
traditional
elements
the
next
wave
will
be
multimodal
apps
where
you
can
say
what
you
want
like
with
siri
and
get
back
information
as
a
map
text
or
even
a
spoken
response
another
problematic
aspect
of
the
sweeping
nature
of
hype
is
that
it
tends
to
bypass
essential
questions
like
these
for
plenty
of
companies
bots
just
aren’t
the
right
solution
the
past
two
years
are
littered
with
cases
of
bots
being
blindly
applied
to
problems
where
they
aren’t
needed
building
a
bot
for
the
sake
of
it
letting
it
loose
and
hoping
for
the
best
will
never
end
well:
the
vast
majority
of
bots
are
built
using
decision-tree
logic
where
the
bot’s
canned
response
relies
on
spotting
specific
keywords
in
the
user
input
the
advantage
of
this
approach
is
that
it’s
pretty
easy
to
list
all
the
cases
that
they
are
designed
to
cover
and
that’s
precisely
their
disadvantage
too
that’s
because
these
bots
are
purely
a
reflection
of
the
capability
fastidiousness
and
patience
of
the
person
who
created
them
and
how
many
user
needs
and
inputs
they
were
able
to
anticipate
problems
arise
when
life
refuses
to
fit
into
those
boxes
according
to
recent
reports
70%
of
the
100000
bots
on
facebook
messenger
are
failing
to
fulfil
simple
user
requests
this
is
partly
a
result
of
developers
failing
to
narrow
their
bot
down
to
one
strong
area
of
focus
when
we
were
building
growthbot
we
decided
to
make
it
specific
to
sales
and
marketers:
not
an
‘all-rounder’
despite
the
temptation
to
get
overexcited
about
potential
capabilties
remember:
a
bot
that
does
one
thing
well
is
infinitely
more
helpful
than
a
bot
that
does
multiple
things
poorly
a
competent
developer
can
build
a
basic
bot
in
minutes
—
but
one
that
can
hold
a
conversation?
that’s
another
story
despite
the
constant
hype
around
ai
we’re
still
a
long
way
from
achieving
anything
remotely
human-like
in
an
ideal
world
the
technology
known
as
nlp
natural
language
processing
should
allow
a
chatbot
to
understand
the
messages
it
receives
but
nlp
is
only
just
emerging
from
research
labs
and
is
very
much
in
its
infancy
some
platforms
provide
a
bit
of
nlp
but
even
the
best
is
at
toddler-level
capacity
for
example
think
about
siri
understanding
your
words
but
not
their
meaning
as
matt
asay
outlines
this
results
in
another
issue:
failure
to
capture
the
attention
and
creativity
of
developers
and
conversations
are
complex
they’re
not
linear
topics
spin
around
each
other
take
random
turns
restart
or
abruptly
finish
today’s
rule-based
dialogue
systems
are
too
brittle
to
deal
with
this
kind
of
unpredictability
and
statistical
approaches
using
machine
learning
are
just
as
limited
the
level
of
ai
required
for
human-like
conversation
just
isn’t
available
yet
and
in
the
meantime
there
are
few
high-quality
examples
of
trailblazing
bots
to
lead
the
way
as
dave
feldman
remarked:
once
upon
a
time
the
only
way
to
interact
with
computers
was
by
typing
arcane
commands
to
the
terminal
visual
interfaces
using
windows
icons
or
a
mouse
were
a
revolution
in
how
we
manipulate
information
there’s
a
reasons
computing
moved
from
text-based
to
graphical
user
interfaces
guis
on
the
input
side
it’s
easier
and
faster
to
click
than
it
is
to
type
tapping
or
selecting
is
obviously
preferable
to
typing
out
a
whole
sentence
even
with
predictive
often
error-prone
""
text
on
the
output
side
the
old
adage
that
a
picture
is
worth
a
thousand
words
is
usually
true
we
love
optical
displays
of
information
because
we
are
highly
visual
creatures
it’s
no
accident
that
kids
love
touch
screens
the
pioneers
who
dreamt
up
graphical
interface
were
inspired
by
cognitive
psychology
the
study
of
how
the
brain
deals
with
communication
conversational
uis
are
meant
to
replicate
the
way
humans
prefer
to
communicate
but
they
end
up
requiring
extra
cognitive
effort
essentially
we’re
swapping
something
simple
for
a
more-complex
alternative
sure
there
are
some
concepts
that
we
can
only
express
using
language
show
me
all
the
ways
of
getting
to
a
museum
that
give
me
2000
steps
but
don’t
take
longer
than
35
minutes
but
most
tasks
can
be
carried
out
more
efficiently
and
intuitively
with
guis
than
with
a
conversational
ui
aiming
for
a
human
dimension
in
business
interactions
makes
sense
if
there’s
one
thing
that’s
broken
about
sales
and
marketing
it’s
the
lack
of
humanity:
brands
hide
behind
ticket
numbers
feedback
forms
do-not-reply-emails
automated
responses
and
gated
‘contact
us’
forms
facebook’s
goal
is
that
their
bots
should
pass
the
so-called
turing
test
meaning
you
can’t
tell
whether
you
are
talking
to
a
bot
or
a
human
but
a
bot
isn’t
the
same
as
a
human
it
never
will
be
a
conversation
encompasses
so
much
more
than
just
text
humans
can
read
between
the
lines
leverage
contextual
information
and
understand
double
layers
like
sarcasm
bots
quickly
forget
what
they’re
talking
about
meaning
it’s
a
bit
like
conversing
with
someone
who
has
little
or
no
short-term
memory
as
hubspot
team
pinpointed:
people
aren’t
easily
fooled
and
pretending
a
bot
is
a
human
is
guaranteed
to
diminish
returns
not
to
mention
the
fact
that
you’re
lying
to
your
users
and
even
those
rare
bots
that
are
powered
by
state-of-the-art
nlp
and
excel
at
processing
and
producing
content
will
fall
short
in
comparison
and
here’s
the
other
thing
conversational
uis
are
built
to
replicate
the
way
humans
prefer
to
communicate
—
with
other
humans
but
is
that
how
humans
prefer
to
interact
with
machines?
not
necessarily
at
the
end
of
the
day
no
amount
of
witty
quips
or
human-like
mannerisms
will
save
a
bot
from
conversational
failure
in
a
way
those
early-adopters
weren’t
entirely
wrong
people
are
yelling
at
google
home
to
play
their
favorite
song
ordering
pizza
from
the
domino’s
bot
and
getting
makeup
tips
from
sephora
but
in
terms
of
consumer
response
and
developer
involvement
chatbots
haven’t
lived
up
to
the
hype
generated
circa
201516
not
even
close
computers
are
good
at
being
computers
searching
for
data
crunching
numbers
analyzing
opinions
and
condensing
that
information
computers
aren’t
good
at
understanding
human
emotion
the
state
of
nlp
means
they
still
don’t
‘get’
what
we’re
asking
them
never
mind
how
we
feel
that’s
why
it’s
still
impossible
to
imagine
effective
customer
support
sales
or
marketing
without
the
essential
human
touch:
empathy
and
emotional
intelligence
for
now
bots
can
continue
to
help
us
with
automated
repetitive
low-level
tasks
and
queries
as
cogs
in
a
larger
more
complex
system
and
we
did
them
and
ourselves
a
disservice
by
expecting
so
much
so
soon
but
that’s
not
the
whole
story
yes
our
industry
massively
overestimated
the
initial
impact
chatbots
would
have
emphasis
on
initial
as
bill
gates
once
said:
the
hype
is
over
and
that’s
a
good
thing
now
we
can
start
examining
the
middle-grounded
grey
area
instead
of
the
hyper-inflated
frantic
black
and
white
zone
i
believe
we’re
at
the
very
beginning
of
explosive
growth
this
sense
of
anti-climax
is
completely
normal
for
transformational
technology
messaging
will
continue
to
gain
traction
chatbots
aren’t
going
away
nlp
and
ai
are
becoming
more
sophisticated
every
day
developers
apps
and
platforms
will
continue
to
experiment
with
and
heavily
invest
in
conversational
marketing
and
i
can’t
wait
to
see
what
happens
next
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
head
of
growth
for
growthbot
messaging
""
conversational
strategy
@hubspot
medium's
largest
publication
for
makers
subscribe
to
receive
our
top
stories
here
→
https:googlzhclji
""
if
you’ve
ever
found
yourself
looking
up
the
same
question
concept
or
syntax
over
and
over
again
when
programming
you’re
not
alone
i
find
myself
doing
this
constantly
while
it’s
not
unnatural
to
look
things
up
on
stackoverflow
or
other
resources
it
does
slow
you
down
a
good
bit
and
raise
questions
as
to
your
complete
understanding
of
the
language
we
live
in
a
world
where
there
is
a
seemingly
infinite
amount
of
accessible
free
resources
looming
just
one
search
away
at
all
times
however
this
can
be
both
a
blessing
and
a
curse
when
not
managed
effectively
an
over-reliance
on
these
resources
can
build
poor
habits
that
will
set
you
back
long-term
personally
i
find
myself
pulling
code
from
similar
discussion
threads
several
times
rather
than
taking
the
time
to
learn
and
solidify
the
concept
so
that
i
can
reproduce
the
code
myself
the
next
time
this
approach
is
lazy
and
while
it
may
be
the
path
of
least
resistance
in
the
short-term
it
will
ultimately
hurt
your
growth
productivity
and
ability
to
recall
syntax
cough
interviews
down
the
line
recently
i’ve
been
working
through
an
online
data
science
course
titled
python
for
data
science
and
machine
learning
on
udemy
oh
god
i
sound
like
that
guy
on
youtube
over
the
early
lectures
in
the
series
i
was
reminded
of
some
concepts
and
syntax
that
i
consistently
overlook
when
performing
data
analysis
in
python
in
the
interest
of
solidifying
my
understanding
of
these
concepts
once
and
for
all
and
saving
you
guys
a
couple
of
stackoverflow
searches
here’s
the
stuff
that
i’m
always
forgetting
when
working
with
python
numpy
and
pandas
i’ve
included
a
short
description
and
example
for
each
however
for
your
benefit
i
will
also
include
links
to
videos
and
other
resources
that
explore
each
concept
more
in-depth
as
well
writing
out
a
for
loop
every
time
you
need
to
define
some
sort
of
list
is
tedious
luckily
python
has
a
built-in
way
to
address
this
problem
in
just
one
line
of
code
the
syntax
can
be
a
little
hard
to
wrap
your
head
around
but
once
you
get
familiar
with
this
technique
you’ll
use
it
fairly
often
see
the
example
above
and
below
for
how
you
would
normally
go
about
list
comprehension
with
a
for
loop
vs
creating
your
list
with
in
one
simple
line
with
no
loops
necessary
ever
get
tired
of
creating
function
after
function
for
limited
use
cases?
lambda
functions
to
the
rescue!
lambda
functions
are
used
for
creating
small
one-time
and
anonymous
function
objects
in
python
basically
they
let
you
create
a
function
without
creating
a
function
the
basic
syntax
of
lambda
functions
is:
note
that
lambda
functions
can
do
everything
that
regular
functions
can
do
as
long
as
there’s
just
one
expression
check
out
the
simple
example
below
and
the
upcoming
video
to
get
a
better
feel
for
the
power
of
lambda
functions:
once
you
have
a
grasp
on
lambda
functions
learning
to
pair
them
with
the
map
and
filter
functions
can
be
a
powerful
tool
specifically
map
takes
in
a
list
and
transforms
it
into
a
new
list
by
performing
some
sort
of
operation
on
each
element
in
this
example
it
goes
through
each
element
and
maps
the
result
of
itself
times
2
to
a
new
list
note
that
the
list
function
simply
converts
the
output
to
list
type
the
filter
function
takes
in
a
list
and
a
rule
much
like
map
however
it
returns
a
subset
of
the
original
list
by
comparing
each
element
against
the
boolean
filtering
rule
for
creating
quick
and
easy
numpy
arrays
look
no
further
than
the
arange
and
linspace
functions
each
one
has
their
specific
purpose
but
the
appeal
here
instead
of
using
range
is
that
they
output
numpy
arrays
which
are
typically
easier
to
work
with
for
data
science
arange
returns
evenly
spaced
values
within
a
given
interval
along
with
a
starting
and
stopping
point
you
can
also
define
a
step
size
or
data
type
if
necessary
note
that
the
stopping
point
is
a
‘cut-off’
value
so
it
will
not
be
included
in
the
array
output
linspace
is
very
similar
but
with
a
slight
twist
linspace
returns
evenly
spaced
numbers
over
a
specified
interval
so
given
a
starting
and
stopping
point
as
well
as
a
number
of
values
linspace
will
evenly
space
them
out
for
you
in
a
numpy
array
this
is
especially
helpful
for
data
visualizations
and
declaring
axes
when
plotting
you
may
have
ran
into
this
when
dropping
a
column
in
pandas
or
summing
values
in
numpy
matrix
if
not
then
you
surely
will
at
some
point
let’s
use
the
example
of
dropping
a
column
for
now:
i
don’t
know
how
many
times
i
wrote
this
line
of
code
before
i
actually
knew
why
i
was
declaring
axis
what
i
was
as
you
can
probably
deduce
from
above
set
axis
to
1
if
you
want
to
deal
with
columns
and
set
it
to
0
if
you
want
rows
but
why
is
this?
my
favorite
reasoning
or
atleast
how
i
remember
this:
calling
the
shape
attribute
from
a
pandas
dataframe
gives
us
back
a
tuple
with
the
first
value
representing
the
number
of
rows
and
the
second
value
representing
the
number
of
columns
if
you
think
about
how
this
is
indexed
in
python
rows
are
at
0
and
columns
are
at
1
much
like
how
we
declare
our
axis
value
crazy
right?
if
you’re
familiar
with
sql
then
these
concepts
will
probably
come
a
lot
easier
for
you
anyhow
these
functions
are
essentially
just
ways
to
combine
dataframes
in
specific
ways
it
can
be
difficult
to
keep
track
of
which
is
best
to
use
at
which
time
so
let’s
review
it
concat
allows
the
user
to
append
one
or
more
dataframes
to
each
other
either
below
or
next
to
it
depending
on
how
you
define
the
axis
merge
combines
multiple
dataframes
on
specific
common
columns
that
serve
as
the
primary
key
join
much
like
merge
combines
two
dataframes
however
it
joins
them
based
on
their
indices
rather
than
some
specified
column
check
out
the
excellent
pandas
documentation
for
specific
syntax
and
more
concrete
examples
as
well
as
some
special
cases
that
you
may
run
into
think
of
apply
as
a
map
function
but
made
for
pandas
dataframes
or
more
specifically
for
series
if
you’re
not
as
familiar
series
are
pretty
similar
to
numpy
arrays
for
the
most
part
apply
sends
a
function
to
every
element
along
a
column
or
row
depending
on
what
you
specify
you
might
imagine
how
useful
this
can
be
especially
for
formatting
and
manipulating
values
across
a
whole
dataframe
column
without
having
to
loop
at
all
last
but
certainly
not
least
is
pivot
tables
if
you’re
familiar
with
microsoft
excel
then
you’ve
probably
heard
of
pivot
tables
in
some
respect
the
pandas
built-in
pivot_table
function
creates
a
spreadsheet-style
pivot
table
as
a
dataframe
note
that
the
levels
in
the
pivot
table
are
stored
in
multiindex
objects
on
the
index
and
columns
of
the
resulting
dataframe
that’s
it
for
now
i
hope
a
couple
of
these
overviews
have
effectively
jogged
your
memory
regarding
important
yet
somewhat
tricky
methods
functions
and
concepts
you
frequently
encounter
when
using
python
for
data
science
personally
i
know
that
even
the
act
of
writing
these
out
and
trying
to
explain
them
in
simple
terms
has
helped
me
out
a
ton
if
you’re
interested
in
receiving
my
weekly
rundown
of
interesting
articles
and
resources
focused
on
data
science
machine
learning
and
artificial
intelligence
then
subscribe
to
self
driven
data
science
using
the
form
below!
if
you
enjoyed
this
post
feel
free
to
hit
the
clap
button
and
if
you’re
interested
in
posts
to
come
make
sure
to
follow
me
on
medium
at
the
link
below
—
i’ll
be
writing
and
shipping
every
day
this
month
as
part
of
a
30-day
challenge
this
article
was
originally
published
on
conordeweycom
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
data
scientist
""
writer
|
wwwconordeweycom
sharing
concepts
ideas
and
codes
""
machine
learning
is
increasingly
moving
from
hand-designed
models
to
automatically
optimized
pipelines
using
tools
such
as
h20
tpot
and
auto-sklearn
these
libraries
along
with
methods
such
as
random
search
aim
to
simplify
the
model
selection
and
tuning
parts
of
machine
learning
by
finding
the
best
model
for
a
dataset
with
little
to
no
manual
intervention
however
feature
engineering
an
arguably
more
valuable
aspect
of
the
machine
learning
pipeline
remains
almost
entirely
a
human
labor
feature
engineering
also
known
as
feature
creation
is
the
process
of
constructing
new
features
from
existing
data
to
train
a
machine
learning
model
this
step
can
be
more
important
than
the
actual
model
used
because
a
machine
learning
algorithm
only
learns
from
the
data
we
give
it
and
creating
features
that
are
relevant
to
a
task
is
absolutely
crucial
see
the
excellent
paper
a
few
useful
things
to
know
about
machine
learning
typically
feature
engineering
is
a
drawn-out
manual
process
relying
on
domain
knowledge
intuition
and
data
manipulation
this
process
can
be
extremely
tedious
and
the
final
features
will
be
limited
both
by
human
subjectivity
and
time
automated
feature
engineering
aims
to
help
the
data
scientist
by
automatically
creating
many
candidate
features
out
of
a
dataset
from
which
the
best
can
be
selected
and
used
for
training
in
this
article
we
will
walk
through
an
example
of
using
automated
feature
engineering
with
the
featuretools
python
library
we
will
use
an
example
dataset
to
show
the
basics
stay
tuned
for
future
posts
using
real-world
data
the
complete
code
for
this
article
is
available
on
github
feature
engineering
means
building
additional
features
out
of
existing
data
which
is
often
spread
across
multiple
related
tables
feature
engineering
requires
extracting
the
relevant
information
from
the
data
and
getting
it
into
a
single
table
which
can
then
be
used
to
train
a
machine
learning
model
the
process
of
constructing
features
is
very
time-consuming
because
each
new
feature
usually
requires
several
steps
to
build
especially
when
using
information
from
more
than
one
table
we
can
group
the
operations
of
feature
creation
into
two
categories:
transformations
and
aggregations
let’s
look
at
a
few
examples
to
see
these
concepts
in
action
a
transformation
acts
on
a
single
table
thinking
in
terms
of
python
a
table
is
just
a
pandas
dataframe
""
by
creating
new
features
out
of
one
or
more
of
the
existing
columns
as
an
example
if
we
have
the
table
of
clients
below
we
can
create
features
by
finding
the
month
of
the
joined
column
or
taking
the
natural
log
of
the
income
column
these
are
both
transformations
because
they
use
information
from
only
one
table
on
the
other
hand
aggregations
are
performed
across
tables
and
use
a
one-to-many
relationship
to
group
observations
and
then
calculate
statistics
for
example
if
we
have
another
table
with
information
on
the
loans
of
clients
where
each
client
may
have
multiple
loans
we
can
calculate
statistics
such
as
the
average
maximum
and
minimum
of
loans
for
each
client
this
process
involves
grouping
the
loans
table
by
the
client
calculating
the
aggregations
and
then
merging
the
resulting
data
into
the
client
data
here’s
how
we
would
do
that
in
python
using
the
language
of
pandas
these
operations
are
not
difficult
by
themselves
but
if
we
have
hundreds
of
variables
spread
across
dozens
of
tables
this
process
is
not
feasible
to
do
by
hand
ideally
we
want
a
solution
that
can
automatically
perform
transformations
and
aggregations
across
multiple
tables
and
combine
the
resulting
data
into
a
single
table
although
pandas
is
a
great
resource
there’s
only
so
much
data
manipulation
we
want
to
do
by
hand!
for
more
on
manual
feature
engineering
check
out
the
excellent
python
data
science
handbook
fortunately
featuretools
is
exactly
the
solution
we
are
looking
for
this
open-source
python
library
will
automatically
create
many
features
from
a
set
of
related
tables
featuretools
is
based
on
a
method
known
as
deep
feature
synthesis
which
sounds
a
lot
more
imposing
than
it
actually
is
the
name
comes
from
stacking
multiple
features
not
because
it
uses
deep
learning!
deep
feature
synthesis
stacks
multiple
transformation
and
aggregation
operations
which
are
called
feature
primitives
in
the
vocab
of
featuretools
to
create
features
from
data
spread
across
many
tables
like
most
ideas
in
machine
learning
it’s
a
complex
method
built
on
a
foundation
of
simple
concepts
by
learning
one
building
block
at
a
time
we
can
form
a
good
understanding
of
this
powerful
method
first
let’s
take
a
look
at
our
example
data
we
already
saw
some
of
the
dataset
above
and
the
complete
collection
of
tables
is
as
follows:
if
we
have
a
machine
learning
task
such
as
predicting
whether
a
client
will
repay
a
future
loan
we
will
want
to
combine
all
the
information
about
clients
into
a
single
table
the
tables
are
related
through
the
client_id
and
the
loan_id
variables
and
we
could
use
a
series
of
transformations
and
aggregations
to
do
this
process
by
hand
however
we
will
shortly
see
that
we
can
instead
use
featuretools
to
automate
the
process
the
first
two
concepts
of
featuretools
are
entities
and
entitysets
an
entity
is
simply
a
table
or
a
dataframe
if
you
think
in
pandas
an
entityset
is
a
collection
of
tables
and
the
relationships
between
them
think
of
an
entityset
as
just
another
python
data
structure
with
its
own
methods
and
attributes
we
can
create
an
empty
entityset
in
featuretools
using
the
following:
now
we
have
to
add
entities
each
entity
must
have
an
index
which
is
a
column
with
all
unique
elements
that
is
each
value
in
the
index
must
appear
in
the
table
only
once
the
index
in
the
clients
dataframe
is
the
client_idbecause
each
client
has
only
one
row
in
this
dataframe
we
add
an
entity
with
an
existing
index
to
an
entityset
using
the
following
syntax:
the
loans
dataframe
also
has
a
unique
index
loan_id
and
the
syntax
to
add
this
to
the
entityset
is
the
same
as
for
clients
however
for
the
payments
dataframe
there
is
no
unique
index
when
we
add
this
entity
to
the
entityset
we
need
to
pass
in
the
parameter
make_index
=
true
and
specify
the
name
of
the
index
also
although
featuretools
will
automatically
infer
the
data
type
of
each
column
in
an
entity
we
can
override
this
by
passing
in
a
dictionary
of
column
types
to
the
parameter
variable_types
""
for
this
dataframe
even
though
missed
is
an
integer
this
is
not
a
numeric
variable
since
it
can
only
take
on
2
discrete
values
so
we
tell
featuretools
to
treat
is
as
a
categorical
variable
after
adding
the
dataframes
to
the
entityset
we
inspect
any
of
them:
the
column
types
have
been
correctly
inferred
with
the
modification
we
specified
next
we
need
to
specify
how
the
tables
in
the
entityset
are
related
the
best
way
to
think
of
a
relationship
between
two
tables
is
the
analogy
of
parent
to
child
this
is
a
one-to-many
relationship:
each
parent
can
have
multiple
children
in
the
realm
of
tables
a
parent
table
has
one
row
for
every
parent
but
the
child
table
may
have
multiple
rows
corresponding
to
multiple
children
of
the
same
parent
for
example
in
our
dataset
the
clients
dataframe
is
a
parent
of
the
loans
dataframe
each
client
has
only
one
row
in
clients
but
may
have
multiple
rows
in
loans
likewise
loans
is
the
parent
of
payments
because
each
loan
will
have
multiple
payments
the
parents
are
linked
to
their
children
by
a
shared
variable
when
we
perform
aggregations
we
group
the
child
table
by
the
parent
variable
and
calculate
statistics
across
the
children
of
each
parent
to
formalize
a
relationship
in
featuretools
we
only
need
to
specify
the
variable
that
links
two
tables
together
the
clients
and
the
loans
table
are
linked
via
the
client_id
variable
and
loans
and
payments
are
linked
with
the
loan_id
the
syntax
for
creating
a
relationship
and
adding
it
to
the
entityset
are
shown
below:
the
entityset
now
contains
the
three
entities
tables
and
the
relationships
that
link
these
entities
together
after
adding
entities
and
formalizing
relationships
our
entityset
is
complete
and
we
are
ready
to
make
features
before
we
can
quite
get
to
deep
feature
synthesis
we
need
to
understand
feature
primitives
we
already
know
what
these
are
but
we
have
just
been
calling
them
by
different
names!
these
are
simply
the
basic
operations
that
we
use
to
form
new
features:
new
features
are
created
in
featuretools
using
these
primitives
either
by
themselves
or
stacking
multiple
primitives
below
is
a
list
of
some
of
the
feature
primitives
in
featuretools
we
can
also
define
custom
primitives:
these
primitives
can
be
used
by
themselves
or
combined
to
create
features
to
make
features
with
specified
primitives
we
use
the
ftdfs
function
standing
for
deep
feature
synthesis
we
pass
in
the
entityset
the
target_entity
""
which
is
the
table
where
we
want
to
add
the
features
the
selected
trans_primitives
transformations
and
agg_primitives
aggregations:
the
result
is
a
dataframe
of
new
features
for
each
client
because
we
made
clients
the
target_entity
for
example
we
have
the
month
each
client
joined
which
is
a
transformation
feature
primitive:
we
also
have
a
number
of
aggregation
primitives
such
as
the
average
payment
amounts
for
each
client:
even
though
we
specified
only
a
few
feature
primitives
featuretools
created
many
new
features
by
combining
and
stacking
these
primitives
the
complete
dataframe
has
793
columns
of
new
features!
we
now
have
all
the
pieces
in
place
to
understand
deep
feature
synthesis
dfs
in
fact
we
already
performed
dfs
in
the
previous
function
call!
a
deep
feature
is
simply
a
feature
made
of
stacking
multiple
primitives
and
dfs
is
the
name
of
process
that
makes
these
features
the
depth
of
a
deep
feature
is
the
number
of
primitives
required
to
make
the
feature
for
example
the
meanpaymentspayment_amount
column
is
a
deep
feature
with
a
depth
of
1
because
it
was
created
using
a
single
aggregation
a
feature
with
a
depth
of
two
is
lastloansmeanpaymentspayment_amount
this
is
made
by
stacking
two
aggregations:
last
most
recent
on
top
of
mean
this
represents
the
average
payment
size
of
the
most
recent
loan
for
each
client
we
can
stack
features
to
any
depth
we
want
but
in
practice
i
have
never
gone
beyond
a
depth
of
2
after
this
point
the
features
are
difficult
to
interpret
but
i
encourage
anyone
interested
to
try
going
deeper
we
do
not
have
to
manually
specify
the
feature
primitives
but
instead
can
let
featuretools
automatically
choose
features
for
us
to
do
this
we
use
the
same
ftdfs
function
call
but
do
not
pass
in
any
feature
primitives:
featuretools
has
built
many
new
features
for
us
to
use
while
this
process
does
automatically
create
new
features
it
will
not
replace
the
data
scientist
because
we
still
have
to
figure
out
what
to
do
with
all
these
features
for
example
if
our
goal
is
to
predict
whether
or
not
a
client
will
repay
a
loan
we
could
look
for
the
features
most
correlated
with
a
specified
outcome
moreover
if
we
have
domain
knowledge
we
can
use
that
to
choose
specific
feature
primitives
or
seed
deep
feature
synthesis
with
candidate
features
automated
feature
engineering
has
solved
one
problem
but
created
another:
too
many
features
although
it’s
difficult
to
say
before
fitting
a
model
which
of
these
features
will
be
important
it’s
likely
not
all
of
them
will
be
relevant
to
a
task
we
want
to
train
our
model
on
moreover
having
too
many
features
can
lead
to
poor
model
performance
because
the
less
useful
features
drown
out
those
that
are
more
important
the
problem
of
too
many
features
is
known
as
the
curse
of
dimensionality
as
the
number
of
features
increases
the
dimension
of
the
data
grows
it
becomes
more
and
more
difficult
for
a
model
to
learn
the
mapping
between
features
and
targets
in
fact
the
amount
of
data
needed
for
the
model
to
perform
well
scales
exponentially
with
the
number
of
features
the
curse
of
dimensionality
is
combated
with
feature
reduction
also
known
as
feature
selection:
the
process
of
removing
irrelevant
features
this
can
take
on
many
forms:
principal
component
analysis
pca
selectkbest
using
feature
importances
from
a
model
or
auto-encoding
using
deep
neural
networks
however
feature
reduction
is
a
different
topic
for
another
article
for
now
we
know
that
we
can
use
featuretools
to
create
numerous
features
from
many
tables
with
minimal
effort!
like
many
topics
in
machine
learning
automated
feature
engineering
with
featuretools
is
a
complicated
concept
built
on
simple
ideas
using
concepts
of
entitysets
entities
and
relationships
featuretools
can
perform
deep
feature
synthesis
to
create
new
features
deep
feature
synthesis
in
turn
stacks
feature
primitives
—
aggregations
which
act
across
a
one-to-many
relationship
between
tables
and
transformations
functions
applied
to
one
or
more
columns
in
a
single
table
—
to
build
new
features
from
multiple
tables
in
future
articles
i’ll
show
how
to
use
this
technique
on
a
real
world
problem
the
home
credit
default
risk
competition
currently
being
hosted
on
kaggle
stay
tuned
for
that
post
and
in
the
meantime
read
this
introduction
to
get
started
in
the
competition!
i
hope
that
you
can
now
use
automated
feature
engineering
as
an
aid
in
a
data
science
pipeline
our
models
are
only
as
good
as
the
data
we
give
them
and
automated
feature
engineering
can
help
to
make
the
feature
creation
process
more
efficient
for
more
information
on
featuretools
including
advanced
usage
check
out
the
online
documentation
to
see
how
featuretools
is
used
in
practice
read
about
the
work
of
feature
labs
the
company
behind
the
open-source
library
as
always
i
welcome
feedback
and
constructive
criticism
and
can
be
reached
on
twitter
@koehrsen_will
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
data
scientist
and
master
student
data
science
communicator
and
advocate
sharing
concepts
ideas
and
codes
""
if
your
understanding
of
ai
and
machine
learning
is
a
big
question
mark
then
this
is
the
blog
post
for
you
here
i
gradually
increase
your
awesomenessicitytm
by
gluing
inspirational
videos
together
with
friendly
text
sit
down
and
relax
these
videos
take
time
and
if
they
don’t
inspire
you
to
continue
to
the
next
section
fair
enough
however
if
you
find
yourself
at
the
bottom
of
this
article
you’ve
earned
your
well-rounded
knowledge
and
passion
for
this
new
world
where
you
go
from
there
is
up
to
you
ai
was
always
cool
from
moving
a
paddle
in
pong
to
lighting
you
up
with
combos
in
street
fighter
ai
has
always
revolved
around
a
programmer’s
functional
guess
at
how
something
should
behave
fun
but
programmers
aren’t
always
gifted
in
programming
ai
as
we
often
see
just
google
epic
game
fails
to
see
glitches
in
ai
physics
and
sometimes
even
experienced
human
players
regardless
ai
has
a
new
talent
you
can
teach
a
computer
to
play
video
games
understand
language
and
even
how
to
identify
people
or
things
this
tip-of-the-iceberg
new
skill
comes
from
an
old
concept
that
only
recently
got
the
processing
power
to
exist
outside
of
theory
i’m
talking
about
machine
learning
you
don’t
need
to
come
up
with
advanced
algorithms
anymore
you
just
have
to
teach
a
computer
to
come
up
with
its
own
advanced
algorithm
so
how
does
something
like
that
even
work?
an
algorithm
isn’t
really
written
as
much
as
it
is
sort
of
bred
i’m
not
using
breeding
as
an
analogy
watch
this
short
video
which
gives
excellent
commentary
and
animations
to
the
high-level
concept
of
creating
the
ai
wow!
right?
that’s
a
crazy
process!
now
how
is
it
that
we
can’t
even
understand
the
algorithm
when
it’s
done?
one
great
visual
was
when
the
ai
was
written
to
beat
mario
games
as
a
human
we
all
understand
how
to
play
a
side-scroller
but
identifying
the
predictive
strategy
of
the
resulting
ai
is
insane
impressed?
there’s
something
amazing
about
this
idea
right?
the
only
problem
is
we
don’t
know
machine
learning
and
we
don’t
know
how
to
hook
it
up
to
video
games
fortunately
for
you
elon
musk
already
provided
a
non-profit
company
to
do
the
latter
yes
in
a
dozen
lines
of
code
you
can
hook
up
any
ai
you
want
to
countless
gamestasks!
i
have
two
good
answers
on
why
you
should
care
firstly
machine
learning
ml
is
making
computers
do
things
that
we’ve
never
made
computers
do
before
if
you
want
to
do
something
new
not
just
new
to
you
but
to
the
world
you
can
do
it
with
ml
secondly
if
you
don’t
influence
the
world
the
world
will
influence
you
right
now
significant
companies
are
investing
in
ml
and
we’re
already
seeing
it
change
the
world
thought-leaders
are
warning
that
we
can’t
let
this
new
age
of
algorithms
exist
outside
of
the
public
eye
imagine
if
a
few
corporate
monoliths
controlled
the
internet
if
we
don’t
take
up
arms
the
science
won’t
be
ours
i
think
christian
heilmann
said
it
best
in
his
talk
on
ml
the
concept
is
useful
and
cool
we
understand
it
at
a
high
level
but
what
the
heck
is
actually
happening?
how
does
this
work?
if
you
want
to
jump
straight
in
i
suggest
you
skip
this
section
and
move
on
to
the
next
how
do
i
get
started
section
if
you’re
motivated
to
be
a
doer
in
ml
you
won’t
need
these
videos
if
you’re
still
trying
to
grasp
how
this
could
even
be
a
thing
the
following
video
is
perfect
for
walking
you
through
the
logic
using
the
classic
ml
problem
of
handwriting
pretty
cool
huh?
that
video
shows
that
each
layer
gets
simpler
rather
than
more
complicated
like
the
function
is
chewing
data
into
smaller
pieces
that
end
in
an
abstract
concept
you
can
get
your
hands
dirty
in
interacting
with
this
process
on
this
site
by
adam
harley
it’s
cool
watching
data
go
through
a
trained
model
but
you
can
even
watch
your
neural
network
get
trained
one
of
the
classic
real-world
examples
of
machine
learning
in
action
is
the
iris
data
set
from
1936
in
a
presentation
i
attended
by
javafxpert’s
overview
on
machine
learning
i
learned
how
you
can
use
his
tool
to
visualize
the
adjustment
and
back
propagation
of
weights
to
neurons
on
a
neural
network
you
get
to
watch
it
train
the
neural
model!
even
if
you’re
not
a
java
buff
the
presentation
jim
gives
on
all
things
machine
learning
is
a
pretty
cool
15
hour
introduction
into
ml
concepts
which
includes
more
info
on
many
of
the
examples
above
these
concepts
are
exciting!
are
you
ready
to
be
the
einstein
of
this
new
era?
breakthroughs
are
happening
every
day
so
get
started
now
there
are
tons
of
resources
available
i’ll
be
recommending
two
approaches
in
this
approach
you’ll
understand
machine
learning
down
to
the
algorithms
and
the
math
i
know
this
way
sounds
tough
but
how
cool
would
it
be
to
really
get
into
the
details
and
code
this
stuff
from
scratch!
if
you
want
to
be
a
force
in
ml
and
hold
your
own
in
deep
conversations
then
this
is
the
route
for
you
i
recommend
that
you
try
out
brilliantorg’s
app
always
great
for
any
science
lover
and
take
the
artificial
neural
network
course
this
course
has
no
time
limits
and
helps
you
learn
ml
while
killing
time
in
line
on
your
phone
this
one
costs
money
after
level
1
combine
the
above
with
simultaneous
enrollment
in
andrew
ng’s
stanford
course
on
machine
learning
in
11
weeks
this
is
the
course
that
jim
weaver
recommended
in
his
video
above
i’ve
also
had
this
course
independently
suggested
to
me
by
jen
looper
everyone
provides
a
caveat
that
this
course
is
tough
for
some
of
you
that’s
a
show
stopper
but
for
others
that’s
why
you’re
going
to
put
yourself
through
it
and
collect
a
certificate
saying
you
did
this
course
is
100%
free
you
only
have
to
pay
for
a
certificate
if
you
want
one
with
those
two
courses
you’ll
have
a
lot
of
work
to
do
everyone
should
be
impressed
if
you
make
it
through
because
that’s
not
simple
but
more
so
if
you
do
make
it
through
you’ll
have
a
deep
understanding
of
the
implementation
of
machine
learning
that
will
catapult
you
into
successfully
applying
it
in
new
and
world-changing
ways
if
you’re
not
interested
in
writing
the
algorithms
but
you
want
to
use
them
to
create
the
next
breathtaking
websiteapp
you
should
jump
into
tensorflow
and
the
crash
course
tensorflow
is
the
de
facto
open-source
software
library
for
machine
learning
it
can
be
used
in
countless
ways
and
even
with
javascript
here’s
a
crash
course
plenty
more
information
on
available
courses
and
rankings
can
be
found
here
if
taking
a
course
is
not
your
style
you’re
still
in
luck
you
don’t
have
to
learn
the
nitty-gritty
of
ml
in
order
to
use
it
today
you
can
efficiently
utilize
ml
as
a
service
in
many
ways
with
tech
giants
who
have
trained
models
ready
i
would
still
caution
you
that
there’s
no
guarantee
that
your
data
is
safe
or
even
yours
but
the
offerings
of
services
for
ml
are
quite
attractive!
using
an
ml
service
might
be
the
best
solution
for
you
if
you’re
excited
and
able
to
upload
your
data
to
amazonmicrosoftgoogle
i
like
to
think
of
these
services
as
a
gateway
drug
to
advanced
ml
either
way
it’s
good
to
get
started
now
i
have
to
say
thank
you
to
all
the
aforementioned
people
and
videos
they
were
my
inspiration
to
get
started
and
though
i’m
still
a
newb
in
the
ml
world
i’m
happy
to
light
the
path
for
others
as
we
embrace
this
awe-inspiring
age
we
find
ourselves
in
it’s
imperative
to
reach
out
and
connect
with
people
if
you
take
up
learning
this
craft
without
friendly
faces
answers
and
sounding
boards
anything
can
be
hard
just
being
able
to
ask
and
get
a
response
is
a
game
changer
add
me
and
add
the
people
mentioned
above
friendly
people
with
friendly
advice
helps!
see?
i
hope
this
article
has
inspired
you
and
those
around
you
to
learn
ml!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
software
consultant
adjunct
professor
published
author
award
winning
speaker
mentor
organizer
and
immature
nerd
:d
—
lately
full
of
react
native
tech
our
community
publishes
stories
worth
reading
on
development
design
and
data
science
""
want
to
learn
about
applied
artificial
intelligence
from
leading
practitioners
in
silicon
valley
new
york
or
toronto?
learn
more
about
the
insight
artificial
intelligence
fellows
program
are
you
a
company
working
in
ai
and
would
like
to
get
involved
in
the
insight
ai
fellows
program?
feel
free
to
get
in
touch
recently
i
gave
a
talk
at
the
o’reilly
ai
conference
in
beijing
about
some
of
the
interesting
lessons
we’ve
learned
in
the
world
of
nlp
while
there
i
was
lucky
enough
to
attend
a
tutorial
on
deep
reinforcement
learning
deep
rl
from
scratch
by
unity
technologies
i
thought
that
the
session
led
by
arthur
juliani
was
extremely
informative
and
wanted
to
share
some
big
takeaways
below
in
our
conversations
with
companies
we’ve
seen
a
rise
of
interesting
deep
rl
applications
tools
and
results
in
parallel
the
inner
workings
and
applications
of
deep
rl
such
as
alphago
pictured
above
can
often
seem
esoteric
and
hard
to
understand
in
this
post
i
will
give
an
overview
of
core
aspects
of
the
field
that
can
be
understood
by
anyone
many
of
the
visuals
are
from
the
slides
of
the
talk
and
some
are
new
the
explanations
and
opinions
are
mine
if
anything
is
unclear
reach
out
to
me
here!
deep
rl
is
a
field
that
has
seen
vast
amounts
of
research
interest
including
learning
to
play
atari
games
beating
pro
players
at
dota
2
and
defeating
go
champions
contrary
to
many
classical
deep
learning
problems
that
often
focus
on
perception
does
this
image
contain
a
stop
sign?
deep
rl
adds
the
dimension
of
actions
that
influence
the
environment
what
is
the
goal
and
how
do
i
get
there?
in
dialog
systems
for
example
classical
deep
learning
aims
to
learn
the
right
response
for
a
given
query
on
the
other
hand
deep
reinforcement
learning
focuses
on
the
right
sequences
of
sentences
that
will
lead
to
a
positive
outcome
for
example
a
happy
customer
this
makes
deep
rl
particularly
attractive
for
tasks
that
require
planning
and
adaptation
such
as
manufacturing
or
self-driving
however
industry
applications
have
trailed
behind
the
rapidly
advancing
results
coming
out
of
the
research
community
a
major
reason
is
that
deep
rl
often
requires
an
agent
to
experiment
millions
of
times
before
learning
anything
useful
the
best
way
to
do
this
rapidly
is
by
using
a
simulation
environment
this
tutorial
will
be
using
unity
to
create
environments
to
train
agents
in
for
this
workshop
led
by
arthur
juliani
and
leon
chen
their
goal
was
to
get
every
participants
to
successfully
train
multiple
deep
rl
algorithms
in
4
hours
a
tall
order!
below
is
a
comprehensive
overview
of
many
of
the
main
algorithms
that
power
deep
rl
today
for
a
more
complete
set
of
tutorials
arthur
juliani
wrote
an
8-part
series
starting
here
deep
rl
can
be
used
to
best
the
top
human
players
at
go
but
to
understand
how
that’s
done
you
first
need
to
understand
a
few
simple
concepts
starting
with
much
easier
problems
1it
all
starts
with
slot
machines
let’s
imagine
you
are
faced
with
4
chests
that
you
can
pick
from
at
each
turn
each
of
them
have
a
different
average
payout
and
your
goal
is
to
maximize
the
total
payout
you
receive
after
a
fixed
number
of
turns
this
is
a
classic
problem
called
multi-armed
bandits
and
is
where
we
will
start
the
crux
of
the
problem
is
to
balance
exploration
which
helps
us
learn
about
which
states
are
good
and
exploitation
where
we
now
use
what
we
know
to
pick
the
best
slot
machine
here
we
will
utilize
a
value
function
that
maps
our
actions
to
an
estimated
reward
called
the
q
function
first
we’ll
initialize
all
q
values
at
equal
values
then
we’ll
update
the
q
value
of
each
action
picking
each
chest
based
on
how
good
the
payout
was
after
choosing
this
action
this
allows
us
to
learn
a
good
value
function
we
will
approximate
our
q
function
using
a
neural
network
starting
with
a
very
shallow
one
that
learns
a
probability
distribution
by
using
a
softmax
over
the
4
potential
chests
while
the
value
function
tells
us
how
good
we
estimate
each
action
to
be
the
policy
is
the
function
that
determines
which
actions
we
end
up
taking
intuitively
we
might
want
to
use
a
policy
that
picks
the
action
with
the
highest
q
value
this
performs
poorly
in
practice
as
our
q
estimates
will
be
very
wrong
at
the
start
before
we
gather
enough
experience
through
trial
and
error
this
is
why
we
need
to
add
a
mechanism
to
our
policy
to
encourage
exploration
one
way
to
do
that
is
to
use
epsilon
greedy
which
consists
of
taking
a
random
action
with
probability
epsilon
we
start
with
epsilon
being
close
to
1
always
choosing
random
actions
and
lower
epsilon
as
we
go
along
and
learn
more
about
which
chests
are
good
eventually
we
learn
which
chests
are
best
in
practice
we
might
want
to
take
a
more
subtle
approach
than
either
taking
the
action
we
think
is
the
best
or
a
random
action
a
popular
method
is
boltzmann
exploration
which
adjust
probabilities
based
on
our
current
estimate
of
how
good
each
chest
is
adding
in
a
randomness
factor
2adding
different
states
the
previous
example
was
a
world
in
which
we
were
always
in
the
same
state
waiting
to
pick
from
the
same
4
chests
in
front
of
us
most
real-word
problems
consist
of
many
different
states
that
is
what
we
will
add
to
our
environment
next
now
the
background
behind
chests
alternates
between
3
colors
at
each
turn
changing
the
average
values
of
the
chests
this
means
we
need
to
learn
a
q
function
that
depends
not
only
on
the
action
the
chest
we
pick
but
the
state
what
the
color
of
the
background
is
this
version
of
the
problem
is
called
contextual
multi-armed
bandits
surprisingly
we
can
use
the
same
approach
as
before
the
only
thing
we
need
to
add
is
an
extra
dense
layer
to
our
neural
network
that
will
take
in
as
input
a
vector
representing
the
current
state
of
the
world
3learning
about
the
consequences
of
our
actions
there
is
another
key
factor
that
makes
our
current
problem
simpler
than
mosts
in
most
environments
such
as
in
the
maze
depicted
above
the
actions
that
we
take
have
an
impact
on
the
state
of
the
world
if
we
move
up
on
this
grid
we
might
receive
a
reward
or
we
might
receive
nothing
but
the
next
turn
we
will
be
in
a
different
state
this
is
where
we
finally
introduce
a
need
for
planning
first
we
will
define
our
q
function
as
the
immediate
reward
in
our
current
state
plus
the
discounted
reward
we
are
expecting
by
taking
all
of
our
future
actions
this
solution
works
if
our
q
estimate
of
states
is
accurate
so
how
can
we
learn
a
good
estimate?
we
will
use
a
method
called
temporal
difference
td
learning
to
learn
a
good
q
function
the
idea
is
to
only
look
at
a
limited
number
of
steps
in
the
future
td1
for
example
only
uses
the
next
2
states
to
evaluate
the
reward
surprisingly
we
can
use
td0
which
looks
at
the
current
state
and
our
estimate
of
the
reward
the
next
turn
and
get
great
results
the
structure
of
the
network
is
the
same
but
we
need
to
go
through
one
forward
step
before
receiving
the
error
we
then
use
this
error
to
back
propagate
gradients
like
in
traditional
deep
learning
and
update
our
value
estimates
3introducing
monte
carlo
another
method
to
estimate
the
eventual
success
of
our
actions
is
monte
carlo
estimates
this
consists
of
playing
out
the
entire
episode
with
our
current
policy
until
we
reach
an
end
success
by
reaching
a
green
block
or
failure
by
reaching
a
red
block
in
the
image
above
and
use
that
result
to
update
our
value
estimates
for
each
traversed
state
this
allows
us
to
propagate
values
efficiently
in
one
batch
at
the
end
of
an
episode
instead
of
every
time
we
make
a
move
the
cost
is
that
we
are
introducing
noise
to
our
estimates
since
we
attribute
very
distant
rewards
to
them
4the
world
is
rarely
discrete
the
previous
methods
were
using
neural
networks
to
approximate
our
value
estimates
by
mapping
from
a
discrete
number
of
states
and
actions
to
a
value
in
the
maze
for
example
there
were
49
states
squares
and
4
actions
move
in
each
adjacent
direction
in
this
environment
we
are
trying
to
learn
how
to
balance
a
ball
on
a
2
dimensional
paddle
by
deciding
at
each
time
step
whether
we
want
to
tilt
the
paddle
left
or
right
here
the
state
space
becomes
continuous
the
angle
of
the
paddle
and
the
position
of
the
ball
the
good
news
is
we
can
still
use
neural
networks
to
approximate
this
function!
a
note
about
off-policy
vs
on-policy
learning:
the
methods
we
used
previously
are
off-policy
methods
meaning
we
can
generate
data
with
any
strategyusing
epsilon
greedy
for
example
and
learn
from
it
on-policy
methods
can
only
learn
from
actions
that
were
taken
following
our
policy
remember
a
policy
is
the
method
we
use
to
determine
which
actions
to
take
this
constrains
our
learning
process
as
we
have
to
have
an
exploration
strategy
that
is
built
in
to
the
policy
itself
but
allows
us
to
tie
results
directly
to
our
reasoning
and
enables
us
to
learn
more
efficiently
the
approach
we
will
use
here
is
called
policy
gradients
and
is
an
on-policy
method
previously
we
were
first
learning
a
value
function
q
for
each
action
in
each
state
and
then
building
a
policy
on
top
in
vanilla
policy
gradient
we
still
use
monte
carlo
estimates
but
we
learn
our
policy
directly
through
a
loss
function
that
increases
the
probability
of
choosing
rewarding
actions
since
we
are
learning
on
policy
we
cannot
use
methods
such
as
epsilon
greedy
which
includes
random
choices
to
get
our
agent
to
explore
the
environment
the
way
that
we
encourage
exploration
is
by
using
a
method
called
entropy
regularization
which
pushes
our
probability
estimates
to
be
wider
and
thus
will
encourage
us
to
make
riskier
choices
to
explore
the
space
4leveraging
deep
learning
for
representations
in
practice
many
state
of
the
art
rl
methods
require
learning
both
a
policy
and
value
estimates
the
way
we
do
this
with
deep
learning
is
by
having
both
be
two
separate
outputs
of
the
same
backbone
neural
network
which
will
make
it
easier
for
our
neural
network
to
learn
good
representations
one
method
to
do
this
is
advantage
actor
critic
a2c
we
learn
our
policy
directly
with
policy
gradients
defined
above
and
learn
a
value
function
using
something
called
advantage
instead
of
updating
our
value
function
based
on
rewards
we
update
it
based
on
our
advantage
which
measures
how
much
better
or
worse
an
action
was
than
our
previous
value
function
estimated
it
to
be
this
helps
make
learning
more
stable
compared
to
simple
q
learning
and
vanilla
policy
gradients
5learning
directly
from
the
screen
there
is
an
additional
advantage
to
using
deep
learning
for
these
methods
which
is
that
deep
neural
networks
excel
at
perceptive
tasks
when
a
human
plays
a
game
the
information
received
is
not
a
list
of
states
but
an
image
usually
of
a
screen
or
a
board
or
the
surrounding
environment
image-based
learning
combines
a
convolutional
neural
network
cnn
with
rl
in
this
environment
we
pass
in
a
raw
image
instead
of
features
and
add
a
2
layer
cnn
to
our
architecture
without
changing
anything
else!
we
can
even
inspect
activations
to
see
what
the
network
picks
up
on
to
determine
value
and
policy
in
the
example
below
we
can
see
that
the
network
uses
the
current
score
and
distant
obstacles
to
estimate
the
value
of
the
current
state
while
focusing
on
nearby
obstacles
for
determining
actions
neat!
as
a
side
note
while
toying
around
with
the
provided
implementation
i’ve
found
that
visual
learning
is
very
sensitive
to
hyperparameters
changing
the
discount
rate
slightly
for
example
completely
prevented
the
neural
network
from
learning
even
on
a
toy
application
this
is
a
widely
known
problem
but
it
is
interesting
to
see
it
first
hand
6nuanced
actions
so
far
we’ve
played
with
environments
with
continuous
and
discrete
state
spaces
however
every
environment
we
studied
had
a
discrete
action
space:
we
could
move
in
one
of
four
directions
or
tilt
the
paddle
to
the
left
or
right
ideally
for
applications
such
as
self-driving
cars
we
would
like
to
learn
continuous
actions
such
as
turning
the
steering
wheel
between
0
and
360
degrees
in
this
environment
called
3d
ball
world
we
can
choose
to
tilt
the
paddle
to
any
value
on
each
of
its
axes
this
gives
us
more
control
as
to
how
we
perform
actions
but
makes
the
action
space
much
larger
we
can
approach
this
by
approximating
our
potential
choices
with
gaussian
distributions
we
learn
a
probability
distribution
over
potential
actions
by
learning
the
mean
and
variance
of
a
gaussian
distribution
and
our
policy
we
sample
from
that
distribution
simple
in
theory
:
7next
steps
for
the
brave
there
are
a
few
concepts
that
separate
the
algorithms
described
above
from
state
of
the
art
approaches
it’s
interesting
to
see
that
conceptually
the
best
robotics
and
game-playing
algorithms
are
not
that
far
away
from
the
ones
we
just
explored:
that’s
it
for
this
overview
i
hope
this
has
been
informative
and
fun!
if
you
are
looking
to
dive
deeper
into
the
theory
of
rl
give
arthur’s
posts
a
read
or
diving
deeper
by
following
david
silver’s
ucl
course
if
you
are
looking
to
learn
more
about
the
projects
we
do
at
insight
or
how
we
work
with
companies
please
check
us
out
below
or
reach
out
to
me
here
want
to
learn
about
applied
artificial
intelligence
from
leading
practitioners
in
silicon
valley
new
york
or
toronto?
learn
more
about
the
insight
artificial
intelligence
fellows
program
are
you
a
company
working
in
ai
and
would
like
to
get
involved
in
the
insight
ai
fellows
program?
feel
free
to
get
in
touch
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ai
lead
at
insight
ai
@emmanuelameisen
insight
fellows
program
-
your
bridge
to
a
career
in
data
""
the
advent
of
powerful
and
versatile
deep
learning
frameworks
in
recent
years
has
made
it
possible
to
implement
convolution
layers
into
a
deep
learning
model
an
extremely
simple
task
often
achievable
in
a
single
line
of
code
however
understanding
convolutions
especially
for
the
first
time
can
often
feel
a
bit
unnerving
with
terms
like
kernels
filters
channels
and
so
on
all
stacked
onto
each
other
yet
convolutions
as
a
concept
are
fascinatingly
powerful
and
highly
extensible
and
in
this
post
we’ll
break
down
the
mechanics
of
the
convolution
operation
step-by-step
relate
it
to
the
standard
fully
connected
network
and
explore
just
how
they
build
up
a
strong
visual
hierarchy
making
them
powerful
feature
extractors
for
images
the
2d
convolution
is
a
fairly
simple
operation
at
heart:
you
start
with
a
kernel
which
is
simply
a
small
matrix
of
weights
this
kernel
slides
over
the
2d
input
data
performing
an
elementwise
multiplication
with
the
part
of
the
input
it
is
currently
on
and
then
summing
up
the
results
into
a
single
output
pixel
the
kernel
repeats
this
process
for
every
location
it
slides
over
converting
a
2d
matrix
of
features
into
yet
another
2d
matrix
of
features
the
output
features
are
essentially
the
weighted
sums
with
the
weights
being
the
values
of
the
kernel
itself
of
the
input
features
located
roughly
in
the
same
location
of
the
output
pixel
on
the
input
layer
whether
or
not
an
input
feature
falls
within
this
roughly
same
location
gets
determined
directly
by
whether
it’s
in
the
area
of
the
kernel
that
produced
the
output
or
not
this
means
the
size
of
the
kernel
directly
determines
how
many
or
few
input
features
get
combined
in
the
production
of
a
new
output
feature
this
is
all
in
pretty
stark
contrast
to
a
fully
connected
layer
in
the
above
example
we
have
5×5=25
input
features
and
3×3=9
output
features
if
this
were
a
standard
fully
connected
layer
you’d
have
a
weight
matrix
of
25×9
=
225
parameters
with
every
output
feature
being
the
weighted
sum
of
every
single
input
feature
convolutions
allow
us
to
do
this
transformation
with
only
9
parameters
with
each
output
feature
instead
of
looking
at
every
input
feature
only
getting
to
look
at
input
features
coming
from
roughly
the
same
location
do
take
note
of
this
as
it’ll
be
critical
to
our
later
discussion
before
we
move
on
it’s
definitely
worth
looking
into
two
techniques
that
are
commonplace
in
convolution
layers:
padding
and
strides
padding
does
something
pretty
clever
to
solve
this:
pad
the
edges
with
extra
fake
pixels
usually
of
value
0
hence
the
oft-used
term
zero
padding
this
way
the
kernel
when
sliding
can
allow
the
original
edge
pixels
to
be
at
its
center
while
extending
into
the
fake
pixels
beyond
the
edge
producing
an
output
the
same
size
as
the
input
the
idea
of
the
stride
is
to
skip
some
of
the
slide
locations
of
the
kernel
a
stride
of
1
means
to
pick
slides
a
pixel
apart
so
basically
every
single
slide
acting
as
a
standard
convolution
a
stride
of
2
means
picking
slides
2
pixels
apart
skipping
every
other
slide
in
the
process
downsizing
by
roughly
a
factor
of
2
a
stride
of
3
means
skipping
every
2
slides
downsizing
roughly
by
factor
3
and
so
on
more
modern
networks
such
as
the
resnet
architectures
entirely
forgo
pooling
layers
in
their
internal
layers
in
favor
of
strided
convolutions
when
needing
to
reduce
their
output
sizes
of
course
the
diagrams
above
only
deals
with
the
case
where
the
image
has
a
single
input
channel
in
practicality
most
input
images
have
3
channels
and
that
number
only
increases
the
deeper
you
go
into
a
network
it’s
pretty
easy
to
think
of
channels
in
general
as
being
a
view
of
the
image
as
a
whole
emphasising
some
aspects
de-emphasising
others
so
this
is
where
a
key
distinction
between
terms
comes
in
handy:
whereas
in
the
1
channel
case
where
the
term
filter
and
kernel
are
interchangeable
in
the
general
case
they’re
actually
pretty
different
each
filter
actually
happens
to
be
a
collection
of
kernels
with
there
being
one
kernel
for
every
single
input
channel
to
the
layer
and
each
kernel
being
unique
each
filter
in
a
convolution
layer
produces
one
and
only
one
output
channel
and
they
do
it
like
so:
each
of
the
kernels
of
the
filter
slides
over
their
respective
input
channels
producing
a
processed
version
of
each
some
kernels
may
have
stronger
weights
than
others
to
give
more
emphasis
to
certain
input
channels
than
others
eg
a
filter
may
have
a
red
kernel
channel
with
stronger
weights
than
others
and
hence
respond
more
to
differences
in
the
red
channel
features
than
the
others
each
of
the
per-channel
processed
versions
are
then
summed
together
to
form
one
channel
the
kernels
of
a
filter
each
produce
one
version
of
each
channel
and
the
filter
as
a
whole
produces
one
overall
output
channel
finally
then
there’s
the
bias
term
the
way
the
bias
term
works
here
is
that
each
output
filter
has
one
bias
term
the
bias
gets
added
to
the
output
channel
so
far
to
produce
the
final
output
channel
and
with
the
single
filter
case
down
the
case
for
any
number
of
filters
is
identical:
each
filter
processes
the
input
with
its
own
different
set
of
kernels
and
a
scalar
bias
with
the
process
described
above
producing
a
single
output
channel
they
are
then
concatenated
together
to
produce
the
overall
output
with
the
number
of
output
channels
being
the
number
of
filters
a
nonlinearity
is
then
usually
applied
before
passing
this
as
input
to
another
convolution
layer
which
then
repeats
this
process
even
with
the
mechanics
of
the
convolution
layer
down
it
can
still
be
hard
to
relate
it
back
to
a
standard
feed-forward
network
and
it
still
doesn’t
explain
why
convolutions
scale
to
and
work
so
much
better
for
image
data
suppose
we
have
a
4×4
input
and
we
want
to
transform
it
into
a
2×2
grid
if
we
were
using
a
feedforward
network
we’d
reshape
the
4×4
input
into
a
vector
of
length
16
and
pass
it
through
a
densely
connected
layer
with
16
inputs
and
4
outputs
one
could
visualize
the
weight
matrix
w
for
a
layer:
and
although
the
convolution
kernel
operation
may
seem
a
bit
strange
at
first
it
is
still
a
linear
transformation
with
an
equivalent
transformation
matrix
if
we
were
to
use
a
kernel
k
of
size
3
on
the
reshaped
4×4
input
to
get
a
2×2
output
the
equivalent
transformation
matrix
would
be:
note:
while
the
above
matrix
is
an
equivalent
transformation
matrix
the
actual
operation
is
usually
implemented
as
a
very
different
matrix
multiplication[2]
the
convolution
then
as
a
whole
is
still
a
linear
transformation
but
at
the
same
time
it’s
also
a
dramatically
different
kind
of
transformation
for
a
matrix
with
64
elements
there’s
just
9
parameters
which
themselves
are
reused
several
times
each
output
node
only
gets
to
see
a
select
number
of
inputs
the
ones
inside
the
kernel
there
is
no
interaction
with
any
of
the
other
inputs
as
the
weights
to
them
are
set
to
0
it’s
useful
to
see
the
convolution
operation
as
a
hard
prior
on
the
weight
matrix
in
this
context
by
prior
i
mean
predefined
network
parameters
for
example
when
you
use
a
pretrained
model
for
image
classification
you
use
the
pretrained
network
parameters
as
your
prior
as
a
feature
extractor
to
your
final
densely
connected
layer
in
that
sense
there’s
a
direct
intuition
between
why
both
are
so
efficient
compared
to
their
alternatives
transfer
learning
is
efficient
by
orders
of
magnitude
compared
to
random
initialization
because
you
only
really
need
to
optimize
the
parameters
of
the
final
fully
connected
layer
which
means
you
can
have
fantastic
performance
with
only
a
few
dozen
images
per
class
here
you
don’t
need
to
optimize
all
64
parameters
because
we
set
most
of
them
to
zero
and
they’ll
stay
that
way
and
the
rest
we
convert
to
shared
parameters
resulting
in
only
9
actual
parameters
to
optimize
this
efficiency
matters
because
when
you
move
from
the
784
inputs
of
mnist
to
real
world
224×224×3
images
thats
over
150000
inputs
a
dense
layer
attempting
to
halve
the
input
to
75000
inputs
would
still
require
over
10
billion
parameters
for
comparison
the
entirety
of
resnet-50
has
some
25
million
parameters
so
fixing
some
parameters
to
0
and
tying
parameters
increases
efficiency
but
unlike
the
transfer
learning
case
where
we
know
the
prior
is
good
because
it
works
on
a
large
general
set
of
images
how
do
we
know
this
is
any
good?
the
answer
lies
in
the
feature
combinations
the
prior
leads
the
parameters
to
learn
early
on
in
this
article
we
discussed
that:
so
with
backpropagation
coming
in
all
the
way
from
the
classification
nodes
of
the
network
the
kernels
have
the
interesting
task
of
learning
weights
to
produce
features
only
from
a
set
of
local
inputs
additionally
because
the
kernel
itself
is
applied
across
the
entire
image
the
features
the
kernel
learns
must
be
general
enough
to
come
from
any
part
of
the
image
if
this
were
any
other
kind
of
data
eg
categorical
data
of
app
installs
this
would’ve
been
a
disaster
for
just
because
your
number
of
app
installs
and
app
type
columns
are
next
to
each
other
doesn’t
mean
they
have
any
local
shared
features
common
with
app
install
dates
and
time
used
sure
the
four
may
have
an
underlying
higher
level
feature
eg
which
apps
people
want
most
that
can
be
found
but
that
gives
us
no
reason
to
believe
the
parameters
for
the
first
two
are
exactly
the
same
as
the
parameters
for
the
latter
two
the
four
could’ve
been
in
any
consistent
order
and
still
be
valid!
pixels
however
always
appear
in
a
consistent
order
and
nearby
pixels
influence
a
pixel
eg
if
all
nearby
pixels
are
red
it’s
pretty
likely
the
pixel
is
also
red
if
there
are
deviations
that’s
an
interesting
anomaly
that
could
be
converted
into
a
feature
and
all
this
can
be
detected
from
comparing
a
pixel
with
its
neighbors
with
other
pixels
in
its
locality
and
this
idea
is
really
what
a
lot
of
earlier
computer
vision
feature
extraction
methods
were
based
around
for
instance
for
edge
detection
one
can
use
a
sobel
edge
detection
filter
a
kernel
with
fixed
parameters
operating
just
like
the
standard
one-channel
convolution:
for
a
non-edge
containing
grid
eg
the
background
sky
most
of
the
pixels
are
the
same
value
so
the
overall
output
of
the
kernel
at
that
point
is
0
for
a
grid
with
an
vertical
edge
there
is
a
difference
between
the
pixels
to
the
left
and
right
of
the
edge
and
the
kernel
computes
that
difference
to
be
non-zero
activating
and
revealing
the
edges
the
kernel
only
works
only
a
3×3
grids
at
a
time
detecting
anomalies
on
a
local
scale
yet
when
applied
across
the
entire
image
is
enough
to
detect
a
certain
feature
on
a
global
scale
anywhere
in
the
image!
so
the
key
difference
we
make
with
deep
learning
is
ask
this
question:
can
useful
kernels
be
learnt?
for
early
layers
operating
on
raw
pixels
we
could
reasonably
expect
feature
detectors
of
fairly
low
level
features
like
edges
lines
etc
there’s
an
entire
branch
of
deep
learning
research
focused
on
making
neural
network
models
interpretable
one
of
the
most
powerful
tools
to
come
out
of
that
is
feature
visualization
using
optimization[3]
the
idea
at
core
is
simple:
optimize
a
image
usually
initialized
with
random
noise
to
activate
a
filter
as
strongly
as
possible
this
does
make
intuitive
sense:
if
the
optimized
image
is
completely
filled
with
edges
that’s
strong
evidence
that’s
what
the
filter
itself
is
looking
for
and
is
activated
by
using
this
we
can
peek
into
the
learnt
filters
and
the
results
are
stunning:
one
important
thing
to
notice
here
is
that
convolved
images
are
still
images
the
output
of
a
small
grid
of
pixels
from
the
top
left
of
an
image
will
still
be
on
the
top
left
so
you
can
run
another
convolution
layer
on
top
of
another
such
as
the
two
on
the
left
to
extract
deeper
features
which
we
visualize
yet
however
deep
our
feature
detectors
get
without
any
further
changes
they’ll
still
be
operating
on
very
small
patches
of
the
image
no
matter
how
deep
your
detectors
are
you
can’t
detect
faces
from
a
3×3
grid
and
this
is
where
the
idea
of
the
receptive
field
comes
in
a
essential
design
choice
of
any
cnn
architecture
is
that
the
input
sizes
grow
smaller
and
smaller
from
the
start
to
the
end
of
the
network
while
the
number
of
channels
grow
deeper
this
as
mentioned
earlier
is
often
done
through
strides
or
pooling
layers
locality
determines
what
inputs
from
the
previous
layer
the
outputs
get
to
see
the
receptive
field
determines
what
area
of
the
original
input
to
the
entire
network
the
output
gets
to
see
the
idea
of
a
strided
convolution
is
that
we
only
process
slides
a
fixed
distance
apart
and
skip
the
ones
in
the
middle
from
a
different
point
of
view
we
only
keep
outputs
a
fixed
distance
apart
and
remove
the
rest[1]
we
then
apply
a
nonlinearity
to
the
output
and
per
usual
then
stack
another
new
convolution
layer
on
top
and
this
is
where
things
get
interesting
even
if
were
we
to
apply
a
kernel
of
the
same
size
3×3
having
the
same
local
area
to
the
output
of
the
strided
convolution
the
kernel
would
have
a
larger
effective
receptive
field:
this
is
because
the
output
of
the
strided
layer
still
does
represent
the
same
image
it
is
not
so
much
cropping
as
it
is
resizing
only
thing
is
that
each
single
pixel
in
the
output
is
a
representative
of
a
larger
area
of
whose
other
pixels
were
discarded
from
the
same
rough
location
from
the
original
input
so
when
the
next
layer’s
kernel
operates
on
the
output
it’s
operating
on
pixels
collected
from
a
larger
area
note:
if
you’re
familiar
with
dilated
convolutions
note
that
the
above
is
not
a
dilated
convolution
both
are
methods
of
increasing
the
receptive
field
but
dilated
convolutions
are
a
single
layer
while
this
takes
place
on
a
regular
convolution
following
a
strided
convolution
with
a
nonlinearity
inbetween
this
expansion
of
the
receptive
field
allows
the
convolution
layers
to
combine
the
low
level
features
lines
edges
into
higher
level
features
curves
textures
as
we
see
in
the
mixed3a
layer
followed
by
a
poolingstrided
layer
the
network
continues
to
create
detectors
for
even
higher
level
features
parts
patterns
as
we
see
for
mixed4a
the
repeated
reduction
in
image
size
across
the
network
results
in
by
the
5th
block
on
convolutions
input
sizes
of
just
7×7
compared
to
inputs
of
224×224
at
this
point
each
single
pixel
represents
a
grid
of
32×32
pixels
which
is
huge
compared
to
earlier
layers
where
an
activation
meant
detecting
an
edge
here
an
activation
on
the
tiny
7×7
grid
is
one
for
a
very
high
level
feature
such
as
for
birds
the
network
as
a
whole
progresses
from
a
small
number
of
filters
64
in
case
of
googlenet
detecting
low
level
features
to
a
very
large
number
of
filters1024
in
the
final
convolution
each
looking
for
an
extremely
specific
high
level
feature
followed
by
a
final
pooling
layer
which
collapses
each
7×7
grid
into
a
single
pixel
each
channel
is
a
feature
detector
with
a
receptive
field
equivalent
to
the
entire
image
compared
to
what
a
standard
feedforward
network
would
have
done
the
output
here
is
really
nothing
short
of
awe-inspiring
a
standard
feedforward
network
would
have
produced
abstract
feature
vectors
from
combinations
of
every
single
pixel
in
the
image
requiring
intractable
amounts
of
data
to
train
the
cnn
with
the
priors
imposed
on
it
starts
by
learning
very
low
level
feature
detectors
and
as
across
the
layers
as
its
receptive
field
is
expanded
learns
to
combine
those
low-level
features
into
progressively
higher
level
features
not
an
abstract
combination
of
every
single
pixel
but
rather
a
strong
visual
hierarchy
of
concepts
by
detecting
low
level
features
and
using
them
to
detect
higher
level
features
as
it
progresses
up
its
visual
hierarchy
it
is
eventually
able
to
detect
entire
visual
concepts
such
as
faces
birds
trees
etc
and
that’s
what
makes
them
such
powerful
yet
efficient
with
image
data
with
the
visual
hierarchy
cnns
build
it
is
pretty
reasonable
to
assume
that
their
vision
systems
are
similar
to
humans
and
they’re
really
great
with
real
world
images
but
they
also
fail
in
ways
that
strongly
suggest
their
vision
systems
aren’t
entirely
human-like
the
most
major
problem:
adversarial
examples[4]
examples
which
have
been
specifically
modified
to
fool
the
model
adversarial
examples
would
be
a
non-issue
if
the
only
tampered
ones
that
caused
the
models
to
fail
were
ones
that
even
humans
would
notice
the
problem
is
the
models
are
susceptible
to
attacks
by
samples
which
have
only
been
tampered
with
ever
so
slightly
and
would
clearly
not
fool
any
human
this
opens
the
door
for
models
to
silently
fail
which
can
be
pretty
dangerous
for
a
wide
range
of
applications
from
self-driving
cars
to
healthcare
robustness
against
adversarial
attacks
is
currently
a
highly
active
area
of
research
the
subject
of
many
papers
and
even
competitions
and
solutions
will
certainly
improve
cnn
architectures
to
become
safer
and
more
reliable
cnns
were
the
models
that
allowed
computer
vision
to
scale
from
simple
applications
to
powering
sophisticated
products
and
services
ranging
from
face
detection
in
your
photo
gallery
to
making
better
medical
diagnoses
they
might
be
the
key
method
in
computer
vision
going
forward
or
some
other
new
breakthrough
might
just
be
around
the
corner
regardless
one
thing
is
for
sure:
they’re
nothing
short
of
amazing
at
the
heart
of
many
present-day
innovative
applications
and
are
most
certainly
worth
deeply
understanding
hope
you
enjoyed
this
article!
if
you’d
like
to
stay
connected
you’ll
find
me
on
twitter
here
if
you
have
a
question
comments
are
welcome!
—
i
find
them
to
be
useful
to
my
own
learning
process
as
well
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
curious
programmer
tinkers
around
in
python
and
deep
learning
sharing
concepts
ideas
and
codes
""
there
is
an
ongoing
debate
about
whether
or
not
designers
should
write
code
wherever
you
fall
on
this
issue
most
people
would
agree
that
designers
should
know
about
code
this
helps
designers
understand
constraints
and
empathize
with
developers
it
also
allows
designers
to
think
outside
of
the
pixel
perfect
box
when
problem
solving
for
the
same
reasons
designers
should
know
about
machine
learning
put
simply
machine
learning
is
a
field
of
study
that
gives
computers
the
ability
to
learn
without
being
explicitly
programmed
arthur
samuel
1959
even
though
arthur
samuel
coined
the
term
over
fifty
years
ago
only
recently
have
we
seen
the
most
exciting
applications
of
machine
learning
—
digital
assistants
autonomous
driving
and
spam-free
email
all
exist
thanks
to
machine
learning
over
the
past
decade
new
algorithms
better
hardware
and
more
data
have
made
machine
learning
an
order
of
magnitude
more
effective
only
in
the
past
few
years
companies
like
google
amazon
and
apple
have
made
some
of
their
powerful
machine
learning
tools
available
to
developers
now
is
the
best
time
to
learn
about
machine
learning
and
apply
it
to
the
products
you
are
building
since
machine
learning
is
now
more
accessible
than
ever
before
designers
today
have
the
opportunity
to
think
about
how
machine
learning
can
be
applied
to
improve
their
products
designers
should
be
able
to
talk
with
software
developers
about
what
is
possible
how
to
prepare
and
what
outcomes
to
expect
below
are
a
few
example
applications
that
should
serve
as
inspiration
for
these
conversations
machine
learning
can
help
create
user-centric
products
by
personalizing
experiences
to
the
individuals
who
use
them
this
allows
us
to
improve
things
like
recommendations
search
results
notifications
and
ads
machine
learning
is
effective
at
finding
abnormal
content
credit
card
companies
use
this
to
detect
fraud
email
providers
use
this
to
detect
spam
and
social
media
companies
use
this
to
detect
things
like
hate
speech
machine
learning
has
enabled
computers
to
begin
to
understand
the
things
we
say
natural-language
processing
and
the
things
we
see
computer
vision
this
allows
siri
to
understand
siri
set
a
reminder
google
photos
to
create
albums
of
your
dog
and
facebook
to
describe
a
photo
to
those
visually
impaired
machine
learning
is
also
helpful
in
understanding
how
users
are
grouped
this
insight
can
then
be
used
to
look
at
analytics
on
a
group-by-group
basis
from
here
different
features
can
be
evaluated
across
groups
or
be
rolled
out
to
only
a
particular
group
of
users
machine
learning
allows
us
to
make
predictions
about
how
a
user
might
behave
next
knowing
this
we
can
help
prepare
for
a
user’s
next
action
for
example
if
we
can
predict
what
content
a
user
is
planning
on
viewing
we
can
preload
that
content
so
it’s
immediately
ready
when
they
want
it
depending
on
the
application
and
what
data
is
available
there
are
different
types
of
machine
learning
algorithms
to
choose
from
i’ll
briefly
cover
each
of
the
following
supervised
learning
allows
us
to
make
predictions
using
correctly
labeled
data
labeled
data
is
a
group
of
examples
that
has
informative
tags
or
outputs
for
example
photos
with
associated
hashtags
or
a
house’s
features
eq
number
of
bedrooms
location
and
its
price
by
using
supervised
learning
we
can
fit
a
line
to
the
labelled
data
that
either
splits
the
data
into
categories
or
represents
the
trend
of
the
data
using
this
line
we
are
able
to
make
predictions
on
new
data
for
example
we
can
look
at
new
photos
and
predict
hashtags
or
look
at
a
new
house’s
features
and
predict
its
price
if
the
output
we
are
trying
to
predict
is
a
list
of
tags
or
values
we
call
it
classification
if
the
output
we
are
trying
to
predict
is
a
number
we
call
it
regression
unsupervised
learning
is
helpful
when
we
have
unlabeled
data
or
we
are
not
exactly
sure
what
outputs
like
an
image’s
hashtags
or
a
house’s
price
are
meaningful
instead
we
can
identify
patterns
among
unlabeled
data
for
example
we
can
identify
related
items
on
an
e-commerce
website
or
recommend
items
to
someone
based
on
others
who
made
similar
purchases
if
the
pattern
is
a
group
we
call
it
a
cluster
if
the
pattern
is
a
rule
eq
if
this
then
that
we
call
it
an
association
reinforcement
learning
doesn’t
use
an
existing
data
set
instead
we
create
an
agent
to
collect
its
own
data
through
trial-and-error
in
an
environment
where
it
is
reinforced
with
a
reward
for
example
an
agent
can
learn
to
play
mario
by
receiving
a
positive
reward
for
collecting
coins
and
a
negative
reward
for
walking
into
a
goomba
reinforcement
learning
is
inspired
by
the
way
that
humans
learn
and
has
turned
out
to
be
an
effective
way
to
teach
computers
specifically
reinforcement
has
been
effective
at
training
computers
to
play
games
like
go
and
dota
understanding
the
problem
you
are
trying
to
solve
and
the
available
data
will
constrain
the
types
of
machine
learning
you
can
use
eq
identifying
objects
in
an
image
with
supervised
learning
requires
a
labeled
data
set
of
images
however
constraints
are
the
fruit
of
creativity
in
some
cases
you
can
set
out
to
collect
data
that
is
not
already
available
or
consider
other
approaches
even
though
machine
learning
is
a
science
it
comes
with
a
margin
of
error
it
is
important
to
consider
how
a
user’s
experience
might
be
impacted
by
this
margin
of
error
for
example
when
an
autonomous
car
fails
to
recognize
its
surroundings
people
can
get
hurt
even
though
machine
learning
has
never
been
as
accessible
as
it
is
today
it
still
requires
additional
resources
developers
and
time
to
be
integrated
into
a
product
this
makes
it
important
to
think
about
whether
the
resulting
impact
justifies
the
amount
of
resources
needed
to
implement
we
have
barely
covered
the
tip
of
the
iceberg
but
hopefully
at
this
point
you
feel
more
comfortable
thinking
about
how
machine
learning
can
be
applied
to
your
product
if
you
are
interested
in
learning
more
about
machine
learning
here
are
some
helpful
resources:
thanks
for
reading
chat
with
me
on
twitter
@samueldrozdov
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
digital
product
designer
samueldrozdovcom
curated
stories
on
user
experience
usability
and
product
design
by
@fabriciot
and
@caioab
""
data
science
interviews
certainly
aren’t
easy
i
know
this
first
hand
i’ve
participated
in
over
50
individual
interviews
and
phone
screens
while
applying
for
competitive
internships
over
the
last
calendar
year
through
this
exciting
and
somewhat
at
times
very
painful
process
i’ve
accumulated
a
plethora
of
useful
resources
that
helped
me
prepare
for
and
eventually
pass
data
science
interviews
long
story
short
i’ve
decided
to
sort
through
all
my
bookmarks
and
notes
in
order
to
deliver
a
comprehensive
list
of
data
science
resources
with
this
list
by
your
side
you
should
have
more
than
enough
effective
tools
at
your
disposal
next
time
you’re
prepping
for
a
big
interview
it’s
worth
noting
that
many
of
these
resources
are
naturally
going
to
geared
towards
entry-level
and
intern
data
science
positions
as
that’s
where
my
expertise
lies
keep
that
in
mind
and
enjoy!
here’s
some
of
the
more
general
resources
covering
data
science
as
a
whole
specifically
i
highly
recommend
checking
out
the
first
two
links
regarding
120
data
science
interview
questions
while
the
ebook
itself
is
a
couple
bucks
out
of
pocket
the
answers
themselves
are
free
on
quora
these
were
some
of
my
favorite
full-coverage
questions
to
practice
with
right
before
an
interview
even
data
scientists
cannot
escape
the
dreaded
algorithmic
coding
interview
in
my
experience
this
isn’t
the
case
100%
of
the
time
but
chances
are
you’ll
be
asked
to
work
through
something
similar
to
an
easy
or
medium
question
on
leetcode
or
hackerrank
as
far
as
language
goes
most
companies
will
let
you
use
whatever
language
you
want
personally
i
did
almost
all
of
my
algorithmic
coding
in
java
even
though
the
positions
were
targeted
at
python
and
r
programmers
if
i
had
to
recommend
one
thing
it’s
to
break
out
your
wallet
and
invest
in
cracking
the
coding
interview
it
absolutely
lives
up
to
the
hype
i
plan
to
continue
using
it
for
years
to
come
once
the
interviewer
knows
that
you
can
think-through
problems
and
code
effectively
chances
are
that
you’ll
move
onto
some
more
data
science
specific
applications
depending
on
the
interviewer
and
the
position
you
will
likely
be
able
to
choose
between
python
and
r
as
your
tool
of
choice
since
i’m
partial
to
python
my
resources
below
will
primarily
focus
on
effectively
using
pandas
and
numpy
for
data
analysis
a
data
science
interview
typically
isn’t
complete
without
checking
your
knowledge
of
sql
this
can
be
done
over
the
phone
or
through
a
live
coding
question
more
likely
the
latter
i’ve
found
that
the
difficulty
level
of
these
questions
can
vary
a
good
bit
ranging
from
being
painfully
easy
to
requiring
complex
joins
and
obscure
functions
our
good
friend
statistics
is
still
crucial
for
data
scientists
and
it’s
reflected
as
such
in
interviews
i
had
many
interviews
begin
by
seeing
if
i
can
explain
a
common
statistics
or
probability
concept
in
simple
and
concise
terms
as
positions
get
more
experienced
i
suspect
this
happens
less
and
less
as
traditional
statistical
questions
begin
to
take
the
more
practical
form
of
ab
testing
scenarios
covered
later
in
the
post
you’ll
notice
that
i’ve
compiled
a
few
more
resources
here
than
in
other
sections
this
isn’t
a
mistake
machine
learning
is
a
complex
field
that
is
a
virtual
guarantee
in
data
science
interviews
today
the
way
that
you’ll
be
tested
on
this
is
no
guarantee
however
it
may
come
up
as
a
conceptual
question
regarding
cross
validation
or
bias-variance
tradeoff
or
it
may
take
the
form
of
a
take
home
assignment
with
a
dataset
attached
i’ve
seen
both
several
times
so
you’ve
got
to
be
prepared
for
anything
specifically
check
out
the
machine
learning
flashcards
below
they’re
only
a
couple
bucks
and
were
my
by
far
my
favorite
way
to
quiz
myself
on
any
conceptual
ml
stuff
this
won’t
be
covered
in
every
single
data
science
interview
but
it’s
certainly
not
uncommon
most
interviews
will
have
atleast
one
section
solely
dedicated
to
product
thinking
which
often
lends
itself
to
ab
testing
of
some
sort
make
sure
your
familiar
with
the
concepts
and
statistical
background
necessary
in
order
to
be
prepared
when
it
comes
up
if
you
have
time
to
spare
i
took
the
free
online
course
by
udacity
and
overall
i
was
pretty
impressed
lastly
i
wanted
to
call
out
all
of
the
posts
related
to
data
science
jobs
and
interviewing
that
i
read
over
and
over
again
to
understand
not
only
how
to
prepare
but
what
to
expect
as
well
if
you
only
check
out
one
section
here
this
is
the
one
to
focus
on
this
is
the
layer
that
sits
on
top
of
all
the
technical
skills
and
application
don’t
overlook
it
i
hope
you
find
these
resources
useful
during
your
next
interview
or
job
search
i
know
i
did
truthfully
i’m
just
glad
that
i
saved
these
links
somewhere
lastly
this
post
is
part
of
an
ongoing
initiative
to
‘open-source’
my
experience
applying
and
interviewing
at
data
science
positions
so
if
you
enjoyed
this
content
then
be
sure
to
follow
me
for
more
stuff
like
this
if
you’re
interested
in
receiving
my
weekly
rundown
of
interesting
articles
and
resources
focused
on
data
science
machine
learning
and
artificial
intelligence
then
subscribe
to
self
driven
data
science
using
the
form
below!
if
you
enjoyed
this
post
feel
free
to
hit
the
clap
button
and
if
you’re
interested
in
posts
to
come
make
sure
to
follow
me
on
medium
at
the
link
below
—
i’ll
be
writing
and
shipping
every
day
this
month
as
part
of
a
30-day
challenge
this
article
was
originally
published
on
conordeweycom
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
data
scientist
""
writer
|
wwwconordeweycom
sharing
concepts
ideas
and
codes
""
information
theory
is
an
important
field
that
has
made
significant
contribution
to
deep
learning
and
ai
and
yet
is
unknown
to
many
information
theory
can
be
seen
as
a
sophisticated
amalgamation
of
basic
building
blocks
of
deep
learning:
calculus
probability
and
statistics
some
examples
of
concepts
in
ai
that
come
from
information
theory
or
related
fields:
in
the
early
20th
century
scientists
and
engineers
were
struggling
with
the
question:
how
to
quantify
the
information?
is
there
a
analytical
way
or
a
mathematical
measure
that
can
tell
us
about
the
information
content?
for
example
consider
below
two
sentences:
it
is
not
difficult
to
tell
that
the
second
sentence
gives
us
more
information
since
it
also
tells
that
bruno
is
big
and
brown
in
addition
to
being
a
dog
how
can
we
quantify
the
difference
between
two
sentences?
can
we
have
a
mathematical
measure
that
tells
us
how
much
more
information
second
sentence
have
as
compared
to
the
first?
scientists
were
struggling
with
these
questions
semantics
domain
and
form
of
data
only
added
to
the
complexity
of
the
problem
then
mathematician
and
engineer
claude
shannon
came
up
with
the
idea
of
entropy
that
changed
our
world
forever
and
marked
the
beginning
of
digital
information
age
shannon
proposed
that
the
semantic
aspects
of
data
are
irrelevant
and
nature
and
meaning
of
data
doesn’t
matter
when
it
comes
to
information
content
instead
he
quantified
information
in
terms
of
probability
distribution
and
uncertainty
shannon
also
introduced
the
term
bit
that
he
humbly
credited
to
his
colleague
john
tukey
this
revolutionary
idea
not
only
laid
the
foundation
of
information
theory
but
also
opened
new
avenues
for
progress
in
fields
like
artificial
intelligence
below
we
discuss
four
popular
widely
used
and
must
known
information
theoretic
concepts
in
deep
learning
and
data
sciences:
also
called
information
entropy
or
shannon
entropy
entropy
gives
a
measure
of
uncertainty
in
an
experiment
let’s
consider
two
experiments:
if
we
compare
the
two
experiments
in
exp
2
it
is
easier
to
predict
the
outcome
as
compared
to
exp
1
so
we
can
say
that
exp
1
is
inherently
more
uncertainunpredictable
than
exp
2
this
uncertainty
in
the
experiment
is
measured
using
entropy
therefore
if
there
is
more
inherent
uncertainty
in
the
experiment
then
it
has
higher
entropy
or
lesser
the
experiment
is
predictable
more
is
the
entropy
the
probability
distribution
of
experiment
is
used
to
calculate
the
entropy
a
deterministic
experiment
which
is
completely
predictable
say
tossing
a
coin
with
ph=1
has
entropy
zero
an
experiment
which
is
completely
random
say
rolling
fair
dice
is
least
predictable
has
maximum
uncertainty
and
has
the
highest
entropy
among
such
experiments
another
way
to
look
at
entropy
is
the
average
information
gained
when
we
observe
outcomes
of
an
random
experiment
the
information
gained
for
a
outcome
of
an
experiment
is
defined
as
a
function
of
probability
of
occurrence
of
that
outcome
more
the
rarer
is
the
outcome
more
is
the
information
gained
from
observing
it
for
example
in
an
deterministic
experiment
we
always
know
the
outcome
so
no
new
information
gained
is
here
from
observing
the
outcome
and
hence
entropy
is
zero
for
a
discrete
random
variable
x
with
possible
outcomes
states
x_1x_n
the
entropy
in
unit
of
bits
is
defined
as:
where
px_i
is
the
probability
of
i^th
outcome
of
x
cross
entropy
is
used
to
compare
two
probability
distributions
it
tells
us
how
similar
two
distributions
are
cross
entropy
between
two
probability
distributions
p
and
q
defined
over
same
set
of
outcomes
is
given
by:
mutual
information
is
a
measure
of
mutual
dependency
between
two
probability
distributions
or
random
variables
it
tells
us
how
much
information
about
one
variable
is
carried
by
the
another
variable
mutual
information
captures
dependency
between
random
variables
and
is
more
generalized
than
vanilla
correlation
coefficient
which
captures
only
the
linear
relationship
mutual
information
of
two
discrete
random
variables
x
and
y
is
defined
as:
where
pxy
is
the
joint
probability
distribution
of
x
and
y
and
px
and
py
are
the
marginal
probability
distribution
of
x
and
y
respectively
also
called
relative
entropy
kl
divergence
is
another
measure
to
find
similarities
between
two
probability
distributions
it
measures
how
much
one
distribution
diverges
from
the
other
suppose
we
have
some
data
and
true
distribution
underlying
it
is
‘p’
but
we
don’t
know
this
‘p’
so
we
choose
a
new
distribution
‘q’
to
approximate
this
data
since
‘q’
is
just
an
approximation
it
won’t
be
able
to
approximate
the
data
as
good
as
‘p’
and
some
information
loss
will
occur
this
information
loss
is
given
by
kl
divergence
kl
divergence
between
‘p’
and
‘q’
tells
us
how
much
information
we
lose
when
we
try
to
approximate
data
given
by
‘p’
with
‘q’
kl
divergence
of
a
probability
distribution
q
from
another
probability
distribution
p
is
defined
as:
kl
divergence
is
commonly
used
in
unsupervised
machine
learning
technique
variational
autoencoders
information
theory
was
originally
formulated
by
mathematician
and
electrical
engineer
claude
shannon
in
his
seminal
paper
a
mathematical
theory
of
communication
in
1948
note:
terms
experiments
random
variable
""
ai
machine
learning
deep
learning
data
science
have
been
used
loosely
above
but
have
technically
different
meanings
in
case
you
liked
the
article
do
follow
me
abhishek
parbhakar
for
more
articles
related
to
ai
philosophy
and
economics
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
finding
equilibria
among
ai
philosophy
and
economics
sharing
concepts
ideas
and
codes
""
over
the
past
8
months
i’ve
been
interviewing
at
various
companies
like
google’s
deepmind
wadhwani
institute
of
ai
microsoft
ola
fractal
analytics
and
a
few
others
primarily
for
the
roles
—
data
scientist
software
engineer
""
research
engineer
in
the
process
not
only
did
i
get
an
opportunity
to
interact
with
many
great
minds
but
also
had
a
peek
at
myself
along
with
a
sense
of
what
people
really
look
for
when
interviewing
someone
i
believe
that
if
i’d
had
this
knowledge
before
i
could
have
avoided
many
mistakes
and
have
prepared
in
a
much
better
manner
which
is
what
the
motivation
behind
this
post
is
to
be
able
to
help
someone
bag
their
dream
place
of
work
this
post
arose
from
a
discussion
with
one
of
my
juniors
on
the
lack
of
really
fulfilling
job
opportunities
offered
through
campus
placements
for
people
working
in
ai
also
when
i
was
preparing
i
noticed
people
using
a
lot
of
resources
but
as
per
my
experience
over
the
past
months
i
realised
that
one
can
do
away
with
a
few
minimal
ones
for
most
roles
in
ai
all
of
which
i’m
going
to
mention
at
the
end
of
the
post
i
begin
with
how
to
get
noticed
aka
the
interview
then
i
provide
a
list
of
companies
and
start-ups
to
apply
which
is
followed
by
how
to
ace
that
interview
based
on
whatever
experience
i’ve
had
i
add
a
section
on
what
we
should
strive
to
work
for
i
conclude
with
minimal
resources
you
need
for
preparation
note:
for
people
who
are
sitting
for
campus
placements
there
are
two
things
i’d
like
to
add
firstly
most
of
what
i’m
going
to
say
except
for
the
last
one
maybe
is
not
going
to
be
relevant
to
you
for
placements
but
and
this
is
my
second
point
as
i
mentioned
before
opportunities
on
campus
are
mostly
in
software
engineering
roles
having
no
intersection
with
ai
so
this
post
is
specifically
meant
for
people
who
want
to
work
on
solving
interesting
problems
using
ai
also
i
want
to
add
that
i
haven’t
cleared
all
of
these
interviews
but
i
guess
that’s
the
essence
of
failure
—
it’s
the
greatest
teacher!
the
things
that
i
mention
here
may
not
all
be
useful
but
these
are
things
that
i
did
and
there’s
no
way
for
me
to
know
what
might
have
ended
up
making
my
case
stronger
to
be
honest
this
step
is
the
most
important
one
what
makes
off-campus
placements
so
tough
and
exhausting
is
getting
the
recruiter
to
actually
go
through
your
profile
among
the
plethora
of
applications
that
they
get
having
a
contact
inside
the
organisation
place
a
referral
for
you
would
make
it
quite
easy
but
in
general
this
part
can
be
sub-divided
into
three
keys
steps:
a
do
the
regulatory
preparation
and
do
that
well:
so
with
regulatory
preparation
i
mean
—a
linkedin
profile
a
github
profile
a
portfolio
website
and
a
well-polished
cv
firstly
your
cv
should
be
really
neat
and
concise
follow
this
guide
by
udacity
for
cleaning
up
your
cv
—
resume
revamp
it
has
everything
that
i
intend
to
say
and
i’ve
been
using
it
as
a
reference
guide
myself
as
for
the
cv
template
some
of
the
in-built
formats
on
overleaf
are
quite
nice
i
personally
use
deedy-resume
here’s
a
preview:
as
it
can
be
seen
a
lot
of
content
can
be
fit
into
one
page
however
if
you
really
do
need
more
than
that
then
the
format
linked
above
would
not
work
directly
instead
you
can
find
a
modified
multi-page
format
of
the
same
here
the
next
most
important
thing
to
mention
is
your
github
profile
a
lot
of
people
underestimate
the
potential
of
this
just
because
unlike
linkedin
it
doesn’t
have
a
who
viewed
your
profile
option
people
do
go
through
your
github
because
that’s
the
only
way
they
have
to
validate
what
you
have
mentioned
in
your
cv
given
that
there’s
a
lot
of
noise
today
with
people
associating
all
kinds
of
buzzwords
with
their
profile
especially
for
data
science
open-source
has
a
big
role
to
play
too
with
majority
of
the
tools
implementations
of
various
algorithms
lists
of
learning
resources
all
being
open-sourced
i
discuss
the
benefits
of
getting
involved
in
open-source
and
how
one
can
start
from
scratch
in
an
earlier
post
here
the
bare
minimum
for
now
should
be:
•
create
a
github
account
if
you
don’t
already
have
one•
create
a
repository
for
each
of
the
projects
that
you
have
done•
add
documentation
with
clear
instructions
on
how
to
run
the
code•
add
documentation
for
each
file
mentioning
the
role
of
each
function
the
meaning
of
each
parameter
proper
formatting
eg
pep8
for
python
along
with
a
script
to
automate
the
previous
step
optional
moving
on
the
third
step
is
what
most
people
lack
which
is
having
a
portfolio
website
demonstrating
their
experience
and
personal
projects
making
a
portfolio
indicates
that
you
are
really
serious
about
getting
into
the
field
and
adds
a
lot
of
points
to
the
authenticity
factor
also
you
generally
have
space
constraints
on
your
cv
and
tend
to
miss
out
on
a
lot
of
details
you
can
use
your
portfolio
to
really
delve
deep
into
the
details
if
you
want
to
and
it’s
highly
recommended
to
include
some
sort
of
visualisation
or
demonstration
of
the
projectidea
it’s
really
easy
to
create
one
too
as
there
are
a
lot
of
free
platforms
with
drag-and-drop
features
making
the
process
really
painless
i
personally
use
weebly
which
is
a
widely
used
tool
it’s
better
to
have
a
reference
to
begin
with
there
are
a
lot
of
awesome
ones
out
there
but
i
referred
to
deshraj
yadav’s
personal
website
to
begin
with
making
mine:
finally
a
lot
of
recruiters
and
start-ups
have
nowadays
started
using
linkedin
as
their
go-to
platform
for
hiring
a
lot
of
good
jobs
get
posted
there
apart
from
recruiters
the
people
working
at
influential
positions
are
quite
active
there
as
well
so
if
you
can
grab
their
attention
you
have
a
good
chance
of
getting
in
too
apart
from
that
maintaining
a
clean
profile
is
necessary
for
people
to
have
the
will
to
connect
with
you
an
important
part
of
linkedin
is
their
search
tool
and
for
you
to
show
up
you
must
have
the
relevant
keywords
interspersed
over
your
profile
it
took
me
a
lot
of
iterations
and
re-evaluations
to
finally
have
a
decent
one
also
you
should
definitely
ask
people
with
or
under
whom
you’ve
worked
with
to
endorse
you
for
your
skills
and
add
a
recommendation
talking
about
their
experience
of
working
with
you
all
of
this
increases
your
chance
of
actually
getting
noticed
i’ll
again
point
towards
udacity’s
guide
for
linkedin
and
github
profiles
all
this
might
seem
like
a
lot
but
remember
that
you
don’t
need
to
do
it
in
a
single
day
or
even
a
week
or
a
month
it’s
a
process
it
never
ends
setting
up
everything
at
first
would
definitely
take
some
effort
but
once
it’s
there
and
you
keep
updating
it
regularly
as
events
around
you
keep
happening
you’ll
not
only
find
it
to
be
quite
easy
but
also
you’ll
be
able
to
talk
about
yourself
anywhere
anytime
without
having
to
explicitly
prepare
for
it
because
you
become
so
aware
about
yourself
b
stay
authentic:
i’ve
seen
a
lot
of
people
do
this
mistake
of
presenting
themselves
as
per
different
job
profiles
according
to
me
it’s
always
better
to
first
decide
what
actually
interests
you
what
would
you
be
happy
doing
and
then
search
for
relevant
opportunities
not
the
other
way
round
the
fact
that
the
demand
for
ai
talent
surpasses
the
supply
for
the
same
gives
you
this
opportunity
spending
time
on
your
regulatory
preparation
mentioned
above
would
give
you
an
all-around
perspective
on
yourself
and
help
make
this
decision
easier
also
you
won’t
need
to
prepare
answers
to
various
kinds
of
questions
that
you
get
asked
during
an
interview
most
of
them
would
come
out
naturally
as
you’d
be
talking
about
something
you
really
care
about
c
networking:
once
you’re
done
with
a
figured
out
b
networking
is
what
will
actually
help
you
get
there
if
you
don’t
talk
to
people
you
miss
out
on
hearing
about
many
opportunities
that
you
might
have
a
good
shot
at
it’s
important
to
keep
connecting
with
new
people
each
day
if
not
physically
then
on
linkedin
so
that
upon
compounding
it
after
many
days
you
have
a
large
and
strong
network
networking
is
not
messaging
people
to
place
a
referral
for
you
when
i
was
starting
off
i
did
this
mistake
way
too
often
until
i
stumbled
upon
this
excellent
article
by
mark
meloon
where
he
talks
about
the
importance
of
building
a
real
connection
with
people
by
offering
our
help
first
another
important
step
in
networking
is
to
get
your
content
out
for
example
if
you’re
good
at
something
blog
about
it
and
share
that
blog
on
facebook
and
linkedin
not
only
does
this
help
others
it
helps
you
as
well
once
you
have
a
good
enough
network
your
visibility
increases
multi-fold
you
never
know
how
one
person
from
your
network
liking
or
commenting
on
your
posts
may
help
you
reach
out
to
a
much
broader
audience
including
people
who
might
be
looking
for
someone
of
your
expertise
i’m
presenting
this
list
in
alphabetical
order
to
avoid
the
misinterpretation
of
any
specific
preference
however
i
do
place
a
*
on
the
ones
that
i’d
personally
recommend
this
recommendation
is
based
on
either
of
the
following:
mission
statement
people
personal
interaction
or
scope
of
learning
more
than
1
*
is
purely
based
on
the
2nd
and
3rd
factors
your
interview
begins
the
moment
you
have
entered
the
room
and
a
lot
of
things
can
happen
between
that
moment
and
the
time
when
you’re
asked
to
introduce
yourself
—
your
body
language
and
the
fact
that
you’re
smiling
while
greeting
them
plays
a
big
role
especially
when
you’re
interviewing
for
a
start-up
as
culture-fit
is
something
that
they
extremely
care
about
you
need
to
understand
that
as
much
as
the
interviewer
is
a
stranger
to
you
you’re
a
stranger
to
himher
too
so
they’re
probably
just
as
nervous
as
you
are
it’s
important
to
view
the
interview
as
more
of
a
conversation
between
yourself
and
the
interviewer
both
of
you
are
looking
for
a
mutual
fit
—
you
are
looking
for
an
awesome
place
to
work
at
and
the
interviewer
is
looking
for
an
awesome
person
like
you
to
work
with
so
make
sure
that
you’re
feeling
good
about
yourself
and
that
you
take
the
charge
of
making
the
initial
moments
of
your
conversation
pleasant
for
them
and
the
easiest
way
i
know
how
to
make
that
happen
is
to
smile
there
are
mostly
two
types
of
interviews
—
one
where
the
interviewer
has
come
with
come
prepared
set
of
questions
and
is
going
to
just
ask
you
just
that
irrespective
of
your
profile
and
the
second
where
the
interview
is
based
on
your
cv
i’ll
start
with
the
second
one
this
kind
of
interview
generally
begins
with
a
can
you
tell
me
a
bit
about
yourself?
at
this
point
2
things
are
a
big
no
—
talking
about
your
gpa
in
college
and
talking
about
your
projects
in
detail
an
ideal
statement
should
be
about
a
minute
or
two
long
should
give
a
good
idea
on
what
have
you
been
doing
till
now
and
it’s
not
restricted
to
academics
you
can
talk
about
your
hobbies
like
reading
books
playing
sports
meditation
etc
—
basically
anything
that
contributes
to
defining
you
the
interviewer
will
then
take
something
that
you
talk
about
here
as
a
cue
for
his
next
question
and
then
the
technical
part
of
the
interview
begins
the
motive
of
this
kind
of
interview
is
to
really
check
whether
whatever
you
have
written
on
your
cv
is
true
or
not:
there
would
be
a
lot
of
questions
on
what
could
be
done
differently
or
if
x
was
used
instead
of
y
what
would
have
happened
at
this
point
it’s
important
to
know
the
kind
of
trade-offs
that
is
usually
made
during
implementation
for
eg
if
the
interviewer
says
that
using
a
more
complex
model
would
have
given
better
results
then
you
might
say
that
you
actually
had
less
data
to
work
with
and
that
would
have
lead
to
overfitting
in
one
of
the
interviews
i
was
given
a
case-study
to
work
on
and
it
involved
designing
algorithms
for
a
real-world
use
case
i’ve
noticed
that
once
i’ve
been
given
the
green
flag
to
talk
about
a
project
the
interviewers
really
like
it
when
i
talk
about
it
in
the
following
flow:
problem
>
1
or
2
previous
approaches
>
our
approach
>
result
>
intuition
the
other
kind
of
interview
is
really
just
to
test
your
basic
knowledge
don’t
expect
those
questions
to
be
too
hard
but
they
would
definitely
scratch
every
bit
of
the
basics
that
you
should
be
having
mainly
based
around
linear
algebra
probability
statistics
optimisation
machine
learning
andor
deep
learning
the
resources
mentioned
in
the
minimal
resources
you
need
for
preparation
section
should
suffice
but
make
sure
that
you
don’t
miss
out
one
bit
among
them
the
catch
here
is
the
amount
of
time
you
take
to
answer
those
questions
since
these
cover
the
basics
they
expect
that
you
should
be
answering
them
almost
instantly
so
do
your
preparation
accordingly
throughout
the
process
it’s
important
to
be
confident
and
honest
about
what
you
know
and
what
you
don’t
know
if
there’s
a
question
that
you’re
certain
you
have
no
idea
about
say
it
upfront
rather
than
making
aah
um
sounds
if
some
concept
is
really
important
but
you
are
struggling
with
answering
it
the
interviewer
would
generally
depending
on
how
you
did
in
the
initial
parts
be
happy
to
give
you
a
hint
or
guide
you
towards
the
right
solution
it’s
a
big
plus
if
you
manage
to
pick
their
hints
and
arrive
at
the
correct
solution
try
to
not
get
nervous
and
the
best
way
to
avoid
that
is
by
again
smiling
now
we
come
to
the
conclusion
of
the
interview
where
the
interviewer
would
ask
you
if
you
have
any
questions
for
them
it’s
really
easy
to
think
that
your
interview
is
done
and
just
say
that
you
have
nothing
to
ask
i
know
many
people
who
got
rejected
just
because
of
failing
at
this
last
question
as
i
mentioned
before
it’s
not
only
you
who
is
being
interviewed
you
are
also
looking
for
a
mutual
fit
with
the
company
itself
so
it’s
quite
obvious
that
if
you
really
want
to
join
a
place
you
must
have
many
questions
regarding
the
work
culture
there
or
what
kind
of
role
are
they
seeing
you
in
it
can
be
as
simple
as
being
curious
about
the
person
interviewing
you
there’s
always
something
to
learn
from
everything
around
you
and
you
should
make
sure
that
you
leave
the
interviewer
with
the
impression
that
you’re
truly
interested
in
being
a
part
of
their
team
a
final
question
that
i’ve
started
asking
all
my
interviewers
is
for
a
feedback
on
what
they
might
want
me
to
improve
on
this
has
helped
me
tremendously
and
i
still
remember
every
feedback
that
i’ve
gotten
which
i’ve
incorporated
into
my
daily
life
that’s
it
based
on
my
experience
if
you’re
just
honest
about
yourself
are
competent
truly
care
about
the
company
you’re
interviewing
for
and
have
the
right
mindset
you
should
have
ticked
all
the
right
boxes
and
should
be
getting
a
congratulatory
mail
soon
😄
we
live
in
an
era
full
of
opportunities
and
that
applies
to
anything
that
you
love
you
just
need
to
strive
to
become
the
best
at
it
and
you
will
find
a
way
to
monetise
it
as
gary
vaynerchuk
just
follow
him
already
says:
this
is
a
great
time
to
be
working
in
ai
and
if
you’re
truly
passionate
about
it
you
have
so
much
that
you
can
do
with
ai
you
can
empower
so
many
people
that
have
always
been
under-represented
we
keep
nagging
about
the
problems
surrounding
us
but
there’s
been
never
such
a
time
where
common
people
like
us
can
actually
do
something
about
those
problems
rather
than
just
complaining
jeffrey
hammerbacher
founder
cloudera
had
famously
said:
we
can
do
so
much
with
ai
than
we
can
ever
imagine
there
are
many
extremely
challenging
problems
out
there
which
require
incredibly
smart
people
like
you
to
put
your
head
down
on
and
solve
you
can
make
many
lives
better
time
to
let
go
of
what
is
cool
or
what
would
look
good
think
and
choose
wisely
any
data
science
interview
comprises
of
questions
mostly
of
a
subset
of
the
following
four
categories:
computer
science
math
statistics
and
machine
learning
if
you’re
not
familiar
with
the
math
behind
deep
learning
then
you
should
consider
going
over
my
last
post
for
resources
to
understand
them
however
if
you
are
comfortable
i’ve
found
that
the
chapters
2
3
and
4
of
the
deep
learning
book
are
enough
to
preparerevise
for
theoretical
questions
during
such
interviews
i’ve
been
preparing
summaries
for
a
few
chapters
which
you
can
refer
to
where
i’ve
tried
to
even
explain
a
few
concepts
that
i
found
challenging
to
understand
at
first
in
case
you
are
not
willing
to
go
through
the
entire
chapters
and
if
you’ve
already
done
a
course
on
probability
you
should
be
comfortable
answering
a
few
numerical
as
well
for
stats
covering
these
topics
should
be
enough
now
the
range
of
questions
here
can
vary
depending
on
the
type
of
position
you
are
applying
for
if
it’s
a
more
traditional
machine
learning
based
interview
where
they
want
to
check
your
basic
knowledge
in
ml
you
can
complete
any
one
of
the
following
courses:-
machine
learning
by
andrew
ng
—
cs
229-
machine
learning
course
by
caltech
professor
yaser
abu-mostafa
important
topics
are:
supervised
learning
classification
regression
svm
decision
tree
random
forests
logistic
regression
multi-layer
perceptron
parameter
estimation
bayes’
decision
rule
unsupervised
learning
k-means
clustering
gaussian
mixture
models
dimensionality
reduction
pca
now
if
you’re
applying
for
a
more
advanced
position
there’s
a
high
chance
that
you
might
be
questioned
on
deep
learning
in
that
case
you
should
be
very
comfortable
with
convolutional
neural
networks
cnns
andor
depending
upon
what
you’ve
worked
on
recurrent
neural
networks
rnns
and
their
variants
and
by
being
comfortable
you
must
know
what
is
the
fundamental
idea
behind
deep
learning
how
cnnsrnns
actually
worked
what
kind
of
architectures
have
been
proposed
and
what
has
been
the
motivation
behind
those
architectural
changes
now
there’s
no
shortcut
for
this
either
you
understand
them
or
you
put
enough
time
to
understand
them
for
cnns
the
recommended
resource
is
stanford’s
cs
231n
and
cs
224n
for
rnns
i
found
this
neural
network
class
by
hugo
larochelle
to
be
really
enlightening
too
refer
this
for
a
quick
refresher
too
udacity
coming
to
the
aid
here
too
by
now
you
should
have
figured
out
that
udacity
is
a
really
important
place
for
an
ml
practitioner
there
are
not
a
lot
of
places
working
on
reinforcement
learning
rl
in
india
and
i
too
am
not
experienced
in
rl
as
of
now
so
that’s
one
thing
to
add
to
this
post
sometime
in
the
future
getting
placed
off-campus
is
a
long
journey
of
self-realisation
i
realise
that
this
has
been
another
long
post
and
i’m
again
extremely
grateful
to
you
for
valuing
my
thoughts
i
hope
that
this
post
finds
a
way
of
being
useful
to
you
and
that
it
helped
you
in
some
way
to
prepare
for
your
next
data
science
interview
better
if
it
did
i
request
you
to
really
think
about
what
i
talk
about
in
what
we
should
strive
to
work
for
i’m
very
thankful
to
my
friends
from
iit
guwahati
for
their
helpful
feedback
especially
ameya
godbole
kothapalli
vignesh
and
prabal
jain
a
majority
of
what
i
mention
here
like
viewing
an
interview
as
a
conversation
and
seeking
feedback
from
our
interviewers
arose
from
multiple
discussions
with
prabal
who
has
been
advising
me
constantly
on
how
i
can
improve
my
interviewing
skills
this
story
is
published
in
noteworthy
where
thousands
come
every
day
to
learn
about
the
people
""
ideas
shaping
the
products
we
love
follow
our
publication
to
see
more
product
""
design
stories
featured
by
the
journal
team
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ai
fanatic
•
math
lover
•
dreamer
the
official
journal
blog
""
last
year
i
published
the
article
from
ballerina
to
ai
writer
where
i
described
how
i
embraced
the
technical
part
of
ai
without
a
technical
background
but
having
love
and
passion
for
ai
i
educated
myself
and
was
able
to
build
a
neural
net
classifier
and
do
projects
in
deep
rl
recently
i’ve
become
a
participant
in
the
openai
scholarship
program
openai
is
a
non-profit
that
gathers
top
ai
researchers
to
ensure
the
safety
of
ai
to
benefit
humanity
every
week
for
the
next
three
months
i’ll
publish
blog
posts
sharing
my
story
of
transformation
from
a
person
dedicated
to
15
years
of
professional
dancing
and
then
writing
about
tech
and
ai
to
actually
conducting
ai
research
finding
your
true
calling
—
the
key
component
of
happiness
my
primary
goal
with
the
series
of
blog
posts
from
ballerina
to
ai
researcher
is
to
show
that
it’s
never
too
late
to
embrace
a
new
field
start
over
again
and
find
your
true
calling
finding
work
you
love
is
one
of
the
most
important
components
of
happiness
-
—
something
that
you
do
every
day
and
invest
your
time
in
to
grow
that
makes
you
feel
fulfilled
gives
you
energy
something
that
is
a
refuge
for
your
soul
great
things
never
come
easy
we
have
to
be
able
to
fight
to
make
great
things
happen
but
you
can’t
fight
for
something
you
don’t
believe
in
especially
if
you
don’t
feel
like
it’s
really
important
for
you
and
humanity
finding
that
thing
is
a
real
challenge
i
feel
lucky
that
i
found
my
true
passion
—
ai
to
me
the
technology
itself
and
the
ai
community
—
researchers
scientists
people
who
dedicate
their
lives
to
building
the
most
powerful
technology
of
all
time
with
the
mission
to
benefit
humanity
and
make
it
safe
for
us
—
is
a
great
source
of
energy
the
structure
of
the
blog
post
series
today
i’m
giving
an
overall
intro
of
what
i’m
going
to
cover
in
my
from
ballerina
to
ai
researcher
series
i’ll
dedicate
the
sequence
of
blog
posts
during
the
openai
scholars
program
to
several
aspects
of
ai
technology
i’ll
cover
those
areas
that
concern
me
a
lot
like
ai
and
automation
bias
in
ml
dual
use
of
ai
etc
also
the
structure
of
my
posts
will
include
some
insights
on
what
i’m
working
on
right
now
the
final
technical
project
will
be
available
by
the
end
of
august
and
will
be
open-sourced
i
feel
very
lucky
to
have
alec
radford
an
experienced
researcher
as
my
mentor
who
guides
me
in
the
nlp
and
nlu
research
area
first
week
of
my
scholarship
i’ve
dedicated
my
first
week
within
the
program
to
learning
about
the
transformer
architecture
that
performs
much
better
on
sequential
data
compared
to
rnns
lstms
the
novelty
of
the
architecture
is
its
multi-head
self-attention
mechanism
according
to
the
original
paper
experiments
with
the
transformer
on
two
machine
translation
tasks
showed
the
model
to
be
superior
in
quality
while
being
more
parallelizable
and
requiring
significantly
less
time
to
train
more
concretely
when
rnns
or
cnns
take
a
sequence
as
an
input
it
goes
through
sentences
word
by
word
which
is
a
huge
obstacle
toward
parallelization
of
the
process
takes
more
time
to
train
models
moreover
if
sequences
are
too
long
the
model
tends
to
forget
the
content
of
distant
positions
in
sequence
or
mixes
it
with
the
following
positions’
content
—
this
is
the
fundamental
problem
in
dealing
with
sequential
data
the
transformer
architecture
reduced
this
problem
thanks
to
the
multi-head
self-attention
mechanism
i
digged
into
rnn
lstm
models
to
catch
up
with
the
background
information
to
that
end
i’ve
found
andrew
ng’s
course
on
deep
learning
along
with
the
papers
extremely
useful
to
develop
insights
regarding
the
transformer
i
went
through
the
following
resources:
the
video
by
łukasz
kaiser
from
google
brain
one
of
the
model’s
creators
a
blog
post
with
very
well
elaborated
content
re:
the
model
ran
the
code
tensor2tensor
and
the
code
using
the
pytorch
framework
from
this
paper
to
feel
the
difference
between
the
tf
and
pytorch
frameworks
overall
the
goal
within
the
program
is
to
develop
deep
comprehension
of
the
nlu
research
area:
challenges
current
state
of
the
art
and
to
formulate
and
test
hypotheses
that
tackle
the
most
important
problems
of
the
field
i’ll
share
more
on
what
i’m
working
on
in
my
future
articles
meanwhile
if
you
have
questionsfeedback
please
leave
a
comment
if
you
want
to
learn
more
about
me
here
are
my
facebook
and
twitter
accounts
i’d
appreciate
your
feedback
on
my
posts
such
as
what
topics
are
most
interesting
to
you
that
i
should
consider
further
coverage
on
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
former
ballerina
turned
ai
writer
fan
of
sci-fi
astrophysics
consciousness
is
the
key
founder
of
buzzrobotcom
the
publication
aims
to
cover
practical
aspects
of
ai
technology
use
cases
along
with
interviews
with
notable
people
in
the
ai
field
""
latent
semantic
analysis
works
on
large-scale
datasets
to
generate
representations
to
discover
the
insights
through
natural
language
processing
there
are
different
approaches
to
perform
the
latent
semantic
analysis
at
multiple
levels
such
as
document
level
phrase
level
and
sentence
level
primarily
semantic
analysis
can
be
summarized
into
lexical
semantics
and
the
study
of
combining
individual
words
into
paragraphs
or
sentences
the
lexical
semantics
classifies
and
decomposes
the
lexical
items
applying
lexical
semantic
structures
has
different
contexts
to
identify
the
differences
and
similarities
between
the
words
a
generic
term
in
a
paragraph
or
a
sentence
is
hypernym
and
hyponymy
provides
the
meaning
of
the
relationship
between
instances
of
the
hyponyms
homonyms
contain
similar
syntax
or
similar
spelling
with
similar
structuring
with
different
meanings
homonyms
are
not
related
to
each
other
book
is
an
example
for
homonym
it
can
mean
for
someone
to
read
something
or
an
act
of
making
a
reservation
with
similar
spelling
form
and
syntax
however
the
definition
is
different
polysemy
is
another
phenomenon
of
the
words
where
a
single
word
could
be
associated
with
multiple
related
senses
and
distinct
meanings
the
word
polysemy
is
a
greek
word
which
means
many
signs
python
provides
nltk
library
to
perform
tokenization
of
the
words
by
chopping
the
words
in
larger
chunks
into
phrases
or
meaningful
strings
processing
words
through
tokenization
produce
tokens
word
lemmatization
converts
words
from
the
current
inflected
form
into
the
base
form
latent
semantic
analysis
applying
latent
semantic
analysis
on
large
datasets
of
text
and
documents
represents
the
contextual
meaning
through
mathematical
and
statistical
computation
methods
on
large
corpus
of
text
many
times
latent
semantic
analysis
overtook
human
scores
and
subject
matter
tests
conducted
by
humans
the
accuracy
of
latent
semantic
analysis
is
high
as
it
reads
through
machine
readable
documents
and
texts
at
a
web
scale
latent
semantic
analysis
is
a
technique
that
applies
singular
value
decomposition
and
principal
component
analysis
pca
the
document
can
be
represented
with
z
x
y
matrix
a
the
rows
of
the
matrix
represent
the
document
in
the
collection
the
matrix
a
can
represent
numerous
hundred
thousands
of
rows
and
columns
on
a
typical
large-corpus
text
document
applying
singular
value
decomposition
develops
a
set
of
operations
dubbed
matrix
decomposition
natural
language
processing
in
python
with
nltk
library
applies
a
low-rank
approximation
to
the
term-document
matrix
later
the
low-rank
approximation
aids
in
indexing
and
retrieving
the
document
known
as
latent
semantic
indexing
by
clustering
the
number
of
words
in
the
document
brief
overview
of
linear
algebra
the
a
with
z
x
y
matrix
contains
the
real-valued
entries
with
non-negative
values
for
the
term-document
matrix
determining
the
rank
of
the
matrix
comes
with
the
number
of
linearly
independent
columns
or
rows
in
the
the
matrix
the
rank
of
a
≤
{zy}
a
square
c
x
c
represented
as
diagonal
matrix
where
off-diagonal
entries
are
zero
examining
the
matrix
if
all
the
c
diagonal
matrices
are
one
the
identity
matrix
of
the
dimension
c
represented
by
ic
for
the
square
z
x
z
matrix
a
with
a
vector
k
which
contains
not
all
zeroes
for
λ
the
matrix
decomposition
applies
on
the
square
matrix
factored
into
the
product
of
matrices
from
eigenvectors
this
allows
to
reduce
the
dimensionality
of
the
words
from
multi-dimensions
to
two
dimensions
to
view
on
the
plot
the
dimensionality
reduction
techniques
with
principal
component
analysis
and
singular
value
decomposition
holds
critical
relevance
in
natural
language
processing
the
zipfian
nature
of
the
frequency
of
the
words
in
a
document
makes
it
difficult
to
determine
the
similarity
of
the
words
in
a
static
stage
hence
eigen
decomposition
is
a
by-product
of
singular
value
decomposition
as
the
input
of
the
document
is
highly
asymmetrical
the
latent
semantic
analysis
is
a
particular
technique
in
semantic
space
to
parse
through
the
document
and
identify
the
words
with
polysemy
with
nlkt
library
the
resources
such
as
punkt
and
wordnet
have
to
be
downloaded
from
nltk
deep
learning
at
scale
with
google
colab
notebooks
training
machine
learning
or
deep
learning
models
on
cpus
could
take
hours
and
could
be
pretty
expensive
in
terms
of
the
programming
language
efficiency
with
time
and
energy
of
the
computer
resources
google
built
colab
notebooks
environment
for
research
and
development
purposes
it
runs
entirely
on
the
cloud
without
requiring
any
additional
hardware
or
software
setup
for
each
machine
it’s
entirely
equivalent
of
a
jupyter
notebook
that
aids
the
data
scientists
to
share
the
colab
notebooks
by
storing
on
google
drive
just
like
any
other
google
sheets
or
documents
in
a
collaborative
environment
there
are
no
additional
costs
associated
with
enabling
gpu
at
runtime
for
acceleration
on
the
runtime
there
are
some
challenges
of
uploading
the
data
into
colab
unlike
jupyter
notebook
that
can
access
the
data
directly
from
the
local
directory
of
the
machine
in
colab
there
are
multiple
options
to
upload
the
files
from
the
local
file
system
or
a
drive
can
be
mounted
to
load
the
data
through
drive
fuse
wrapper
once
this
step
is
complete
it
shows
the
following
log
without
errors:
the
next
step
would
be
generating
the
authentication
tokens
to
authenticate
the
google
credentials
for
the
drive
and
colab
if
it
shows
successful
retrieval
of
access
token
then
colab
is
all
set
at
this
stage
the
drive
is
not
mounted
yet
it
will
show
false
when
accessing
the
contents
of
the
text
file
once
the
drive
is
mounted
colab
has
access
to
the
datasets
from
google
drive
once
the
files
are
accessible
the
python
can
be
executed
similar
to
executing
in
jupyter
environment
colab
notebook
also
displays
the
results
similar
to
what
we
see
on
jupyter
notebook
pycharm
ide
the
program
can
be
run
compiled
on
pycharm
ide
environment
and
run
on
pycharm
or
can
be
executed
from
osx
terminal
results
from
osx
terminal
jupyter
notebook
on
standalone
machine
jupyter
notebook
gives
a
similar
output
running
the
latent
semantic
analysis
on
the
local
machine:
references
gorrell
g
2006
generalized
hebbian
algorithm
for
incremental
singular
value
decomposition
in
natural
language
processing
retrieved
from
https:wwwaclweborganthologye06-1013
hardeniya
n
2016
natural
language
processing:
python
and
nltk
""
birmingham
england:
packt
publishing
landauer
t
k
foltz
p
w
laham
d
""
university
of
colorado
at
boulder
1998
an
introduction
to
latent
semantic
analysis
retrieved
from
http:lsacoloradoedupapersdp1lsaintropdf
stackoverflow
2018
mounting
google
drive
on
google
colab
retrieved
from
https:stackoverflowcomquestions50168315mounting-google-drive-on-google-colab
stanford
university
2009
matrix
decompositions
and
latent
semantic
indexing
retrieved
from
https:nlpstanfordeduir-bookhtmlhtmleditionmatrix-decompositions-and-latent-semantic-indexing-1html
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ganapathi
pulipaka
|
founder
and
ceo
@deepsingularity
|
bestselling
author
|
big
data
|
iot
|
startups
|
sap
|
machinelearning
|
deeplearning
|
datascience
""
an
alternate
version
of
this
article
was
originally
published
in
the
boston
globe
on
december
2nd
1942
a
team
of
scientists
led
by
enrico
fermi
came
back
from
lunch
and
watched
as
humanity
created
the
first
self-sustaining
nuclear
reaction
inside
a
pile
of
bricks
and
wood
underneath
a
football
field
at
the
university
of
chicago
known
to
history
as
chicago
pile-1
it
was
celebrated
in
silence
with
a
single
bottle
of
chianti
for
those
who
were
there
understood
exactly
what
it
meant
for
humankind
without
any
need
for
words
now
something
new
has
occurred
that
again
quietly
changed
the
world
forever
like
a
whispered
word
in
a
foreign
language
it
was
quiet
in
that
you
may
have
heard
it
but
its
full
meaning
may
not
have
been
comprehended
however
it’s
vital
we
understand
this
new
language
and
what
it’s
increasingly
telling
us
for
the
ramifications
are
set
to
alter
everything
we
take
for
granted
about
the
way
our
globalized
economy
functions
and
the
ways
in
which
we
as
humans
exist
within
it
the
language
is
a
new
class
of
machine
learning
known
as
deep
learning
and
the
whispered
word
was
a
computer’s
use
of
it
to
seemingly
out
of
nowhere
defeat
three-time
european
go
champion
fan
hui
not
once
but
five
times
in
a
row
without
defeat
many
who
read
this
news
considered
that
as
impressive
but
in
no
way
comparable
to
a
match
against
lee
se-dol
instead
who
many
consider
to
be
one
of
the
world’s
best
living
go
players
if
not
the
best
imagining
such
a
grand
duel
of
man
versus
machine
china’s
top
go
player
predicted
that
lee
would
not
lose
a
single
game
and
lee
himself
confidently
expected
to
possibly
lose
one
at
the
most
what
actually
ended
up
happening
when
they
faced
off?
lee
went
on
to
lose
all
but
one
of
their
match’s
five
games
an
ai
named
alphago
is
now
a
better
go
player
than
any
human
and
has
been
granted
the
divine
rank
of
9
dan
in
other
words
its
level
of
play
borders
on
godlike
go
has
officially
fallen
to
machine
just
as
jeopardy
did
before
it
to
watson
and
chess
before
that
to
deep
blue
so
what
is
go?
very
simply
think
of
go
as
super
ultra
mega
chess
this
may
still
sound
like
a
small
accomplishment
another
feather
in
the
cap
of
machines
as
they
continue
to
prove
themselves
superior
in
the
fun
games
we
play
but
it
is
no
small
accomplishment
and
what’s
happening
is
no
game
alphago’s
historic
victory
is
a
clear
signal
that
we’ve
gone
from
linear
to
parabolic
advances
in
technology
are
now
so
visibly
exponential
in
nature
that
we
can
expect
to
see
a
lot
more
milestones
being
crossed
long
before
we
would
otherwise
expect
these
exponential
advances
most
notably
in
forms
of
artificial
intelligence
limited
to
specific
tasks
we
are
entirely
unprepared
for
as
long
as
we
continue
to
insist
upon
employment
as
our
primary
source
of
income
this
may
all
sound
like
exaggeration
so
let’s
take
a
few
decade
steps
back
and
look
at
what
computer
technology
has
been
actively
doing
to
human
employment
so
far:
let
the
above
chart
sink
in
do
not
be
fooled
into
thinking
this
conversation
about
the
automation
of
labor
is
set
in
the
future
it’s
already
here
computer
technology
is
already
eating
jobs
and
has
been
since
1990
all
work
can
be
divided
into
four
types:
routine
and
nonroutine
cognitive
and
manual
routine
work
is
the
same
stuff
day
in
and
day
out
while
nonroutine
work
varies
within
these
two
varieties
is
the
work
that
requires
mostly
our
brains
cognitive
and
the
work
that
requires
mostly
our
bodies
manual
where
once
all
four
types
saw
growth
the
stuff
that
is
routine
stagnated
back
in
1990
this
happened
because
routine
labor
is
easiest
for
technology
to
shoulder
rules
can
be
written
for
work
that
doesn’t
change
and
that
work
can
be
better
handled
by
machines
distressingly
it’s
exactly
routine
work
that
once
formed
the
basis
of
the
american
middle
class
it’s
routine
manual
work
that
henry
ford
transformed
by
paying
people
middle
class
wages
to
perform
and
it’s
routine
cognitive
work
that
once
filled
us
office
spaces
such
jobs
are
now
increasingly
unavailable
leaving
only
two
kinds
of
jobs
with
rosy
outlooks:
jobs
that
require
so
little
thought
we
pay
people
little
to
do
them
and
jobs
that
require
so
much
thought
we
pay
people
well
to
do
them
if
we
can
now
imagine
our
economy
as
a
plane
with
four
engines
where
it
can
still
fly
on
only
two
of
them
as
long
as
they
both
keep
roaring
we
can
avoid
concerning
ourselves
with
crashing
but
what
happens
when
our
two
remaining
engines
also
fail?
that’s
what
the
advancing
fields
of
robotics
and
ai
represent
to
those
final
two
engines
because
for
the
first
time
we
are
successfully
teaching
machines
to
learn
i’m
a
writer
at
heart
but
my
educational
background
happens
to
be
in
psychology
and
physics
i’m
fascinated
by
both
of
them
so
my
undergraduate
focus
ended
up
being
in
the
physics
of
the
human
brain
otherwise
known
as
cognitive
neuroscience
i
think
once
you
start
to
look
into
how
the
human
brain
works
how
our
mass
of
interconnected
neurons
somehow
results
in
what
we
describe
as
the
mind
everything
changes
at
least
it
did
for
me
as
a
quick
primer
in
the
way
our
brains
function
they’re
a
giant
network
of
interconnected
cells
some
of
these
connections
are
short
and
some
are
long
some
cells
are
only
connected
to
one
other
and
some
are
connected
to
many
electrical
signals
then
pass
through
these
connections
at
various
rates
and
subsequent
neural
firings
happen
in
turn
it’s
all
kind
of
like
falling
dominoes
but
far
faster
larger
and
more
complex
the
result
amazingly
is
us
and
what
we’ve
been
learning
about
how
we
work
we’ve
now
begun
applying
to
the
way
machines
work
one
of
these
applications
is
the
creation
of
deep
neural
networks
-
kind
of
like
pared-down
virtual
brains
they
provide
an
avenue
to
machine
learning
that’s
made
incredible
leaps
that
were
previously
thought
to
be
much
further
down
the
road
if
even
possible
at
all
how?
it’s
not
just
the
obvious
growing
capability
of
our
computers
and
our
expanding
knowledge
in
the
neurosciences
but
the
vastly
growing
expanse
of
our
collective
data
aka
big
data
big
data
isn’t
just
some
buzzword
it’s
information
and
when
it
comes
to
information
we’re
creating
more
and
more
of
it
every
day
in
fact
we’re
creating
so
much
that
a
2013
report
by
sintef
estimated
that
90%
of
all
information
in
the
world
had
been
created
in
the
prior
two
years
this
incredible
rate
of
data
creation
is
even
doubling
every
15
years
thanks
to
the
internet
where
in
2015
every
minute
we
were
liking
42
million
things
on
facebook
uploading
300
hours
of
video
to
youtube
and
sending
350000
tweets
everything
we
do
is
generating
data
like
never
before
and
lots
of
data
is
exactly
what
machines
need
in
order
to
learn
to
learn
why?
imagine
programming
a
computer
to
recognize
a
chair
you’d
need
to
enter
a
ton
of
instructions
and
the
result
would
still
be
a
program
detecting
chairs
that
aren’t
and
not
detecting
chairs
that
are
so
how
did
we
learn
to
detect
chairs?
our
parents
pointed
at
a
chair
and
said
chair
then
we
thought
we
had
that
whole
chair
thing
all
figured
out
so
we
pointed
at
a
table
and
said
chair
which
is
when
our
parents
told
us
that
was
table
this
is
called
reinforcement
learning
the
label
chair
gets
connected
to
every
chair
we
see
such
that
certain
neural
pathways
are
weighted
and
others
aren’t
for
chair
to
fire
in
our
brains
what
we
perceive
has
to
be
close
enough
to
our
previous
chair
encounters
essentially
our
lives
are
big
data
filtered
through
our
brains
the
power
of
deep
learning
is
that
it’s
a
way
of
using
massive
amounts
of
data
to
get
machines
to
operate
more
like
we
do
without
giving
them
explicit
instructions
instead
of
describing
chairness
to
a
computer
we
instead
just
plug
it
into
the
internet
and
feed
it
millions
of
pictures
of
chairs
it
can
then
have
a
general
idea
of
chairness
next
we
test
it
with
even
more
images
where
it’s
wrong
we
correct
it
which
further
improves
its
chairness
detection
repetition
of
this
process
results
in
a
computer
that
knows
what
a
chair
is
when
it
sees
it
for
the
most
part
as
well
as
we
can
the
important
difference
though
is
that
unlike
us
it
can
then
sort
through
millions
of
images
within
a
matter
of
seconds
this
combination
of
deep
learning
and
big
data
has
resulted
in
astounding
accomplishments
just
in
the
past
year
aside
from
the
incredible
accomplishment
of
alphago
google’s
deepmind
ai
learned
how
to
read
and
comprehend
what
it
read
through
hundreds
of
thousands
of
annotated
news
articles
deepmind
also
taught
itself
to
play
dozens
of
atari
2600
video
games
better
than
humans
just
by
looking
at
the
screen
and
its
score
and
playing
games
repeatedly
an
ai
named
giraffe
taught
itself
how
to
play
chess
in
a
similar
manner
using
a
dataset
of
175
million
chess
positions
attaining
international
master
level
status
in
just
72
hours
by
repeatedly
playing
itself
in
2015
an
ai
even
passed
a
visual
turing
test
by
learning
to
learn
in
a
way
that
enabled
it
to
be
shown
an
unknown
character
in
a
fictional
alphabet
then
instantly
reproduce
that
letter
in
a
way
that
was
entirely
indistinguishable
from
a
human
given
the
same
task
these
are
all
major
milestones
in
ai
however
despite
all
these
milestones
when
asked
to
estimate
when
a
computer
would
defeat
a
prominent
go
player
the
answer
even
just
months
prior
to
the
announcement
by
google
of
alphago’s
victory
was
by
experts
essentially
maybe
in
another
ten
years
a
decade
was
considered
a
fair
guess
because
go
is
a
game
so
complex
i’ll
just
let
ken
jennings
of
jeopardy
fame
another
former
champion
human
defeated
by
ai
describe
it:
such
confounding
complexity
makes
impossible
any
brute-force
approach
to
scan
every
possible
move
to
determine
the
next
best
move
but
deep
neural
networks
get
around
that
barrier
in
the
same
way
our
own
minds
do
by
learning
to
estimate
what
feels
like
the
best
move
we
do
this
through
observation
and
practice
and
so
did
alphago
by
analyzing
millions
of
professional
games
and
playing
itself
millions
of
times
so
the
answer
to
when
the
game
of
go
would
fall
to
machines
wasn’t
even
close
to
ten
years
the
correct
answer
ended
up
being
any
time
now
any
time
now
that’s
the
new
go-to
response
in
the
21st
century
for
any
question
involving
something
new
machines
can
do
better
than
humans
and
we
need
to
try
to
wrap
our
heads
around
it
we
need
to
recognize
what
it
means
for
exponential
technological
change
to
be
entering
the
labor
market
space
for
nonroutine
jobs
for
the
first
time
ever
machines
that
can
learn
mean
nothing
humans
do
as
a
job
is
uniquely
safe
anymore
from
hamburgers
to
healthcare
machines
can
be
created
to
successfully
perform
such
tasks
with
no
need
or
less
need
for
humans
and
at
lower
costs
than
humans
amelia
is
just
one
ai
out
there
currently
being
beta-tested
in
companies
right
now
created
by
ipsoft
over
the
past
16
years
she’s
learned
how
to
perform
the
work
of
call
center
employees
she
can
learn
in
seconds
what
takes
us
months
and
she
can
do
it
in
20
languages
because
she’s
able
to
learn
she’s
able
to
do
more
over
time
in
one
company
putting
her
through
the
paces
she
successfully
handled
one
of
every
ten
calls
in
the
first
week
and
by
the
end
of
the
second
month
she
could
resolve
six
of
ten
calls
because
of
this
it’s
been
estimated
that
she
can
put
250
million
people
out
of
a
job
worldwide
viv
is
an
ai
coming
soon
from
the
creators
of
siri
who’ll
be
our
own
personal
assistant
she’ll
perform
tasks
online
for
us
and
even
function
as
a
facebook
news
feed
on
steroids
by
suggesting
we
consume
the
media
she’ll
know
we’ll
like
best
in
doing
all
of
this
for
us
we’ll
see
far
fewer
ads
and
that
means
the
entire
advertising
industry
—
that
industry
the
entire
internet
is
built
upon
—
stands
to
be
hugely
disrupted
a
world
with
amelia
and
viv
—
and
the
countless
other
ai
counterparts
coming
online
soon
—
in
combination
with
robots
like
boston
dynamics’
next
generation
atlas
portends
is
a
world
where
machines
can
do
all
four
types
of
jobs
and
that
means
serious
societal
reconsiderations
if
a
machine
can
do
a
job
instead
of
a
human
should
any
human
be
forced
at
the
threat
of
destitution
to
perform
that
job?
should
income
itself
remain
coupled
to
employment
such
that
having
a
job
is
the
only
way
to
obtain
income
when
jobs
for
many
are
entirely
unobtainable?
if
machines
are
performing
an
increasing
percentage
of
our
jobs
for
us
and
not
getting
paid
to
do
them
where
does
that
money
go
instead?
and
what
does
it
no
longer
buy?
is
it
even
possible
that
many
of
the
jobs
we’re
creating
don’t
need
to
exist
at
all
and
only
do
because
of
the
incomes
they
provide?
these
are
questions
we
need
to
start
asking
and
fast
fortunately
people
are
beginning
to
ask
these
questions
and
there’s
an
answer
that’s
building
up
momentum
the
idea
is
to
put
machines
to
work
for
us
but
empower
ourselves
to
seek
out
the
forms
of
remaining
work
we
as
humans
find
most
valuable
by
simply
providing
everyone
a
monthly
paycheck
independent
of
work
this
paycheck
would
be
granted
to
all
citizens
unconditionally
and
its
name
is
universal
basic
income
by
adopting
ubi
aside
from
immunizing
against
the
negative
effects
of
automation
we’d
also
be
decreasing
the
risks
inherent
in
entrepreneurship
and
the
sizes
of
bureaucracies
necessary
to
boost
incomes
it’s
for
these
reasons
it
has
cross-partisan
support
and
is
even
now
in
the
beginning
stages
of
possible
implementation
in
countries
like
switzerland
finland
the
netherlands
and
canada
the
future
is
a
place
of
accelerating
changes
it
seems
unwise
to
continue
looking
at
the
future
as
if
it
were
the
past
where
just
because
new
jobs
have
historically
appeared
they
always
will
the
wef
started
2016
off
by
estimating
the
creation
by
2020
of
2
million
new
jobs
alongside
the
elimination
of
7
million
that’s
a
net
loss
not
a
net
gain
of
5
million
jobs
in
a
frequently
cited
paper
an
oxford
study
estimated
the
automation
of
about
half
of
all
existing
jobs
by
2033
meanwhile
self-driving
vehicles
again
thanks
to
machine
learning
have
the
capability
of
drastically
impacting
all
economies
—
especially
the
us
economy
as
i
wrote
last
year
about
automating
truck
driving
—
by
eliminating
millions
of
jobs
within
a
short
span
of
time
and
now
even
the
white
house
in
a
stunning
report
to
congress
has
put
the
probability
at
83
percent
that
a
worker
making
less
than
$20
an
hour
in
2010
will
eventually
lose
their
job
to
a
machine
even
workers
making
as
much
as
$40
an
hour
face
odds
of
31
percent
to
ignore
odds
like
these
is
tantamount
to
our
now
laughable
duck
and
cover
strategies
for
avoiding
nuclear
blasts
during
the
cold
war
all
of
this
is
why
it’s
those
most
knowledgeable
in
the
ai
field
who
are
now
actively
sounding
the
alarm
for
basic
income
during
a
panel
discussion
at
the
end
of
2015
at
singularity
university
prominent
data
scientist
jeremy
howard
asked
do
you
want
half
of
people
to
starve
because
they
literally
can’t
add
economic
value
or
not?
before
going
on
to
suggest
if
the
answer
is
not
then
the
smartest
way
to
distribute
the
wealth
is
by
implementing
a
universal
basic
income
ai
pioneer
chris
eliasmith
director
of
the
centre
for
theoretical
neuroscience
warned
about
the
immediate
impacts
of
ai
on
society
in
an
interview
with
futurism
ai
is
already
having
a
big
impact
on
our
economies
my
suspicion
is
that
more
countries
will
have
to
follow
finland’s
lead
in
exploring
basic
income
guarantees
for
people
moshe
vardi
expressed
the
same
sentiment
after
speaking
at
the
2016
annual
meeting
of
the
american
association
for
the
advancement
of
science
about
the
emergence
of
intelligent
machines
we
need
to
rethink
the
very
basic
structure
of
our
economic
system
we
may
have
to
consider
instituting
a
basic
income
guarantee
even
baidu’s
chief
scientist
and
founder
of
google’s
google
brain
deep
learning
project
andrew
ng
during
an
onstage
interview
at
this
year’s
deep
learning
summit
expressed
the
shared
notion
that
basic
income
must
be
seriously
considered
by
governments
citing
a
high
chance
that
ai
will
create
massive
labor
displacement
when
those
building
the
tools
begin
warning
about
the
implications
of
their
use
shouldn’t
those
wishing
to
use
those
tools
listen
with
the
utmost
attention
especially
when
it’s
the
very
livelihoods
of
millions
of
people
at
stake?
if
not
then
what
about
when
nobel
prize
winning
economists
begin
agreeing
with
them
in
increasing
numbers?
no
nation
is
yet
ready
for
the
changes
ahead
high
labor
force
non-participation
leads
to
social
instability
and
a
lack
of
consumers
within
consumer
economies
leads
to
economic
instability
so
let’s
ask
ourselves
what’s
the
purpose
of
the
technologies
we’re
creating?
what’s
the
purpose
of
a
car
that
can
drive
for
us
or
artificial
intelligence
that
can
shoulder
60%
of
our
workload?
is
it
to
allow
us
to
work
more
hours
for
even
less
pay?
or
is
it
to
enable
us
to
choose
how
we
work
and
to
decline
any
payhours
we
deem
insufficient
because
we’re
already
earning
the
incomes
that
machines
aren’t?
what’s
the
big
lesson
to
learn
in
a
century
when
machines
can
learn?
i
offer
it’s
that
jobs
are
for
machines
and
life
is
for
people
this
article
was
written
on
a
crowdfunded
monthly
basic
income
if
you
found
value
in
this
article
you
can
support
it
along
with
all
my
advocacy
for
basic
income
with
a
monthly
patron
pledge
of
$1
special
thanks
to
arjun
banker
steven
grimm
larry
cohen
topher
hunt
aaron
marcus-kubitza
andrew
stern
keith
davis
albert
wenger
richard
just
chris
smothers
mark
witham
david
ihnen
danielle
texeira
katie
doemland
paul
wicks
jan
smole
joe
esposito
jack
wagner
joe
ballou
stuart
matthews
natalie
foster
chris
mccoy
michael
honey
gary
aranovich
kai
wong
john
david
hodge
louise
whitmore
dan
o’sullivan
harish
venkatesan
michiel
dral
gerald
huff
susanne
berg
cameron
ottens
kian
alavi
gray
scott
kirk
israel
robert
solovay
jeff
schulman
andrew
henderson
robert
f
greene
martin
jordo
victor
lau
shane
gordon
paolo
narciso
johan
grahn
tony
destefano
erhan
altay
bryan
herdliska
stephane
boisvert
dave
shelton
rise
""
shine
pac
luke
sampson
lee
irving
kris
roadruck
amy
shaffer
thomas
welsh
olli
niinimäki
casey
young
elizabeth
balcar
masud
shah
allen
bauer
all
my
other
funders
for
their
support
and
my
amazing
partner
katie
smith
scott
santens
writes
about
basic
income
on
his
blog
you
can
also
follow
him
here
on
medium
on
twitter
on
facebook
or
on
reddit
where
he
is
a
moderator
for
the
rbasicincome
community
of
over
30000
subscribers
if
you
feel
others
would
appreciate
this
article
please
click
the
green
heart
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
new
orleans
writer
focused
on
the
potential
for
human
civilization
to
gets
its
act
together
in
the
21st
century
moderator
of
rbasicincome
on
reddit
articles
discussing
the
concept
of
the
universal
basic
income
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
日本語
português
português
alternate
türkçe
français
한국어
""
العَرَبِيَّة‎‎
español
méxico
español
españa
polski
italiano
普通话
русский
한국어
""
tiếng
việt
or
فارسی
bigger
update:
the
content
of
this
article
is
now
available
as
a
full-length
video
course
that
walks
you
through
every
step
of
the
code
you
can
take
the
course
for
free
and
access
everything
else
on
lyndacom
free
for
30
days
if
you
sign
up
with
this
link
have
you
heard
people
talking
about
machine
learning
but
only
have
a
fuzzy
idea
of
what
that
means?
are
you
tired
of
nodding
your
way
through
conversations
with
co-workers?
let’s
change
that!
this
guide
is
for
anyone
who
is
curious
about
machine
learning
but
has
no
idea
where
to
start
i
imagine
there
are
a
lot
of
people
who
tried
reading
the
wikipedia
article
got
frustrated
and
gave
up
wishing
someone
would
just
give
them
a
high-level
explanation
that’s
what
this
is
the
goal
is
be
accessible
to
anyone
—
which
means
that
there’s
a
lot
of
generalizations
but
who
cares?
if
this
gets
anyone
more
interested
in
ml
then
mission
accomplished
machine
learning
is
the
idea
that
there
are
generic
algorithms
that
can
tell
you
something
interesting
about
a
set
of
data
without
you
having
to
write
any
custom
code
specific
to
the
problem
instead
of
writing
code
you
feed
data
to
the
generic
algorithm
and
it
builds
its
own
logic
based
on
the
data
for
example
one
kind
of
algorithm
is
a
classification
algorithm
it
can
put
data
into
different
groups
the
same
classification
algorithm
used
to
recognize
handwritten
numbers
could
also
be
used
to
classify
emails
into
spam
and
not-spam
without
changing
a
line
of
code
it’s
the
same
algorithm
but
it’s
fed
different
training
data
so
it
comes
up
with
different
classification
logic
machine
learning
is
an
umbrella
term
covering
lots
of
these
kinds
of
generic
algorithms
you
can
think
of
machine
learning
algorithms
as
falling
into
one
of
two
main
categories
—
supervised
learning
and
unsupervised
learning
the
difference
is
simple
but
really
important
let’s
say
you
are
a
real
estate
agent
your
business
is
growing
so
you
hire
a
bunch
of
new
trainee
agents
to
help
you
out
but
there’s
a
problem
—
you
can
glance
at
a
house
and
have
a
pretty
good
idea
of
what
a
house
is
worth
but
your
trainees
don’t
have
your
experience
so
they
don’t
know
how
to
price
their
houses
to
help
your
trainees
and
maybe
free
yourself
up
for
a
vacation
you
decide
to
write
a
little
app
that
can
estimate
the
value
of
a
house
in
your
area
based
on
it’s
size
neighborhood
etc
and
what
similar
houses
have
sold
for
so
you
write
down
every
time
someone
sells
a
house
in
your
city
for
3
months
for
each
house
you
write
down
a
bunch
of
details
—
number
of
bedrooms
size
in
square
feet
neighborhood
etc
but
most
importantly
you
write
down
the
final
sale
price:
using
that
training
data
we
want
to
create
a
program
that
can
estimate
how
much
any
other
house
in
your
area
is
worth:
this
is
called
supervised
learning
you
knew
how
much
each
house
sold
for
so
in
other
words
you
knew
the
answer
to
the
problem
and
could
work
backwards
from
there
to
figure
out
the
logic
to
build
your
app
you
feed
your
training
data
about
each
house
into
your
machine
learning
algorithm
the
algorithm
is
trying
to
figure
out
what
kind
of
math
needs
to
be
done
to
make
the
numbers
work
out
this
kind
of
like
having
the
answer
key
to
a
math
test
with
all
the
arithmetic
symbols
erased:
from
this
can
you
figure
out
what
kind
of
math
problems
were
on
the
test?
you
know
you
are
supposed
to
do
something
with
the
numbers
on
the
left
to
get
each
answer
on
the
right
in
supervised
learning
you
are
letting
the
computer
work
out
that
relationship
for
you
and
once
you
know
what
math
was
required
to
solve
this
specific
set
of
problems
you
could
answer
to
any
other
problem
of
the
same
type!
let’s
go
back
to
our
original
example
with
the
real
estate
agent
what
if
you
didn’t
know
the
sale
price
for
each
house?
even
if
all
you
know
is
the
size
location
etc
of
each
house
it
turns
out
you
can
still
do
some
really
cool
stuff
this
is
called
unsupervised
learning
this
is
kind
of
like
someone
giving
you
a
list
of
numbers
on
a
sheet
of
paper
and
saying
i
don’t
really
know
what
these
numbers
mean
but
maybe
you
can
figure
out
if
there
is
a
pattern
or
grouping
or
something
—
good
luck!
so
what
could
do
with
this
data?
for
starters
you
could
have
an
algorithm
that
automatically
identified
different
market
segments
in
your
data
maybe
you’d
find
out
that
home
buyers
in
the
neighborhood
near
the
local
college
really
like
small
houses
with
lots
of
bedrooms
but
home
buyers
in
the
suburbs
prefer
3-bedroom
houses
with
lots
of
square
footage
knowing
about
these
different
kinds
of
customers
could
help
direct
your
marketing
efforts
another
cool
thing
you
could
do
is
automatically
identify
any
outlier
houses
that
were
way
different
than
everything
else
maybe
those
outlier
houses
are
giant
mansions
and
you
can
focus
your
best
sales
people
on
those
areas
because
they
have
bigger
commissions
supervised
learning
is
what
we’ll
focus
on
for
the
rest
of
this
post
but
that’s
not
because
unsupervised
learning
is
any
less
useful
or
interesting
in
fact
unsupervised
learning
is
becoming
increasingly
important
as
the
algorithms
get
better
because
it
can
be
used
without
having
to
label
the
data
with
the
correct
answer
side
note:
there
are
lots
of
other
types
of
machine
learning
algorithms
but
this
is
a
pretty
good
place
to
start
as
a
human
your
brain
can
approach
most
any
situation
and
learn
how
to
deal
with
that
situation
without
any
explicit
instructions
if
you
sell
houses
for
a
long
time
you
will
instinctively
have
a
feel
for
the
right
price
for
a
house
the
best
way
to
market
that
house
the
kind
of
client
who
would
be
interested
etc
the
goal
of
strong
ai
research
is
to
be
able
to
replicate
this
ability
with
computers
but
current
machine
learning
algorithms
aren’t
that
good
yet
—
they
only
work
when
focused
a
very
specific
limited
problem
maybe
a
better
definition
for
learning
in
this
case
is
figuring
out
an
equation
to
solve
a
specific
problem
based
on
some
example
data
unfortunately
machine
figuring
out
an
equation
to
solve
a
specific
problem
based
on
some
example
data
isn’t
really
a
great
name
so
we
ended
up
with
machine
learning
instead
of
course
if
you
are
reading
this
50
years
in
the
future
and
we’ve
figured
out
the
algorithm
for
strong
ai
then
this
whole
post
will
all
seem
a
little
quaint
maybe
stop
reading
and
go
tell
your
robot
servant
to
go
make
you
a
sandwich
future
human
so
how
would
you
write
the
program
to
estimate
the
value
of
a
house
like
in
our
example
above?
think
about
it
for
a
second
before
you
read
further
if
you
didn’t
know
anything
about
machine
learning
you’d
probably
try
to
write
out
some
basic
rules
for
estimating
the
price
of
a
house
like
this:
if
you
fiddle
with
this
for
hours
and
hours
you
might
end
up
with
something
that
sort
of
works
but
your
program
will
never
be
perfect
and
it
will
be
hard
to
maintain
as
prices
change
wouldn’t
it
be
better
if
the
computer
could
just
figure
out
how
to
implement
this
function
for
you?
who
cares
what
exactly
the
function
does
as
long
is
it
returns
the
correct
number:
one
way
to
think
about
this
problem
is
that
the
price
is
a
delicious
stew
and
the
ingredients
are
the
number
of
bedrooms
the
square
footage
and
the
neighborhood
if
you
could
just
figure
out
how
much
each
ingredient
impacts
the
final
price
maybe
there’s
an
exact
ratio
of
ingredients
to
stir
in
to
make
the
final
price
that
would
reduce
your
original
function
with
all
those
crazy
if’s
and
else’s
down
to
something
really
simple
like
this:
notice
the
magic
numbers
in
bold
—
841231951398213
12311231231
23242341421
and
20123432095
these
are
our
weights
if
we
could
just
figure
out
the
perfect
weights
to
use
that
work
for
every
house
our
function
could
predict
house
prices!
a
dumb
way
to
figure
out
the
best
weights
would
be
something
like
this:
start
with
each
weight
set
to
10:
run
every
house
you
know
about
through
your
function
and
see
how
far
off
the
function
is
at
guessing
the
correct
price
for
each
house:
for
example
if
the
first
house
really
sold
for
$250000
but
your
function
guessed
it
sold
for
$178000
you
are
off
by
$72000
for
that
single
house
now
add
up
the
squared
amount
you
are
off
for
each
house
you
have
in
your
data
set
let’s
say
that
you
had
500
home
sales
in
your
data
set
and
the
square
of
how
much
your
function
was
off
for
each
house
was
a
grand
total
of
$86123373
that’s
how
wrong
your
function
currently
is
now
take
that
sum
total
and
divide
it
by
500
to
get
an
average
of
how
far
off
you
are
for
each
house
call
this
average
error
amount
the
cost
of
your
function
if
you
could
get
this
cost
to
be
zero
by
playing
with
the
weights
your
function
would
be
perfect
it
would
mean
that
in
every
case
your
function
perfectly
guessed
the
price
of
the
house
based
on
the
input
data
so
that’s
our
goal
—
get
this
cost
to
be
as
low
as
possible
by
trying
different
weights
repeat
step
2
over
and
over
with
every
single
possible
combination
of
weights
whichever
combination
of
weights
makes
the
cost
closest
to
zero
is
what
you
use
when
you
find
the
weights
that
work
you’ve
solved
the
problem!
that’s
pretty
simple
right?
well
think
about
what
you
just
did
you
took
some
data
you
fed
it
through
three
generic
really
simple
steps
and
you
ended
up
with
a
function
that
can
guess
the
price
of
any
house
in
your
area
watch
out
zillow!
but
here’s
a
few
more
facts
that
will
blow
your
mind:
pretty
crazy
right?
ok
of
course
you
can’t
just
try
every
combination
of
all
possible
weights
to
find
the
combo
that
works
the
best
that
would
literally
take
forever
since
you’d
never
run
out
of
numbers
to
try
to
avoid
that
mathematicians
have
figured
out
lots
of
clever
ways
to
quickly
find
good
values
for
those
weights
without
having
to
try
very
many
here’s
one
way:
first
write
a
simple
equation
that
represents
step
#2
above:
now
let’s
re-write
exactly
the
same
equation
but
using
a
bunch
of
machine
learning
math
jargon
that
you
can
ignore
for
now:
this
equation
represents
how
wrong
our
price
estimating
function
is
for
the
weights
we
currently
have
set
if
we
graph
this
cost
equation
for
all
possible
values
of
our
weights
for
number_of_bedrooms
and
sqft
we’d
get
a
graph
that
might
look
something
like
this:
in
this
graph
the
lowest
point
in
blue
is
where
our
cost
is
the
lowest
—
thus
our
function
is
the
least
wrong
the
highest
points
are
where
we
are
most
wrong
so
if
we
can
find
the
weights
that
get
us
to
the
lowest
point
on
this
graph
we’ll
have
our
answer!
so
we
just
need
to
adjust
our
weights
so
we
are
walking
down
hill
on
this
graph
towards
the
lowest
point
if
we
keep
making
small
adjustments
to
our
weights
that
are
always
moving
towards
the
lowest
point
we’ll
eventually
get
there
without
having
to
try
too
many
different
weights
if
you
remember
anything
from
calculus
you
might
remember
that
if
you
take
the
derivative
of
a
function
it
tells
you
the
slope
of
the
function’s
tangent
at
any
point
in
other
words
it
tells
us
which
way
is
downhill
for
any
given
point
on
our
graph
we
can
use
that
knowledge
to
walk
downhill
so
if
we
calculate
a
partial
derivative
of
our
cost
function
with
respect
to
each
of
our
weights
then
we
can
subtract
that
value
from
each
weight
that
will
walk
us
one
step
closer
to
the
bottom
of
the
hill
keep
doing
that
and
eventually
we’ll
reach
the
bottom
of
the
hill
and
have
the
best
possible
values
for
our
weights
if
that
didn’t
make
sense
don’t
worry
and
keep
reading
that’s
a
high
level
summary
of
one
way
to
find
the
best
weights
for
your
function
called
batch
gradient
descent
don’t
be
afraid
to
dig
deeper
if
you
are
interested
on
learning
the
details
when
you
use
a
machine
learning
library
to
solve
a
real
problem
all
of
this
will
be
done
for
you
but
it’s
still
useful
to
have
a
good
idea
of
what
is
happening
the
three-step
algorithm
i
described
is
called
multivariate
linear
regression
you
are
estimating
the
equation
for
a
line
that
fits
through
all
of
your
house
data
points
then
you
are
using
that
equation
to
guess
the
sales
price
of
houses
you’ve
never
seen
before
based
where
that
house
would
appear
on
your
line
it’s
a
really
powerful
idea
and
you
can
solve
real
problems
with
it
but
while
the
approach
i
showed
you
might
work
in
simple
cases
it
won’t
work
in
all
cases
one
reason
is
because
house
prices
aren’t
always
simple
enough
to
follow
a
continuous
line
but
luckily
there
are
lots
of
ways
to
handle
that
there
are
plenty
of
other
machine
learning
algorithms
that
can
handle
non-linear
data
like
neural
networks
or
svms
with
kernels
there
are
also
ways
to
use
linear
regression
more
cleverly
that
allow
for
more
complicated
lines
to
be
fit
in
all
cases
the
same
basic
idea
of
needing
to
find
the
best
weights
still
applies
also
i
ignored
the
idea
of
overfitting
it’s
easy
to
come
up
with
a
set
of
weights
that
always
works
perfectly
for
predicting
the
prices
of
the
houses
in
your
original
data
set
but
never
actually
works
for
any
new
houses
that
weren’t
in
your
original
data
set
but
there
are
ways
to
deal
with
this
like
regularization
and
using
a
cross-validation
data
set
learning
how
to
deal
with
this
issue
is
a
key
part
of
learning
how
to
apply
machine
learning
successfully
in
other
words
while
the
basic
concept
is
pretty
simple
it
takes
some
skill
and
experience
to
apply
machine
learning
and
get
useful
results
but
it’s
a
skill
that
any
developer
can
learn!
once
you
start
seeing
how
easily
machine
learning
techniques
can
be
applied
to
problems
that
seem
really
hard
like
handwriting
recognition
you
start
to
get
the
feeling
that
you
could
use
machine
learning
to
solve
any
problem
and
get
an
answer
as
long
as
you
have
enough
data
just
feed
in
the
data
and
watch
the
computer
magically
figure
out
the
equation
that
fits
the
data!
but
it’s
important
to
remember
that
machine
learning
only
works
if
the
problem
is
actually
solvable
with
the
data
that
you
have
for
example
if
you
build
a
model
that
predicts
home
prices
based
on
the
type
of
potted
plants
in
each
house
it’s
never
going
to
work
there
just
isn’t
any
kind
of
relationship
between
the
potted
plants
in
each
house
and
the
home’s
sale
price
so
no
matter
how
hard
it
tries
the
computer
can
never
deduce
a
relationship
between
the
two
so
remember
if
a
human
expert
couldn’t
use
the
data
to
solve
the
problem
manually
a
computer
probably
won’t
be
able
to
either
instead
focus
on
problems
where
a
human
could
solve
the
problem
but
where
it
would
be
great
if
a
computer
could
solve
it
much
more
quickly
in
my
mind
the
biggest
problem
with
machine
learning
right
now
is
that
it
mostly
lives
in
the
world
of
academia
and
commercial
research
groups
there
isn’t
a
lot
of
easy
to
understand
material
out
there
for
people
who
would
like
to
get
a
broad
understanding
without
actually
becoming
experts
but
it’s
getting
a
little
better
every
day
if
you
want
to
try
out
what
you’ve
learned
in
this
article
i
made
a
course
that
walks
you
through
every
step
of
this
article
including
writing
all
the
code
give
it
a
try!
if
you
want
to
go
deeper
andrew
ng’s
free
machine
learning
class
on
coursera
is
pretty
amazing
as
a
next
step
i
highly
recommend
it
it
should
be
accessible
to
anyone
who
has
a
comp
sci
degree
and
who
remembers
a
very
minimal
amount
of
math
also
you
can
play
around
with
tons
of
machine
learning
algorithms
by
downloading
and
installing
scikit-learn
it’s
a
python
framework
that
has
black
box
versions
of
all
the
standard
algorithms
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
newsletter:
also
please
check
out
the
full-length
course
version
of
this
article
it
covers
everything
in
this
article
in
more
detail
including
writing
the
actual
code
in
python
you
can
get
a
free
30-day
trial
to
watch
the
course
if
you
sign
up
with
this
link
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun
part
2!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
普通话
русский
한국어
português
tiếng
việt
or
italiano
are
you
tired
of
reading
endless
news
stories
about
deep
learning
and
not
really
knowing
what
that
means?
let’s
change
that!
this
time
we
are
going
to
learn
how
to
write
programs
that
recognize
objects
in
images
using
deep
learning
in
other
words
we’re
going
to
explain
the
black
magic
that
allows
google
photos
to
search
your
photos
based
on
what
is
in
the
picture:
just
like
part
1
and
part
2
this
guide
is
for
anyone
who
is
curious
about
machine
learning
but
has
no
idea
where
to
start
the
goal
is
be
accessible
to
anyone
—
which
means
that
there’s
a
lot
of
generalizations
and
we
skip
lots
of
details
but
who
cares?
if
this
gets
anyone
more
interested
in
ml
then
mission
accomplished!
if
you
haven’t
already
read
part
1
and
part
2
read
them
now!
you
might
have
seen
this
famous
xkcd
comic
before
the
goof
is
based
on
the
idea
that
any
3-year-old
child
can
recognize
a
photo
of
a
bird
but
figuring
out
how
to
make
a
computer
recognize
objects
has
puzzled
the
very
best
computer
scientists
for
over
50
years
in
the
last
few
years
we’ve
finally
found
a
good
approach
to
object
recognition
using
deep
convolutional
neural
networks
that
sounds
like
a
a
bunch
of
made
up
words
from
a
william
gibson
sci-fi
novel
but
the
ideas
are
totally
understandable
if
you
break
them
down
one
by
one
so
let’s
do
it
—
let’s
write
a
program
that
can
recognize
birds!
before
we
learn
how
to
recognize
pictures
of
birds
let’s
learn
how
to
recognize
something
much
simpler
—
the
handwritten
number
8
in
part
2
we
learned
about
how
neural
networks
can
solve
complex
problems
by
chaining
together
lots
of
simple
neurons
we
created
a
small
neural
network
to
estimate
the
price
of
a
house
based
on
how
many
bedrooms
it
had
how
big
it
was
and
which
neighborhood
it
was
in:
we
also
know
that
the
idea
of
machine
learning
is
that
the
same
generic
algorithms
can
be
reused
with
different
data
to
solve
different
problems
so
let’s
modify
this
same
neural
network
to
recognize
handwritten
text
but
to
make
the
job
really
simple
we’ll
only
try
to
recognize
one
letter
—
the
numeral
8
machine
learning
only
works
when
you
have
data
—
preferably
a
lot
of
data
so
we
need
lots
and
lots
of
handwritten
8s
to
get
started
luckily
researchers
created
the
mnist
data
set
of
handwritten
numbers
for
this
very
purpose
mnist
provides
60000
images
of
handwritten
digits
each
as
an
18x18
image
here
are
some
8s
from
the
data
set:
the
neural
network
we
made
in
part
2
only
took
in
a
three
numbers
as
the
input
3
bedrooms
2000
sq
feet
""
etc
but
now
we
want
to
process
images
with
our
neural
network
how
in
the
world
do
we
feed
images
into
a
neural
network
instead
of
just
numbers?
the
answer
is
incredible
simple
a
neural
network
takes
numbers
as
input
to
a
computer
an
image
is
really
just
a
grid
of
numbers
that
represent
how
dark
each
pixel
is:
to
feed
an
image
into
our
neural
network
we
simply
treat
the
18x18
pixel
image
as
an
array
of
324
numbers:
the
handle
324
inputs
we’ll
just
enlarge
our
neural
network
to
have
324
input
nodes:
notice
that
our
neural
network
also
has
two
outputs
now
instead
of
just
one
the
first
output
will
predict
the
likelihood
that
the
image
is
an
8
and
thee
second
output
will
predict
the
likelihood
it
isn’t
an
8
by
having
a
separate
output
for
each
type
of
object
we
want
to
recognize
we
can
use
a
neural
network
to
classify
objects
into
groups
our
neural
network
is
a
lot
bigger
than
last
time
324
inputs
instead
of
3!
but
any
modern
computer
can
handle
a
neural
network
with
a
few
hundred
nodes
without
blinking
this
would
even
work
fine
on
your
cell
phone
all
that’s
left
is
to
train
the
neural
network
with
images
of
8s
and
not-8s
so
it
learns
to
tell
them
apart
when
we
feed
in
an
8
we’ll
tell
it
the
probability
the
image
is
an
8
is
100%
and
the
probability
it’s
not
an
8
is
0%
vice
versa
for
the
counter-example
images
here’s
some
of
our
training
data:
we
can
train
this
kind
of
neural
network
in
a
few
minutes
on
a
modern
laptop
when
it’s
done
we’ll
have
a
neural
network
that
can
recognize
pictures
of
8s
with
a
pretty
high
accuracy
welcome
to
the
world
of
late
1980’s-era
image
recognition!
it’s
really
neat
that
simply
feeding
pixels
into
a
neural
network
actually
worked
to
build
image
recognition!
machine
learning
is
magic!
right?
well
of
course
it’s
not
that
simple
first
the
good
news
is
that
our
8
recognizer
really
does
work
well
on
simple
images
where
the
letter
is
right
in
the
middle
of
the
image:
but
now
the
really
bad
news:
our
8
recognizer
totally
fails
to
work
when
the
letter
isn’t
perfectly
centered
in
the
image
just
the
slightest
position
change
ruins
everything:
this
is
because
our
network
only
learned
the
pattern
of
a
perfectly-centered
8
it
has
absolutely
no
idea
what
an
off-center
8
is
it
knows
exactly
one
pattern
and
one
pattern
only
that’s
not
very
useful
in
the
real
world
real
world
problems
are
never
that
clean
and
simple
so
we
need
to
figure
out
how
to
make
our
neural
network
work
in
cases
where
the
8
isn’t
perfectly
centered
we
already
created
a
really
good
program
for
finding
an
8
centered
in
an
image
what
if
we
just
scan
all
around
the
image
for
possible
8s
in
smaller
sections
one
section
at
a
time
until
we
find
one?
this
approach
called
a
sliding
window
it’s
the
brute
force
solution
it
works
well
in
some
limited
cases
but
it’s
really
inefficient
you
have
to
check
the
same
image
over
and
over
looking
for
objects
of
different
sizes
we
can
do
better
than
this!
when
we
trained
our
network
we
only
showed
it
8s
that
were
perfectly
centered
what
if
we
train
it
with
more
data
including
8s
in
all
different
positions
and
sizes
all
around
the
image?
we
don’t
even
need
to
collect
new
training
data
we
can
just
write
a
script
to
generate
new
images
with
the
8s
in
all
kinds
of
different
positions
in
the
image:
using
this
technique
we
can
easily
create
an
endless
supply
of
training
data
more
data
makes
the
problem
harder
for
our
neural
network
to
solve
but
we
can
compensate
for
that
by
making
our
network
bigger
and
thus
able
to
learn
more
complicated
patterns
to
make
the
network
bigger
we
just
stack
up
layer
upon
layer
of
nodes:
we
call
this
a
deep
neural
network
because
it
has
more
layers
than
a
traditional
neural
network
this
idea
has
been
around
since
the
late
1960s
but
until
recently
training
this
large
of
a
neural
network
was
just
too
slow
to
be
useful
but
once
we
figured
out
how
to
use
3d
graphics
cards
which
were
designed
to
do
matrix
multiplication
really
fast
instead
of
normal
computer
processors
working
with
large
neural
networks
suddenly
became
practical
in
fact
the
exact
same
nvidia
geforce
gtx
1080
video
card
that
you
use
to
play
overwatch
can
be
used
to
train
neural
networks
incredibly
quickly
but
even
though
we
can
make
our
neural
network
really
big
and
train
it
quickly
with
a
3d
graphics
card
that
still
isn’t
going
to
get
us
all
the
way
to
a
solution
we
need
to
be
smarter
about
how
we
process
images
into
our
neural
network
think
about
it
it
doesn’t
make
sense
to
train
a
network
to
recognize
an
8
at
the
top
of
a
picture
separately
from
training
it
to
recognize
an
8
at
the
bottom
of
a
picture
as
if
those
were
two
totally
different
objects
there
should
be
some
way
to
make
the
neural
network
smart
enough
to
know
that
an
8
anywhere
in
the
picture
is
the
same
thing
without
all
that
extra
training
luckily
there
is!
as
a
human
you
intuitively
know
that
pictures
have
a
hierarchy
or
conceptual
structure
consider
this
picture:
as
a
human
you
instantly
recognize
the
hierarchy
in
this
picture:
most
importantly
we
recognize
the
idea
of
a
child
no
matter
what
surface
the
child
is
on
we
don’t
have
to
re-learn
the
idea
of
child
for
every
possible
surface
it
could
appear
on
but
right
now
our
neural
network
can’t
do
this
it
thinks
that
an
8
in
a
different
part
of
the
image
is
an
entirely
different
thing
it
doesn’t
understand
that
moving
an
object
around
in
the
picture
doesn’t
make
it
something
different
this
means
it
has
to
re-learn
the
identify
of
each
object
in
every
possible
position
that
sucks
we
need
to
give
our
neural
network
understanding
of
translation
invariance
—
an
8
is
an
8
no
matter
where
in
the
picture
it
shows
up
we’ll
do
this
using
a
process
called
convolution
the
idea
of
convolution
is
inspired
partly
by
computer
science
and
partly
by
biology
ie
mad
scientists
literally
poking
cat
brains
with
weird
probes
to
figure
out
how
cats
process
images
instead
of
feeding
entire
images
into
our
neural
network
as
one
grid
of
numbers
we’re
going
to
do
something
a
lot
smarter
that
takes
advantage
of
the
idea
that
an
object
is
the
same
no
matter
where
it
appears
in
a
picture
here’s
how
it’s
going
to
work
step
by
step
—
similar
to
our
sliding
window
search
above
let’s
pass
a
sliding
window
over
the
entire
original
image
and
save
each
result
as
a
separate
tiny
picture
tile:
by
doing
this
we
turned
our
original
image
into
77
equally-sized
tiny
image
tiles
earlier
we
fed
a
single
image
into
a
neural
network
to
see
if
it
was
an
8
we’ll
do
the
exact
same
thing
here
but
we’ll
do
it
for
each
individual
image
tile:
however
there’s
one
big
twist:
we’ll
keep
the
same
neural
network
weights
for
every
single
tile
in
the
same
original
image
in
other
words
we
are
treating
every
image
tile
equally
if
something
interesting
appears
in
any
given
tile
we’ll
mark
that
tile
as
interesting
we
don’t
want
to
lose
track
of
the
arrangement
of
the
original
tiles
so
we
save
the
result
from
processing
each
tile
into
a
grid
in
the
same
arrangement
as
the
original
image
it
looks
like
this:
in
other
words
we’ve
started
with
a
large
image
and
we
ended
with
a
slightly
smaller
array
that
records
which
sections
of
our
original
image
were
the
most
interesting
the
result
of
step
3
was
an
array
that
maps
out
which
parts
of
the
original
image
are
the
most
interesting
but
that
array
is
still
pretty
big:
to
reduce
the
size
of
the
array
we
downsample
it
using
an
algorithm
called
max
pooling
it
sounds
fancy
but
it
isn’t
at
all!
we’ll
just
look
at
each
2x2
square
of
the
array
and
keep
the
biggest
number:
the
idea
here
is
that
if
we
found
something
interesting
in
any
of
the
four
input
tiles
that
makes
up
each
2x2
grid
square
we’ll
just
keep
the
most
interesting
bit
this
reduces
the
size
of
our
array
while
keeping
the
most
important
bits
so
far
we’ve
reduced
a
giant
image
down
into
a
fairly
small
array
guess
what?
that
array
is
just
a
bunch
of
numbers
so
we
can
use
that
small
array
as
input
into
another
neural
network
this
final
neural
network
will
decide
if
the
image
is
or
isn’t
a
match
to
differentiate
it
from
the
convolution
step
we
call
it
a
fully
connected
network
so
from
start
to
finish
our
whole
five-step
pipeline
looks
like
this:
our
image
processing
pipeline
is
a
series
of
steps:
convolution
max-pooling
and
finally
a
fully-connected
network
when
solving
problems
in
the
real
world
these
steps
can
be
combined
and
stacked
as
many
times
as
you
want!
you
can
have
two
three
or
even
ten
convolution
layers
you
can
throw
in
max
pooling
wherever
you
want
to
reduce
the
size
of
your
data
the
basic
idea
is
to
start
with
a
large
image
and
continually
boil
it
down
step-by-step
until
you
finally
have
a
single
result
the
more
convolution
steps
you
have
the
more
complicated
features
your
network
will
be
able
to
learn
to
recognize
for
example
the
first
convolution
step
might
learn
to
recognize
sharp
edges
the
second
convolution
step
might
recognize
beaks
using
it’s
knowledge
of
sharp
edges
the
third
step
might
recognize
entire
birds
using
it’s
knowledge
of
beaks
etc
here’s
what
a
more
realistic
deep
convolutional
network
like
you
would
find
in
a
research
paper
looks
like:
in
this
case
they
start
a
224
x
224
pixel
image
apply
convolution
and
max
pooling
twice
apply
convolution
3
more
times
apply
max
pooling
and
then
have
two
fully-connected
layers
the
end
result
is
that
the
image
is
classified
into
one
of
1000
categories!
so
how
do
you
know
which
steps
you
need
to
combine
to
make
your
image
classifier
work?
honestly
you
have
to
answer
this
by
doing
a
lot
of
experimentation
and
testing
you
might
have
to
train
100
networks
before
you
find
the
optimal
structure
and
parameters
for
the
problem
you
are
solving
machine
learning
involves
a
lot
of
trial
and
error!
now
finally
we
know
enough
to
write
a
program
that
can
decide
if
a
picture
is
a
bird
or
not
as
always
we
need
some
data
to
get
started
the
free
cifar10
data
set
contains
6000
pictures
of
birds
and
52000
pictures
of
things
that
are
not
birds
but
to
get
even
more
data
we’ll
also
add
in
the
caltech-ucsd
birds-200–2011
data
set
that
has
another
12000
bird
pics
here’s
a
few
of
the
birds
from
our
combined
data
set:
and
here’s
some
of
the
52000
non-bird
images:
this
data
set
will
work
fine
for
our
purposes
but
72000
low-res
images
is
still
pretty
small
for
real-world
applications
if
you
want
google-level
performance
you
need
millions
of
large
images
in
machine
learning
having
more
data
is
almost
always
more
important
that
having
better
algorithms
now
you
know
why
google
is
so
happy
to
offer
you
unlimited
photo
storage
they
want
your
sweet
sweet
data!
to
build
our
classifier
we’ll
use
tflearn
tflearn
is
a
wrapper
around
google’s
tensorflow
deep
learning
library
that
exposes
a
simplified
api
it
makes
building
convolutional
neural
networks
as
easy
as
writing
a
few
lines
of
code
to
define
the
layers
of
our
network
here’s
the
code
to
define
and
train
the
network:
if
you
are
training
with
a
good
video
card
with
enough
ram
like
an
nvidia
geforce
gtx
980
ti
or
better
this
will
be
done
in
less
than
an
hour
if
you
are
training
with
a
normal
cpu
it
might
take
a
lot
longer
as
it
trains
the
accuracy
will
increase
after
the
first
pass
i
got
754%
accuracy
after
just
10
passes
it
was
already
up
to
917%
after
50
or
so
passes
it
capped
out
around
955%
accuracy
and
additional
training
didn’t
help
so
i
stopped
it
there
congrats!
our
program
can
now
recognize
birds
in
images!
now
that
we
have
a
trained
neural
network
we
can
use
it!
here’s
a
simple
script
that
takes
in
a
single
image
file
and
predicts
if
it
is
a
bird
or
not
but
to
really
see
how
effective
our
network
is
we
need
to
test
it
with
lots
of
images
the
data
set
i
created
held
back
15000
images
for
validation
when
i
ran
those
15000
images
through
the
network
it
predicted
the
correct
answer
95%
of
the
time
that
seems
pretty
good
right?
well
it
depends!
our
network
claims
to
be
95%
accurate
but
the
devil
is
in
the
details
that
could
mean
all
sorts
of
different
things
for
example
what
if
5%
of
our
training
images
were
birds
and
the
other
95%
were
not
birds?
a
program
that
guessed
not
a
bird
every
single
time
would
be
95%
accurate!
but
it
would
also
be
100%
useless
we
need
to
look
more
closely
at
the
numbers
than
just
the
overall
accuracy
to
judge
how
good
a
classification
system
really
is
we
need
to
look
closely
at
how
it
failed
not
just
the
percentage
of
the
time
that
it
failed
instead
of
thinking
about
our
predictions
as
right
and
wrong
let’s
break
them
down
into
four
separate
categories
—
using
our
validation
set
of
15000
images
here’s
how
many
times
our
predictions
fell
into
each
category:
why
do
we
break
our
results
down
like
this?
because
not
all
mistakes
are
created
equal
imagine
if
we
were
writing
a
program
to
detect
cancer
from
an
mri
image
if
we
were
detecting
cancer
we’d
rather
have
false
positives
than
false
negatives
false
negatives
would
be
the
worse
possible
case
—
that’s
when
the
program
told
someone
they
definitely
didn’t
have
cancer
but
they
actually
did
instead
of
just
looking
at
overall
accuracy
we
calculate
precision
and
recall
metrics
precision
and
recall
metrics
give
us
a
clearer
picture
of
how
well
we
did:
this
tells
us
that
97%
of
the
time
we
guessed
bird
we
were
right!
but
it
also
tells
us
that
we
only
found
90%
of
the
actual
birds
in
the
data
set
in
other
words
we
might
not
find
every
bird
but
we
are
pretty
sure
about
it
when
we
do
find
one!
now
that
you
know
the
basics
of
deep
convolutional
networks
you
can
try
out
some
of
the
examples
that
come
with
tflearn
to
get
your
hands
dirty
with
different
neural
network
architectures
it
even
comes
with
built-in
data
sets
so
you
don’t
even
have
to
find
your
own
images
you
also
know
enough
now
to
start
branching
and
learning
about
other
areas
of
machine
learning
why
not
learn
how
to
use
algorithms
to
train
computers
how
to
play
atari
games
next?
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
email
list
i’ll
only
email
you
when
i
have
something
new
and
awesome
to
share
it’s
the
best
way
to
find
out
when
i
write
more
articles
like
this
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun
part
4
part
5
and
part
6!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
普通话
русский
한국어
português
tiếng
việt
or
italiano
have
you
noticed
that
facebook
has
developed
an
uncanny
ability
to
recognize
your
friends
in
your
photographs?
in
the
old
days
facebook
used
to
make
you
to
tag
your
friends
in
photos
by
clicking
on
them
and
typing
in
their
name
now
as
soon
as
you
upload
a
photo
facebook
tags
everyone
for
you
like
magic:
this
technology
is
called
face
recognition
facebook’s
algorithms
are
able
to
recognize
your
friends’
faces
after
they
have
been
tagged
only
a
few
times
it’s
pretty
amazing
technology
—
facebook
can
recognize
faces
with
98%
accuracy
which
is
pretty
much
as
good
as
humans
can
do!
let’s
learn
how
modern
face
recognition
works!
but
just
recognizing
your
friends
would
be
too
easy
we
can
push
this
tech
to
the
limit
to
solve
a
more
challenging
problem
—
telling
will
ferrell
famous
actor
apart
from
chad
smith
famous
rock
musician!
so
far
in
part
1
2
and
3
we’ve
used
machine
learning
to
solve
isolated
problems
that
have
only
one
step
—
estimating
the
price
of
a
house
generating
new
data
based
on
existing
data
and
telling
if
an
image
contains
a
certain
object
all
of
those
problems
can
be
solved
by
choosing
one
machine
learning
algorithm
feeding
in
data
and
getting
the
result
but
face
recognition
is
really
a
series
of
several
related
problems:
as
a
human
your
brain
is
wired
to
do
all
of
this
automatically
and
instantly
in
fact
humans
are
too
good
at
recognizing
faces
and
end
up
seeing
faces
in
everyday
objects:
computers
are
not
capable
of
this
kind
of
high-level
generalization
at
least
not
yet
so
we
have
to
teach
them
how
to
do
each
step
in
this
process
separately
we
need
to
build
a
pipeline
where
we
solve
each
step
of
face
recognition
separately
and
pass
the
result
of
the
current
step
to
the
next
step
in
other
words
we
will
chain
together
several
machine
learning
algorithms:
let’s
tackle
this
problem
one
step
at
a
time
for
each
step
we’ll
learn
about
a
different
machine
learning
algorithm
i’m
not
going
to
explain
every
single
algorithm
completely
to
keep
this
from
turning
into
a
book
but
you’ll
learn
the
main
ideas
behind
each
one
and
you’ll
learn
how
you
can
build
your
own
facial
recognition
system
in
python
using
openface
and
dlib
the
first
step
in
our
pipeline
is
face
detection
obviously
we
need
to
locate
the
faces
in
a
photograph
before
we
can
try
to
tell
them
apart!
if
you’ve
used
any
camera
in
the
last
10
years
you’ve
probably
seen
face
detection
in
action:
face
detection
is
a
great
feature
for
cameras
when
the
camera
can
automatically
pick
out
faces
it
can
make
sure
that
all
the
faces
are
in
focus
before
it
takes
the
picture
but
we’ll
use
it
for
a
different
purpose
—
finding
the
areas
of
the
image
we
want
to
pass
on
to
the
next
step
in
our
pipeline
face
detection
went
mainstream
in
the
early
2000's
when
paul
viola
and
michael
jones
invented
a
way
to
detect
faces
that
was
fast
enough
to
run
on
cheap
cameras
however
much
more
reliable
solutions
exist
now
we’re
going
to
use
a
method
invented
in
2005
called
histogram
of
oriented
gradients
—
or
just
hog
for
short
to
find
faces
in
an
image
we’ll
start
by
making
our
image
black
and
white
because
we
don’t
need
color
data
to
find
faces:
then
we’ll
look
at
every
single
pixel
in
our
image
one
at
a
time
for
every
single
pixel
we
want
to
look
at
the
pixels
that
directly
surrounding
it:
our
goal
is
to
figure
out
how
dark
the
current
pixel
is
compared
to
the
pixels
directly
surrounding
it
then
we
want
to
draw
an
arrow
showing
in
which
direction
the
image
is
getting
darker:
if
you
repeat
that
process
for
every
single
pixel
in
the
image
you
end
up
with
every
pixel
being
replaced
by
an
arrow
these
arrows
are
called
gradients
and
they
show
the
flow
from
light
to
dark
across
the
entire
image:
this
might
seem
like
a
random
thing
to
do
but
there’s
a
really
good
reason
for
replacing
the
pixels
with
gradients
if
we
analyze
pixels
directly
really
dark
images
and
really
light
images
of
the
same
person
will
have
totally
different
pixel
values
but
by
only
considering
the
direction
that
brightness
changes
both
really
dark
images
and
really
bright
images
will
end
up
with
the
same
exact
representation
that
makes
the
problem
a
lot
easier
to
solve!
but
saving
the
gradient
for
every
single
pixel
gives
us
way
too
much
detail
we
end
up
missing
the
forest
for
the
trees
it
would
be
better
if
we
could
just
see
the
basic
flow
of
lightnessdarkness
at
a
higher
level
so
we
could
see
the
basic
pattern
of
the
image
to
do
this
we’ll
break
up
the
image
into
small
squares
of
16x16
pixels
each
in
each
square
we’ll
count
up
how
many
gradients
point
in
each
major
direction
how
many
point
up
point
up-right
point
right
etc
then
we’ll
replace
that
square
in
the
image
with
the
arrow
directions
that
were
the
strongest
the
end
result
is
we
turn
the
original
image
into
a
very
simple
representation
that
captures
the
basic
structure
of
a
face
in
a
simple
way:
to
find
faces
in
this
hog
image
all
we
have
to
do
is
find
the
part
of
our
image
that
looks
the
most
similar
to
a
known
hog
pattern
that
was
extracted
from
a
bunch
of
other
training
faces:
using
this
technique
we
can
now
easily
find
faces
in
any
image:
if
you
want
to
try
this
step
out
yourself
using
python
and
dlib
here’s
code
showing
how
to
generate
and
view
hog
representations
of
images
whew
we
isolated
the
faces
in
our
image
but
now
we
have
to
deal
with
the
problem
that
faces
turned
different
directions
look
totally
different
to
a
computer:
to
account
for
this
we
will
try
to
warp
each
picture
so
that
the
eyes
and
lips
are
always
in
the
sample
place
in
the
image
this
will
make
it
a
lot
easier
for
us
to
compare
faces
in
the
next
steps
to
do
this
we
are
going
to
use
an
algorithm
called
face
landmark
estimation
there
are
lots
of
ways
to
do
this
but
we
are
going
to
use
the
approach
invented
in
2014
by
vahid
kazemi
and
josephine
sullivan
the
basic
idea
is
we
will
come
up
with
68
specific
points
called
landmarks
that
exist
on
every
face
—
the
top
of
the
chin
the
outside
edge
of
each
eye
the
inner
edge
of
each
eyebrow
etc
then
we
will
train
a
machine
learning
algorithm
to
be
able
to
find
these
68
specific
points
on
any
face:
here’s
the
result
of
locating
the
68
face
landmarks
on
our
test
image:
now
that
we
know
were
the
eyes
and
mouth
are
we’ll
simply
rotate
scale
and
shear
the
image
so
that
the
eyes
and
mouth
are
centered
as
best
as
possible
we
won’t
do
any
fancy
3d
warps
because
that
would
introduce
distortions
into
the
image
we
are
only
going
to
use
basic
image
transformations
like
rotation
and
scale
that
preserve
parallel
lines
called
affine
transformations:
now
no
matter
how
the
face
is
turned
we
are
able
to
center
the
eyes
and
mouth
are
in
roughly
the
same
position
in
the
image
this
will
make
our
next
step
a
lot
more
accurate
if
you
want
to
try
this
step
out
yourself
using
python
and
dlib
here’s
the
code
for
finding
face
landmarks
and
here’s
the
code
for
transforming
the
image
using
those
landmarks
now
we
are
to
the
meat
of
the
problem
—
actually
telling
faces
apart
this
is
where
things
get
really
interesting!
the
simplest
approach
to
face
recognition
is
to
directly
compare
the
unknown
face
we
found
in
step
2
with
all
the
pictures
we
have
of
people
that
have
already
been
tagged
when
we
find
a
previously
tagged
face
that
looks
very
similar
to
our
unknown
face
it
must
be
the
same
person
seems
like
a
pretty
good
idea
right?
there’s
actually
a
huge
problem
with
that
approach
a
site
like
facebook
with
billions
of
users
and
a
trillion
photos
can’t
possibly
loop
through
every
previous-tagged
face
to
compare
it
to
every
newly
uploaded
picture
that
would
take
way
too
long
they
need
to
be
able
to
recognize
faces
in
milliseconds
not
hours
what
we
need
is
a
way
to
extract
a
few
basic
measurements
from
each
face
then
we
could
measure
our
unknown
face
the
same
way
and
find
the
known
face
with
the
closest
measurements
for
example
we
might
measure
the
size
of
each
ear
the
spacing
between
the
eyes
the
length
of
the
nose
etc
if
you’ve
ever
watched
a
bad
crime
show
like
csi
you
know
what
i
am
talking
about:
ok
so
which
measurements
should
we
collect
from
each
face
to
build
our
known
face
database?
ear
size?
nose
length?
eye
color?
something
else?
it
turns
out
that
the
measurements
that
seem
obvious
to
us
humans
like
eye
color
don’t
really
make
sense
to
a
computer
looking
at
individual
pixels
in
an
image
researchers
have
discovered
that
the
most
accurate
approach
is
to
let
the
computer
figure
out
the
measurements
to
collect
itself
deep
learning
does
a
better
job
than
humans
at
figuring
out
which
parts
of
a
face
are
important
to
measure
the
solution
is
to
train
a
deep
convolutional
neural
network
just
like
we
did
in
part
3
but
instead
of
training
the
network
to
recognize
pictures
objects
like
we
did
last
time
we
are
going
to
train
it
to
generate
128
measurements
for
each
face
the
training
process
works
by
looking
at
3
face
images
at
a
time:
then
the
algorithm
looks
at
the
measurements
it
is
currently
generating
for
each
of
those
three
images
it
then
tweaks
the
neural
network
slightly
so
that
it
makes
sure
the
measurements
it
generates
for
#1
and
#2
are
slightly
closer
while
making
sure
the
measurements
for
#2
and
#3
are
slightly
further
apart:
after
repeating
this
step
millions
of
times
for
millions
of
images
of
thousands
of
different
people
the
neural
network
learns
to
reliably
generate
128
measurements
for
each
person
any
ten
different
pictures
of
the
same
person
should
give
roughly
the
same
measurements
machine
learning
people
call
the
128
measurements
of
each
face
an
embedding
the
idea
of
reducing
complicated
raw
data
like
a
picture
into
a
list
of
computer-generated
numbers
comes
up
a
lot
in
machine
learning
especially
in
language
translation
the
exact
approach
for
faces
we
are
using
was
invented
in
2015
by
researchers
at
google
but
many
similar
approaches
exist
this
process
of
training
a
convolutional
neural
network
to
output
face
embeddings
requires
a
lot
of
data
and
computer
power
even
with
an
expensive
nvidia
telsa
video
card
it
takes
about
24
hours
of
continuous
training
to
get
good
accuracy
but
once
the
network
has
been
trained
it
can
generate
measurements
for
any
face
even
ones
it
has
never
seen
before!
so
this
step
only
needs
to
be
done
once
lucky
for
us
the
fine
folks
at
openface
already
did
this
and
they
published
several
trained
networks
which
we
can
directly
use
thanks
brandon
amos
and
team!
so
all
we
need
to
do
ourselves
is
run
our
face
images
through
their
pre-trained
network
to
get
the
128
measurements
for
each
face
here’s
the
measurements
for
our
test
image:
so
what
parts
of
the
face
are
these
128
numbers
measuring
exactly?
it
turns
out
that
we
have
no
idea
it
doesn’t
really
matter
to
us
all
that
we
care
is
that
the
network
generates
nearly
the
same
numbers
when
looking
at
two
different
pictures
of
the
same
person
if
you
want
to
try
this
step
yourself
openface
provides
a
lua
script
that
will
generate
embeddings
all
images
in
a
folder
and
write
them
to
a
csv
file
you
run
it
like
this
this
last
step
is
actually
the
easiest
step
in
the
whole
process
all
we
have
to
do
is
find
the
person
in
our
database
of
known
people
who
has
the
closest
measurements
to
our
test
image
you
can
do
that
by
using
any
basic
machine
learning
classification
algorithm
no
fancy
deep
learning
tricks
are
needed
we’ll
use
a
simple
linear
svm
classifier
but
lots
of
classification
algorithms
could
work
all
we
need
to
do
is
train
a
classifier
that
can
take
in
the
measurements
from
a
new
test
image
and
tells
which
known
person
is
the
closest
match
running
this
classifier
takes
milliseconds
the
result
of
the
classifier
is
the
name
of
the
person!
so
let’s
try
out
our
system
first
i
trained
a
classifier
with
the
embeddings
of
about
20
pictures
each
of
will
ferrell
chad
smith
and
jimmy
falon:
then
i
ran
the
classifier
on
every
frame
of
the
famous
youtube
video
of
will
ferrell
and
chad
smith
pretending
to
be
each
other
on
the
jimmy
fallon
show:
it
works!
and
look
how
well
it
works
for
faces
in
different
poses
—
even
sideways
faces!
let’s
review
the
steps
we
followed:
now
that
you
know
how
this
all
works
here’s
instructions
from
start-to-finish
of
how
run
this
entire
face
recognition
pipeline
on
your
own
computer:
update
492017:
you
can
still
follow
the
steps
below
to
use
openface
however
i’ve
released
a
new
python-based
face
recognition
library
called
face_recognition
that
is
much
easier
to
install
and
use
so
i’d
recommend
trying
out
face_recognition
first
instead
of
continuing
below!
i
even
put
together
a
pre-configured
virtual
machine
with
face_recognition
opencv
tensorflow
and
lots
of
other
deep
learning
tools
pre-installed
you
can
download
and
run
it
on
your
computer
very
easily
give
the
virtual
machine
a
shot
if
you
don’t
want
to
install
all
these
libraries
yourself!
original
openface
instructions:
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
newsletter:
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun
part
5!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
in
the
five
days
from
july
24th
to
28th
2017
i
interviewed
at
linkedin
salesforce
einstein
google
airbnb
and
facebook
and
got
all
five
job
offers
it
was
a
great
experience
and
i
feel
fortunate
that
my
efforts
paid
off
so
i
decided
to
write
something
about
it
i
will
discuss
how
i
prepared
review
the
interview
process
and
share
my
impressions
about
the
five
companies
i
had
been
at
groupon
for
almost
three
years
it’s
my
first
job
and
i
have
been
working
with
an
amazing
team
and
on
awesome
projects
we’ve
been
building
cool
stuff
making
impact
within
the
company
publishing
papers
and
all
that
but
i
felt
my
learning
rate
was
being
annealed
read:
slowing
down
yet
my
mind
was
craving
more
also
as
a
software
engineer
in
chicago
there
are
so
many
great
companies
that
all
attract
me
in
the
bay
area
life
is
short
and
professional
life
shorter
still
after
talking
with
my
wife
and
gaining
her
full
support
i
decided
to
take
actions
and
make
my
first
ever
career
change
although
i’m
interested
in
machine
learning
positions
the
positions
at
the
five
companies
are
slightly
different
in
the
title
and
the
interviewing
process
three
are
machine
learning
engineer
linkedin
google
facebook
one
is
data
engineer
salesforce
and
one
is
software
engineer
in
general
airbnb
therefore
i
needed
to
prepare
for
three
different
areas:
coding
machine
learning
and
system
design
since
i
also
have
a
full
time
job
it
took
me
2–3
months
in
total
to
prepare
here
is
how
i
prepared
for
the
three
areas
while
i
agree
that
coding
interviews
might
not
be
the
best
way
to
assess
all
your
skills
as
a
developer
there
is
arguably
no
better
way
to
tell
if
you
are
a
good
engineer
in
a
short
period
of
time
imo
it
is
the
necessary
evil
to
get
you
that
job
i
mainly
used
leetcode
and
geeksforgeeks
for
practicing
but
hackerrank
and
lintcode
are
also
good
places
i
spent
several
weeks
going
over
common
data
structures
and
algorithms
then
focused
on
areas
i
wasn’t
too
familiar
with
and
finally
did
some
frequently
seen
problems
due
to
my
time
constraints
i
usually
did
two
problems
per
day
here
are
some
thoughts:
this
area
is
more
closely
related
to
the
actual
working
experience
many
questions
can
be
asked
during
system
design
interviews
including
but
not
limited
to
system
architecture
object
oriented
designdatabase
schema
designdistributed
system
designscalability
etc
there
are
many
resources
online
that
can
help
you
with
the
preparation
for
the
most
part
i
read
articles
on
system
design
interviews
architectures
of
large-scale
systems
and
case
studies
here
are
some
resources
that
i
found
really
helpful:
although
system
design
interviews
can
cover
a
lot
of
topics
there
are
some
general
guidelines
for
how
to
approach
the
problem:
with
all
that
said
the
best
way
to
practice
for
system
design
interviews
is
to
actually
sit
down
and
design
a
system
ie
your
day-to-day
work
instead
of
doing
the
minimal
work
go
deeper
into
the
tools
frameworks
and
libraries
you
use
for
example
if
you
use
hbase
rather
than
simply
using
the
client
to
run
some
ddl
and
do
some
fetches
try
to
understand
its
overall
architecture
such
as
the
readwrite
flow
how
hbase
ensures
strong
consistency
what
minormajor
compactions
do
and
where
lru
cache
and
bloom
filter
are
used
in
the
system
you
can
even
compare
hbase
with
cassandra
and
see
the
similarities
and
differences
in
their
design
then
when
you
are
asked
to
design
a
distributed
key-value
store
you
won’t
feel
ambushed
many
blogs
are
also
a
great
source
of
knowledge
such
as
hacker
noon
and
engineering
blogs
of
some
companies
as
well
as
the
official
documentation
of
open
source
projects
the
most
important
thing
is
to
keep
your
curiosity
and
modesty
be
a
sponge
that
absorbs
everything
it
is
submerged
into
machine
learning
interviews
can
be
divided
into
two
aspects
theory
and
product
design
unless
you
are
have
experience
in
machine
learning
research
or
did
really
well
in
your
ml
course
it
helps
to
read
some
textbooks
classical
ones
such
as
the
elements
of
statistical
learning
and
pattern
recognition
and
machine
learning
are
great
choices
and
if
you
are
interested
in
specific
areas
you
can
read
more
on
those
make
sure
you
understand
basic
concepts
such
as
bias-variance
trade-off
overfitting
gradient
descent
l1l2
regularizationbayes
theorembaggingboostingcollaborative
filteringdimension
reduction
etc
familiarize
yourself
with
common
formulas
such
as
bayes
theorem
and
the
derivation
of
popular
models
such
as
logistic
regression
and
svm
try
to
implement
simple
models
such
as
decision
trees
and
k-means
clustering
if
you
put
some
models
on
your
resume
make
sure
you
understand
it
thoroughly
and
can
comment
on
its
pros
and
cons
for
ml
product
design
understand
the
general
process
of
building
a
ml
product
here’s
what
i
tried
to
do:
here
i
want
to
emphasize
again
on
the
importance
of
remaining
curious
and
learning
continuously
try
not
to
merely
using
the
api
for
spark
mllib
or
xgboost
and
calling
it
done
but
try
to
understand
why
stochastic
gradient
descent
is
appropriate
for
distributed
training
or
understand
how
xgboost
differs
from
traditional
gbdt
eg
what
is
special
about
its
loss
function
why
it
needs
to
compute
the
second
order
derivative
etc
i
started
by
replying
to
hr’s
messages
on
linkedin
and
asking
for
referrals
after
a
failed
attempt
at
a
rock
star
startup
which
i
will
touch
upon
later
i
prepared
hard
for
several
months
and
with
help
from
my
recruiters
i
scheduled
a
full
week
of
onsites
in
the
bay
area
i
flew
in
on
sunday
had
five
full
days
of
interviews
with
around
30
interviewers
at
some
best
tech
companies
in
the
world
and
very
luckily
got
job
offers
from
all
five
of
them
all
phone
screenings
are
standard
the
only
difference
is
in
the
duration:
for
some
companies
like
linkedin
it’s
one
hour
while
for
facebook
and
airbnb
it’s
45
minutes
proficiency
is
the
key
here
since
you
are
under
the
time
gun
and
usually
you
only
get
one
chance
you
would
have
to
very
quickly
recognize
the
type
of
problem
and
give
a
high-level
solution
be
sure
to
talk
to
the
interviewer
about
your
thinking
and
intentions
it
might
slow
you
down
a
little
at
the
beginning
but
communication
is
more
important
than
anything
and
it
only
helps
with
the
interview
do
not
recite
the
solution
as
the
interviewer
would
almost
certainly
see
through
it
for
machine
learning
positions
some
companies
would
ask
ml
questions
if
you
are
interviewing
for
those
make
sure
you
brush
up
your
ml
skills
as
well
to
make
better
use
of
my
time
i
scheduled
three
phone
screenings
in
the
same
afternoon
one
hour
apart
from
each
the
upside
is
that
you
might
benefit
from
the
hot
hand
and
the
downside
is
that
the
later
ones
might
be
affected
if
the
first
one
does
not
go
well
so
i
don’t
recommend
it
for
everyone
one
good
thing
about
interviewing
with
multiple
companies
at
the
same
time
is
that
it
gives
you
certain
advantages
i
was
able
to
skip
the
second
round
phone
screening
with
airbnb
and
salesforce
because
i
got
the
onsite
at
linkedin
and
facebook
after
only
one
phone
screening
more
surprisingly
google
even
let
me
skip
their
phone
screening
entirely
and
schedule
my
onsite
to
fill
the
vacancy
after
learning
i
had
four
onsites
coming
in
the
next
week
i
knew
it
was
going
to
make
it
extremely
tiring
but
hey
nobody
can
refuse
a
google
onsite
invitation!
linkedin
this
is
my
first
onsite
and
i
interviewed
at
the
sunnyvale
location
the
office
is
very
neat
and
people
look
very
professional
as
always
the
sessions
are
one
hour
each
coding
questions
are
standard
but
the
ml
questions
can
get
a
bit
tough
that
said
i
got
an
email
from
my
hr
containing
the
preparation
material
which
was
very
helpful
and
in
the
end
i
did
not
see
anything
that
was
too
surprising
i
heard
the
rumor
that
linkedin
has
the
best
meals
in
the
silicon
valley
and
from
what
i
saw
if
it’s
not
true
it’s
not
too
far
from
the
truth
acquisition
by
microsoft
seems
to
have
lifted
the
financial
burden
from
linkedin
and
freed
them
up
to
do
really
cool
things
new
features
such
as
videos
and
professional
advertisements
are
exciting
as
a
company
focusing
on
professional
development
linkedin
prioritizes
the
growth
of
its
own
employees
a
lot
of
teams
such
as
ads
relevance
and
feed
ranking
are
expanding
so
act
quickly
if
you
want
to
join
salesforce
einstein
rock
star
project
by
rock
star
team
the
team
is
pretty
new
and
feels
very
much
like
a
startup
the
product
is
built
on
the
scala
stack
so
type
safety
is
a
real
thing
there!
great
talks
on
the
optimus
prime
library
by
matthew
tovbin
at
scala
days
chicago
2017
and
leah
mcguire
at
spark
summit
west
2017
i
interviewed
at
their
palo
alto
office
the
team
has
a
cohesive
culture
and
work
life
balance
is
great
there
everybody
is
passionate
about
what
they
are
doing
and
really
enjoys
it
with
four
sessions
it
is
shorter
compared
to
the
other
onsite
interviews
but
i
wish
i
could
have
stayed
longer
after
the
interview
matthew
even
took
me
for
a
walk
to
the
hp
garage
:
google
absolutely
the
industry
leader
and
nothing
to
say
about
it
that
people
don’t
already
know
but
it’s
huge
like
really
really
huge
it
took
me
20
minutes
to
ride
a
bicycle
to
meet
my
friends
there
also
lines
for
food
can
be
too
long
forever
a
great
place
for
developers
i
interviewed
at
one
of
the
many
buildings
on
the
mountain
view
campus
and
i
don’t
know
which
one
it
is
because
it’s
huge
my
interviewers
all
look
very
smart
and
once
they
start
talking
they
are
even
smarter
it
would
be
very
enjoyable
to
work
with
these
people
one
thing
that
i
felt
special
about
google’s
interviews
is
that
the
analysis
of
algorithm
complexity
is
really
important
make
sure
you
really
understand
what
big
o
notation
means!
airbnb
fast
expanding
unicorn
with
a
unique
culture
and
arguably
the
most
beautiful
office
in
the
silicon
valley
new
products
such
as
experiences
and
restaurant
reservation
high
end
niche
market
and
expansion
into
china
all
contribute
to
a
positive
prospect
perfect
choice
if
you
are
risk
tolerant
and
want
a
fast
growing
pre-ipo
experience
airbnb’s
coding
interview
is
a
bit
unique
because
you’ll
be
coding
in
an
ide
instead
of
whiteboarding
so
your
code
needs
to
compile
and
give
the
right
answer
some
problems
can
get
really
hard
and
they’ve
got
the
one-of-a-kind
cross
functional
interviews
this
is
how
airbnb
takes
culture
seriously
and
being
technically
excellent
doesn’t
guarantee
a
job
offer
for
me
the
two
cross
functionals
were
really
enjoyable
i
had
casual
conversations
with
the
interviewers
and
we
all
felt
happy
at
the
end
of
the
session
overall
i
think
airbnb’s
onsite
is
the
hardest
due
to
the
difficulty
of
the
problems
longer
duration
and
unique
cross-functional
interviews
if
you
are
interested
be
sure
to
understand
their
culture
and
core
values
facebook
another
giant
that
is
still
growing
fast
and
smaller
and
faster-paced
compared
to
google
with
its
product
lines
dominating
the
social
network
market
and
big
investments
in
ai
and
vr
i
can
only
see
more
growth
potential
for
facebook
in
the
future
with
stars
like
yann
lecun
and
yangqing
jia
it’s
the
perfect
place
if
you
are
interested
in
machine
learning
i
interviewed
at
building
20
the
one
with
the
rooftop
garden
and
ocean
view
and
also
where
zuckerberg’s
office
is
located
i’m
not
sure
if
the
interviewers
got
instructions
but
i
didn’t
get
clear
signs
whether
my
solutions
were
correct
although
i
believed
they
were
by
noon
the
prior
four
days
started
to
take
its
toll
and
i
was
having
a
headache
i
persisted
through
the
afternoon
sessions
but
felt
i
didn’t
do
well
at
all
i
was
a
bit
surprised
to
learn
that
i
was
getting
an
offer
from
them
as
well
generally
i
felt
people
there
believe
the
company’s
vision
and
are
proud
of
what
they
are
building
being
a
company
with
half
a
trillion
market
cap
and
growing
facebook
is
a
perfect
place
to
grow
your
career
at
this
is
a
big
topic
that
i
won’t
cover
in
this
post
but
i
found
this
article
to
be
very
helpful
some
things
that
i
do
think
are
important:
all
successes
start
with
failures
including
interviews
before
i
started
interviewing
for
these
companies
i
failed
my
interview
at
databricks
in
may
back
in
april
xiangrui
contacted
me
via
linkedin
asking
me
if
i
was
interested
in
a
position
on
the
spark
mllib
team
i
was
extremely
thrilled
because
1
i
use
spark
and
love
scala
2
databricks
engineers
are
top-notch
and
3
spark
is
revolutionizing
the
whole
big
data
world
it
is
an
opportunity
i
couldn’t
miss
so
i
started
interviewing
after
a
few
days
the
bar
is
very
high
and
the
process
is
quite
long
including
one
pre-screening
questionnaire
one
phone
screening
one
coding
assignment
and
one
full
onsite
i
managed
to
get
the
onsite
invitation
and
visited
their
office
in
downtown
san
francisco
where
treasure
island
can
be
seen
my
interviewer
were
incredibly
intelligent
yet
equally
modest
during
the
interviews
i
often
felt
being
pushed
to
the
limits
it
was
fine
until
one
disastrous
session
where
i
totally
messed
up
due
to
insufficient
skills
and
preparation
and
it
ended
up
a
fiasco
xiangrui
was
very
kind
and
walked
me
to
where
i
wanted
to
go
after
the
interview
was
over
and
i
really
enjoyed
talking
to
him
i
got
the
rejection
several
days
later
it
was
expected
but
i
felt
frustrated
for
a
few
days
nonetheless
although
i
missed
the
opportunity
to
work
there
i
wholeheartedly
wish
they
will
continue
to
make
greater
impact
and
achievements
from
the
first
interview
in
may
to
finally
accepting
the
job
offer
in
late
september
my
first
career
change
was
long
and
not
easy
it
was
difficult
for
me
to
prepare
because
i
needed
to
keep
doing
well
at
my
current
job
for
several
weeks
i
was
on
a
regular
schedule
of
preparing
for
the
interview
till
1am
getting
up
at
8:30am
the
next
day
and
fully
devoting
myself
to
another
day
at
work
interviewing
at
five
companies
in
five
days
was
also
highly
stressful
and
risky
and
i
don’t
recommend
doing
it
unless
you
have
a
very
tight
schedule
but
it
does
give
you
a
good
advantage
during
negotiation
should
you
secure
multiple
offers
i’d
like
to
thank
all
my
recruiters
who
patiently
walked
me
through
the
process
the
people
who
spend
their
precious
time
talking
to
me
and
all
the
companies
that
gave
me
the
opportunities
to
interview
and
extended
me
offers
lastly
but
most
importantly
i
want
to
thank
my
family
for
their
love
and
support
—
my
parents
for
watching
me
taking
the
first
and
every
step
my
dear
wife
for
everything
she
has
done
for
me
and
my
daughter
for
her
warming
smile
thanks
for
reading
through
this
long
post
you
can
find
me
on
linkedin
or
twitter
xiaohan
zeng
102217
ps:
since
the
publication
of
this
post
it
has
unexpectedly
received
some
attention
i
would
like
to
thank
everybody
for
the
congratulations
and
shares
and
apologize
for
not
being
able
to
respond
to
each
of
them
this
post
has
been
translated
into
some
other
languages:
it
has
been
reposted
in
tech
in
asia
breaking
into
startups
invited
me
to
a
live
video
streaming
together
with
sophia
ciocca
covershr
did
a
short
qna
with
me
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
critical
mind
""
romantic
heart
""
disclaimer:
i’m
not
an
expert
in
neural
networks
or
machine
learning
since
originally
writing
this
article
many
people
with
far
more
expertise
in
these
fields
than
myself
have
indicated
that
while
impressive
what
google
have
achieved
is
evolutionary
not
revolutionary
in
the
very
least
it’s
fair
to
say
that
i’m
guilty
of
anthropomorphising
in
parts
of
the
text
i’ve
left
the
article’s
content
unchanged
because
i
think
it’s
interesting
to
compare
the
gut
reaction
i
had
with
the
subsequent
comments
of
experts
in
the
field
i
strongly
encourage
readers
to
browse
the
comments
after
reading
the
article
for
some
perspectives
more
sober
and
informed
than
my
own
in
the
closing
weeks
of
2016
google
published
an
article
that
quietly
sailed
under
most
people’s
radars
which
is
a
shame
because
it
may
just
be
the
most
astonishing
article
about
machine
learning
that
i
read
last
year
don’t
feel
bad
if
you
missed
it
not
only
was
the
article
competing
with
the
pre-christmas
rush
that
most
of
us
were
navigating
—
it
was
also
tucked
away
on
google’s
research
blog
beneath
the
geektastic
headline
zero-shot
translation
with
google’s
multilingual
neural
machine
translation
system
this
doesn’t
exactly
scream
must
read
does
it?
especially
when
you’ve
got
projects
to
wind
up
gifts
to
buy
and
family
feuds
to
be
resolved
—
all
while
the
advent
calendar
relentlessly
counts
down
the
days
until
christmas
like
some
kind
of
chocolate-filled
yuletide
doomsday
clock
luckily
i’m
here
to
bring
you
up
to
speed
here’s
the
deal
up
until
september
of
last
year
google
translate
used
phrase-based
translation
it
basically
did
the
same
thing
you
and
i
do
when
we
look
up
key
words
and
phrases
in
our
lonely
planet
language
guides
it’s
effective
enough
and
blisteringly
fast
compared
to
awkwardly
thumbing
your
way
through
a
bunch
of
pages
looking
for
the
french
equivalent
of
please
bring
me
all
of
your
cheese
and
don’t
stop
until
i
fall
over
but
it
lacks
nuance
phrase-based
translation
is
a
blunt
instrument
it
does
the
job
well
enough
to
get
by
but
mapping
roughly
equivalent
words
and
phrases
without
an
understanding
of
linguistic
structures
can
only
produce
crude
results
this
approach
is
also
limited
by
the
extent
of
an
available
vocabulary
phrase-based
translation
has
no
capacity
to
make
educated
guesses
at
words
it
doesn’t
recognize
and
can’t
learn
from
new
input
all
that
changed
in
september
when
google
gave
their
translation
tool
a
new
engine:
the
google
neural
machine
translation
system
gnmt
this
new
engine
comes
fully
loaded
with
all
the
hot
2016
buzzwords
like
neural
network
and
machine
learning
the
short
version
is
that
google
translate
got
smart
it
developed
the
ability
to
learn
from
the
people
who
used
it
it
learned
how
to
make
educated
guesses
about
the
content
tone
and
meaning
of
phrases
based
on
the
context
of
other
words
and
phrases
around
them
and
—
here’s
the
bit
that
should
make
your
brain
explode
—
it
got
creative
google
translate
invented
its
own
language
to
help
it
translate
more
effectively
what’s
more
nobody
told
it
to
it
didn’t
develop
a
language
or
interlingua
as
google
call
it
because
it
was
coded
to
it
developed
a
new
language
because
the
software
determined
over
time
that
this
was
the
most
efficient
way
to
solve
the
problem
of
translation
stop
and
think
about
that
for
a
moment
let
it
sink
in
a
neural
computing
system
designed
to
translate
content
from
one
human
language
into
another
developed
its
own
internal
language
to
make
the
task
more
efficient
without
being
told
to
do
so
in
a
matter
of
weeks
i’ve
added
a
correctionretraction
of
this
paragraph
in
the
notes
to
understand
what’s
going
on
we
need
to
understand
what
zero-shot
translation
capability
is
here’s
google’s
mike
schuster
nikhil
thorat
and
melvin
johnson
from
the
original
blog
post:
here
you
can
see
an
advantage
of
google’s
new
neural
machine
over
the
old
phrase-based
approach
the
gmnt
is
able
to
learn
how
to
translate
between
two
languages
without
being
explicitly
taught
this
wouldn’t
be
possible
in
a
phrase-based
model
where
translation
is
dependent
upon
an
explicit
dictionary
to
map
words
and
phrases
between
each
pair
of
languages
being
translated
and
this
leads
the
google
engineers
onto
that
truly
astonishing
discovery
of
creation:
so
there
you
have
it
in
the
last
weeks
of
2016
as
journos
around
the
world
started
penning
their
was
this
the
worst
year
in
living
memory
thinkpieces
google
engineers
were
quietly
documenting
a
genuinely
astonishing
breakthrough
in
software
engineering
and
linguistics
i
just
thought
maybe
you’d
want
to
know
ok
to
really
understand
what’s
going
on
we
probably
need
multiple
computer
science
and
linguistics
degrees
i’m
just
barely
scraping
the
surface
here
if
you’ve
got
time
to
get
a
few
degrees
or
if
you’ve
already
got
them
please
drop
me
a
line
and
explain
it
all
me
to
slowly
update
1:
in
my
excitement
it’s
fair
to
say
that
i’ve
exaggerated
the
idea
of
this
as
an
‘intelligent’
system
—
at
least
so
far
as
we
would
think
about
human
intelligence
and
decision
making
make
sure
you
read
chris
mcdonald’s
comment
after
the
article
for
a
more
sober
perspective
update
2:
nafrondel’s
excellent
detailed
reply
is
also
a
must
read
for
an
expert
explanation
of
how
neural
networks
function
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
a
tinkerer
our
community
publishes
stories
worth
reading
on
development
design
and
data
science
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
italiano
español
français
türkçe
русский
한국어
português
فارسی
tiếng
việt
or
普通话
in
part
1
we
said
that
machine
learning
is
using
generic
algorithms
to
tell
you
something
interesting
about
your
data
without
writing
any
code
specific
to
the
problem
you
are
solving
if
you
haven’t
already
read
part
1
read
it
now!
this
time
we
are
going
to
see
one
of
these
generic
algorithms
do
something
really
cool
—
create
video
game
levels
that
look
like
they
were
made
by
humans
we’ll
build
a
neural
network
feed
it
existing
super
mario
levels
and
watch
new
ones
pop
out!
just
like
part
1
this
guide
is
for
anyone
who
is
curious
about
machine
learning
but
has
no
idea
where
to
start
the
goal
is
be
accessible
to
anyone
—
which
means
that
there’s
a
lot
of
generalizations
and
we
skip
lots
of
details
but
who
cares?
if
this
gets
anyone
more
interested
in
ml
then
mission
accomplished
back
in
part
1
we
created
a
simple
algorithm
that
estimated
the
value
of
a
house
based
on
its
attributes
given
data
about
a
house
like
this:
we
ended
up
with
this
simple
estimation
function:
in
other
words
we
estimated
the
value
of
the
house
by
multiplying
each
of
its
attributes
by
a
weight
then
we
just
added
those
numbers
up
to
get
the
house’s
value
instead
of
using
code
let’s
represent
that
same
function
as
a
simple
diagram:
however
this
algorithm
only
works
for
simple
problems
where
the
result
has
a
linear
relationship
with
the
input
what
if
the
truth
behind
house
prices
isn’t
so
simple?
for
example
maybe
the
neighborhood
matters
a
lot
for
big
houses
and
small
houses
but
doesn’t
matter
at
all
for
medium-sized
houses
how
could
we
capture
that
kind
of
complicated
detail
in
our
model?
to
be
more
clever
we
could
run
this
algorithm
multiple
times
with
different
of
weights
that
each
capture
different
edge
cases:
now
we
have
four
different
price
estimates
let’s
combine
those
four
price
estimates
into
one
final
estimate
we’ll
run
them
through
the
same
algorithm
again
but
using
another
set
of
weights!
our
new
super
answer
combines
the
estimates
from
our
four
different
attempts
to
solve
the
problem
because
of
this
it
can
model
more
cases
than
we
could
capture
in
one
simple
model
let’s
combine
our
four
attempts
to
guess
into
one
big
diagram:
this
is
a
neural
network!
each
node
knows
how
to
take
in
a
set
of
inputs
apply
weights
to
them
and
calculate
an
output
value
by
chaining
together
lots
of
these
nodes
we
can
model
complex
functions
there’s
a
lot
that
i’m
skipping
over
to
keep
this
brief
including
feature
scaling
and
the
activation
function
but
the
most
important
part
is
that
these
basic
ideas
click:
it’s
just
like
lego!
we
can’t
model
much
with
one
single
lego
block
but
we
can
model
anything
if
we
have
enough
basic
lego
blocks
to
stick
together:
the
neural
network
we’ve
seen
always
returns
the
same
answer
when
you
give
it
the
same
inputs
it
has
no
memory
in
programming
terms
it’s
a
stateless
algorithm
in
many
cases
like
estimating
the
price
of
house
that’s
exactly
what
you
want
but
the
one
thing
this
kind
of
model
can’t
do
is
respond
to
patterns
in
data
over
time
imagine
i
handed
you
a
keyboard
and
asked
you
to
write
a
story
but
before
you
start
my
job
is
to
guess
the
very
first
letter
that
you
will
type
what
letter
should
i
guess?
i
can
use
my
knowledge
of
english
to
increase
my
odds
of
guessing
the
right
letter
for
example
you
will
probably
type
a
letter
that
is
common
at
the
beginning
of
words
if
i
looked
at
stories
you
wrote
in
the
past
i
could
narrow
it
down
further
based
on
the
words
you
usually
use
at
the
beginning
of
your
stories
once
i
had
all
that
data
i
could
use
it
to
build
a
neural
network
to
model
how
likely
it
is
that
you
would
start
with
any
given
letter
our
model
might
look
like
this:
but
let’s
make
the
problem
harder
let’s
say
i
need
to
guess
the
next
letter
you
are
going
to
type
at
any
point
in
your
story
this
is
a
much
more
interesting
problem
let’s
use
the
first
few
words
of
ernest
hemingway’s
the
sun
also
rises
as
an
example:
what
letter
is
going
to
come
next?
you
probably
guessed
’n’
—
the
word
is
probably
going
to
be
boxing
we
know
this
based
on
the
letters
we’ve
already
seen
in
the
sentence
and
our
knowledge
of
common
words
in
english
also
the
word
‘middleweight’
gives
us
an
extra
clue
that
we
are
talking
about
boxing
in
other
words
it’s
easy
to
guess
the
next
letter
if
we
take
into
account
the
sequence
of
letters
that
came
right
before
it
and
combine
that
with
our
knowledge
of
the
rules
of
english
to
solve
this
problem
with
a
neural
network
we
need
to
add
state
to
our
model
each
time
we
ask
our
neural
network
for
an
answer
we
also
save
a
set
of
our
intermediate
calculations
and
re-use
them
the
next
time
as
part
of
our
input
that
way
our
model
will
adjust
its
predictions
based
on
the
input
that
it
has
seen
recently
keeping
track
of
state
in
our
model
makes
it
possible
to
not
just
predict
the
most
likely
first
letter
in
the
story
but
to
predict
the
most
likely
next
letter
given
all
previous
letters
this
is
the
basic
idea
of
a
recurrent
neural
network
we
are
updating
the
network
each
time
we
use
it
this
allows
it
to
update
its
predictions
based
on
what
it
saw
most
recently
it
can
even
model
patterns
over
time
as
long
as
we
give
it
enough
of
a
memory
predicting
the
next
letter
in
a
story
might
seem
pretty
useless
what’s
the
point?
one
cool
use
might
be
auto-predict
for
a
mobile
phone
keyboard:
but
what
if
we
took
this
idea
to
the
extreme?
what
if
we
asked
the
model
to
predict
the
next
most
likely
character
over
and
over
—
forever?
we’d
be
asking
it
to
write
a
complete
story
for
us!
we
saw
how
we
could
guess
the
next
letter
in
hemingway’s
sentence
let’s
try
generating
a
whole
story
in
the
style
of
hemingway
to
do
this
we
are
going
to
use
the
recurrent
neural
network
implementation
that
andrej
karpathy
wrote
andrej
is
a
deep-learning
researcher
at
stanford
and
he
wrote
an
excellent
introduction
to
generating
text
with
rnns
you
can
view
all
the
code
for
the
model
on
github
we’ll
create
our
model
from
the
complete
text
of
the
sun
also
rises
—
362239
characters
using
84
unique
letters
including
punctuation
uppercaselowercase
etc
this
data
set
is
actually
really
small
compared
to
typical
real-world
applications
to
generate
a
really
good
model
of
hemingway’s
style
it
would
be
much
better
to
have
at
several
times
as
much
sample
text
but
this
is
good
enough
to
play
around
with
as
an
example
as
we
just
start
to
train
the
rnn
it’s
not
very
good
at
predicting
letters
here’s
what
it
generates
after
a
100
loops
of
training:
you
can
see
that
it
has
figured
out
that
sometimes
words
have
spaces
between
them
but
that’s
about
it
after
about
1000
iterations
things
are
looking
more
promising:
the
model
has
started
to
identify
the
patterns
in
basic
sentence
structure
it’s
adding
periods
at
the
ends
of
sentences
and
even
quoting
dialog
a
few
words
are
recognizable
but
there’s
also
still
a
lot
of
nonsense
but
after
several
thousand
more
training
iterations
it
looks
pretty
good:
at
this
point
the
algorithm
has
captured
the
basic
pattern
of
hemingway’s
short
direct
dialog
a
few
sentences
even
sort
of
make
sense
compare
that
with
some
real
text
from
the
book:
even
by
only
looking
for
patterns
one
character
at
a
time
our
algorithm
has
reproduced
plausible-looking
prose
with
proper
formatting
that
is
kind
of
amazing!
we
don’t
have
to
generate
text
completely
from
scratch
either
we
can
seed
the
algorithm
by
supplying
the
first
few
letters
and
just
let
it
find
the
next
few
letters
for
fun
let’s
make
a
fake
book
cover
for
our
imaginary
book
by
generating
a
new
author
name
and
a
new
title
using
the
seed
text
of
er
he
and
the
s:
not
bad!
but
the
really
mind-blowing
part
is
that
this
algorithm
can
figure
out
patterns
in
any
sequence
of
data
it
can
easily
generate
real-looking
recipes
or
fake
obama
speeches
but
why
limit
ourselves
human
language?
we
can
apply
this
same
idea
to
any
kind
of
sequential
data
that
has
a
pattern
in
2015
nintendo
released
super
mario
makertm
for
the
wii
u
gaming
system
this
game
lets
you
draw
out
your
own
super
mario
brothers
levels
on
the
gamepad
and
then
upload
them
to
the
internet
so
you
friends
can
play
through
them
you
can
include
all
the
classic
power-ups
and
enemies
from
the
original
mario
games
in
your
levels
it’s
like
a
virtual
lego
set
for
people
who
grew
up
playing
super
mario
brothers
can
we
use
the
same
model
that
generated
fake
hemingway
text
to
generate
fake
super
mario
brothers
levels?
first
we
need
a
data
set
for
training
our
model
let’s
take
all
the
outdoor
levels
from
the
original
super
mario
brothers
game
released
in
1985:
this
game
has
32
levels
and
about
70%
of
them
have
the
same
outdoor
style
so
we’ll
stick
to
those
to
get
the
designs
for
each
level
i
took
an
original
copy
of
the
game
and
wrote
a
program
to
pull
the
level
designs
out
of
the
game’s
memory
super
mario
bros
is
a
30-year-old
game
and
there
are
lots
of
resources
online
that
help
you
figure
out
how
the
levels
were
stored
in
the
game’s
memory
extracting
level
data
from
an
old
video
game
is
a
fun
programming
exercise
that
you
should
try
sometime
here’s
the
first
level
from
the
game
which
you
probably
remember
if
you
ever
played
it:
if
we
look
closely
we
can
see
the
level
is
made
of
a
simple
grid
of
objects:
we
could
just
as
easily
represent
this
grid
as
a
sequence
of
characters
with
one
character
representing
each
object:
we’ve
replaced
each
object
in
the
level
with
a
letter:
and
so
on
using
a
different
letter
for
each
different
kind
of
object
in
the
level
i
ended
up
with
text
files
that
looked
like
this:
looking
at
the
text
file
you
can
see
that
mario
levels
don’t
really
have
much
of
a
pattern
if
you
read
them
line-by-line:
the
patterns
in
a
level
really
emerge
when
you
think
of
the
level
as
a
series
of
columns:
so
in
order
for
the
algorithm
to
find
the
patterns
in
our
data
we
need
to
feed
the
data
in
column-by-column
figuring
out
the
most
effective
representation
of
your
input
data
called
feature
selection
is
one
of
the
keys
of
using
machine
learning
algorithms
well
to
train
the
model
i
needed
to
rotate
my
text
files
by
90
degrees
this
made
sure
the
characters
were
fed
into
the
model
in
an
order
where
a
pattern
would
more
easily
show
up:
just
like
we
saw
when
creating
the
model
of
hemingway’s
prose
a
model
improves
as
we
train
it
after
a
little
training
our
model
is
generating
junk:
it
sort
of
has
an
idea
that
‘-’s
and
‘=’s
should
show
up
a
lot
but
that’s
about
it
it
hasn’t
figured
out
the
pattern
yet
after
several
thousand
iterations
it’s
starting
to
look
like
something:
the
model
has
almost
figured
out
that
each
line
should
be
the
same
length
it
has
even
started
to
figure
out
some
of
the
logic
of
mario:
the
pipes
in
mario
are
always
two
blocks
wide
and
at
least
two
blocks
high
so
the
ps
in
the
data
should
appear
in
2x2
clusters
that’s
pretty
cool!
with
a
lot
more
training
the
model
gets
to
the
point
where
it
generates
perfectly
valid
data:
let’s
sample
an
entire
level’s
worth
of
data
from
our
model
and
rotate
it
back
horizontal:
this
data
looks
great!
there
are
several
awesome
things
to
notice:
finally
let’s
take
this
level
and
recreate
it
in
super
mario
maker:
play
it
yourself!
if
you
have
super
mario
maker
you
can
play
this
level
by
bookmarking
it
online
or
by
looking
it
up
using
level
code
4ac9–0000–0157-f3c3
the
recurrent
neural
network
algorithm
we
used
to
train
our
model
is
the
same
kind
of
algorithm
used
by
real-world
companies
to
solve
hard
problems
like
speech
detection
and
language
translation
what
makes
our
model
a
‘toy’
instead
of
cutting-edge
is
that
our
model
is
generated
from
very
little
data
there
just
aren’t
enough
levels
in
the
original
super
mario
brothers
game
to
provide
enough
data
for
a
really
good
model
if
we
could
get
access
to
the
hundreds
of
thousands
of
user-created
super
mario
maker
levels
that
nintendo
has
we
could
make
an
amazing
model
but
we
can’t
—
because
nintendo
won’t
let
us
have
them
big
companies
don’t
give
away
their
data
for
free
as
machine
learning
becomes
more
important
in
more
industries
the
difference
between
a
good
program
and
a
bad
program
will
be
how
much
data
you
have
to
train
your
models
that’s
why
companies
like
google
and
facebook
need
your
data
so
badly!
for
example
google
recently
open
sourced
tensorflow
its
software
toolkit
for
building
large-scale
machine
learning
applications
it
was
a
pretty
big
deal
that
google
gave
away
such
important
capable
technology
for
free
this
is
the
same
stuff
that
powers
google
translate
but
without
google’s
massive
trove
of
data
in
every
language
you
can’t
create
a
competitor
to
google
translate
data
is
what
gives
google
its
edge
think
about
that
the
next
time
you
open
up
your
google
maps
location
history
or
facebook
location
history
and
notice
that
it
stores
every
place
you’ve
ever
been
in
machine
learning
there’s
never
a
single
way
to
solve
a
problem
you
have
limitless
options
when
deciding
how
to
pre-process
your
data
and
which
algorithms
to
use
often
combining
multiple
approaches
will
give
you
better
results
than
any
single
approach
readers
have
sent
me
links
to
other
interesting
approaches
to
generating
super
mario
levels:
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
email
list
i’ll
only
email
you
when
i
have
something
new
and
awesome
to
share
it’s
the
best
way
to
find
out
when
i
write
more
articles
like
this
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun
part
3!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
a
year
and
a
half
ago
i
dropped
out
of
one
of
the
best
computer
science
programs
in
canada
i
started
creating
my
own
data
science
master’s
program
using
online
resources
i
realized
that
i
could
learn
everything
i
needed
through
edx
coursera
and
udacity
instead
and
i
could
learn
it
faster
more
efficiently
and
for
a
fraction
of
the
cost
i’m
almost
finished
now
i’ve
taken
many
data
science-related
courses
and
audited
portions
of
many
more
i
know
the
options
out
there
and
what
skills
are
needed
for
learners
preparing
for
a
data
analyst
or
data
scientist
role
so
i
started
creating
a
review-driven
guide
that
recommends
the
best
courses
for
each
subject
within
data
science
for
the
first
guide
in
the
series
i
recommended
a
few
coding
classes
for
the
beginner
data
scientist
then
it
was
statistics
and
probability
classes
then
introductions
to
data
science
also
data
visualization
for
this
guide
i
spent
a
dozen
hours
trying
to
identify
every
online
machine
learning
course
offered
as
of
may
2017
extracting
key
bits
of
information
from
their
syllabi
and
reviews
and
compiling
their
ratings
my
end
goal
was
to
identify
the
three
best
courses
available
and
present
them
to
you
below
for
this
task
i
turned
to
none
other
than
the
open
source
class
central
community
and
its
database
of
thousands
of
course
ratings
and
reviews
since
2011
class
central
founder
dhawal
shah
has
kept
a
closer
eye
on
online
courses
than
arguably
anyone
else
in
the
world
dhawal
personally
helped
me
assemble
this
list
of
resources
each
course
must
fit
three
criteria:
we
believe
we
covered
every
notable
course
that
fits
the
above
criteria
since
there
are
seemingly
hundreds
of
courses
on
udemy
we
chose
to
consider
the
most-reviewed
and
highest-rated
ones
only
there’s
always
a
chance
that
we
missed
something
though
so
please
let
us
know
in
the
comments
section
if
we
left
a
good
course
out
we
compiled
average
ratings
and
number
of
reviews
from
class
central
and
other
review
sites
to
calculate
a
weighted
average
rating
for
each
course
we
read
text
reviews
and
used
this
feedback
to
supplement
the
numerical
ratings
we
made
subjective
syllabus
judgment
calls
based
on
three
factors:
a
popular
definition
originates
from
arthur
samuel
in
1959:
machine
learning
is
a
subfield
of
computer
science
that
gives
computers
the
ability
to
learn
without
being
explicitly
programmed
in
practice
this
means
developing
computer
programs
that
can
make
predictions
based
on
data
just
as
humans
can
learn
from
experience
so
can
computers
where
data
=
experience
a
machine
learning
workflow
is
the
process
required
for
carrying
out
a
machine
learning
project
though
individual
projects
can
differ
most
workflows
share
several
common
tasks:
problem
evaluation
data
exploration
data
preprocessing
model
trainingtestingdeployment
etc
below
you’ll
find
helpful
visualization
of
these
core
steps:
the
ideal
course
introduces
the
entire
process
and
provides
interactive
examples
assignments
andor
quizzes
where
students
can
perform
each
task
themselves
first
off
let’s
define
deep
learning
here
is
a
succinct
description:
as
would
be
expected
portions
of
some
of
the
machine
learning
courses
contain
deep
learning
content
i
chose
not
to
include
deep
learning-only
courses
however
if
you
are
interested
in
deep
learning
specifically
we’ve
got
you
covered
with
the
following
article:
my
top
three
recommendations
from
that
list
would
be:
several
courses
listed
below
ask
students
to
have
prior
programming
calculus
linear
algebra
and
statistics
experience
these
prerequisites
are
understandable
given
that
machine
learning
is
an
advanced
discipline
missing
a
few
subjects?
good
news!
some
of
this
experience
can
be
acquired
through
our
recommendations
in
the
first
two
articles
programming
statistics
of
this
data
science
career
guide
several
top-ranked
courses
below
also
provide
gentle
calculus
and
linear
algebra
refreshers
and
highlight
the
aspects
most
relevant
to
machine
learning
for
those
less
familiar
stanford
university’s
machine
learning
on
coursera
is
the
clear
current
winner
in
terms
of
ratings
reviews
and
syllabus
fit
taught
by
the
famous
andrew
ng
google
brain
founder
and
former
chief
scientist
at
baidu
this
was
the
class
that
sparked
the
founding
of
coursera
it
has
a
47-star
weighted
average
rating
over
422
reviews
released
in
2011
it
covers
all
aspects
of
the
machine
learning
workflow
though
it
has
a
smaller
scope
than
the
original
stanford
class
upon
which
it
is
based
it
still
manages
to
cover
a
large
number
of
techniques
and
algorithms
the
estimated
timeline
is
eleven
weeks
with
two
weeks
dedicated
to
neural
networks
and
deep
learning
free
and
paid
options
are
available
ng
is
a
dynamic
yet
gentle
instructor
with
a
palpable
experience
he
inspires
confidence
especially
when
sharing
practical
implementation
tips
and
warnings
about
common
pitfalls
a
linear
algebra
refresher
is
provided
and
ng
highlights
the
aspects
of
calculus
most
relevant
to
machine
learning
evaluation
is
automatic
and
is
done
via
multiple
choice
quizzes
that
follow
each
lesson
and
programming
assignments
the
assignments
there
are
eight
of
them
can
be
completed
in
matlab
or
octave
which
is
an
open-source
version
of
matlab
ng
explains
his
language
choice:
though
python
and
r
are
likely
more
compelling
choices
in
2017
with
the
increased
popularity
of
those
languages
reviewers
note
that
that
shouldn’t
stop
you
from
taking
the
course
a
few
prominent
reviewers
noted
the
following:
columbia
university’s
machine
learning
is
a
relatively
new
offering
that
is
part
of
their
artificial
intelligence
micromasters
on
edx
though
it
is
newer
and
doesn’t
have
a
large
number
of
reviews
the
ones
that
it
does
have
are
exceptionally
strong
professor
john
paisley
is
noted
as
brilliant
clear
and
clever
it
has
a
48-star
weighted
average
rating
over
10
reviews
the
course
also
covers
all
aspects
of
the
machine
learning
workflow
and
more
algorithms
than
the
above
stanford
offering
columbia’s
is
a
more
advanced
introduction
with
reviewers
noting
that
students
should
be
comfortable
with
the
recommended
prerequisites
calculus
linear
algebra
statistics
probability
and
coding
quizzes
11
programming
assignments
4
and
a
final
exam
are
the
modes
of
evaluation
students
can
use
either
python
octave
or
matlab
to
complete
the
assignments
the
course’s
total
estimated
timeline
is
eight
to
ten
hours
per
week
over
twelve
weeks
it
is
free
with
a
verified
certificate
available
for
purchase
below
are
a
few
of
the
aforementioned
sparkling
reviews:
machine
learning
a-ztm
on
udemy
is
an
impressively
detailed
offering
that
provides
instruction
in
both
python
and
r
which
is
rare
and
can’t
be
said
for
any
of
the
other
top
courses
it
has
a
45-star
weighted
average
rating
over
8119
reviews
which
makes
it
the
most
reviewed
course
of
the
ones
considered
it
covers
the
entire
machine
learning
workflow
and
an
almost
ridiculous
in
a
good
way
number
of
algorithms
through
405
hours
of
on-demand
video
the
course
takes
a
more
applied
approach
and
is
lighter
math-wise
than
the
above
two
courses
each
section
starts
with
an
intuition
video
from
eremenko
that
summarizes
the
underlying
theory
of
the
concept
being
taught
de
ponteves
then
walks
through
implementation
with
separate
videos
for
both
python
and
r
as
a
bonus
the
course
includes
python
and
r
code
templates
for
students
to
download
and
use
on
their
own
projects
there
are
quizzes
and
homework
challenges
though
these
aren’t
the
strong
points
of
the
course
eremenko
and
the
superdatascience
team
are
revered
for
their
ability
to
make
the
complex
simple
also
the
prerequisites
listed
are
just
some
high
school
mathematics
so
this
course
might
be
a
better
option
for
those
daunted
by
the
stanford
and
columbia
offerings
a
few
prominent
reviewers
noted
the
following:
our
#1
pick
had
a
weighted
average
rating
of
47
out
of
5
stars
over
422
reviews
let’s
look
at
the
other
alternatives
sorted
by
descending
rating
a
reminder
that
deep
learning-only
courses
are
not
included
in
this
guide
—
you
can
find
those
here
the
analytics
edge
massachusetts
institute
of
technologyedx:
more
focused
on
analytics
in
general
though
it
does
cover
several
machine
learning
topics
uses
r
strong
narrative
that
leverages
familiar
real-world
examples
challenging
ten
to
fifteen
hours
per
week
over
twelve
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
49-star
weighted
average
rating
over
214
reviews
python
for
data
science
and
machine
learning
bootcamp
jose
portillaudemy:
has
large
chunks
of
machine
learning
content
but
covers
the
whole
data
science
process
more
of
a
very
detailed
intro
to
python
amazing
course
though
not
ideal
for
the
scope
of
this
guide
215
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
46-star
weighted
average
rating
over
3316
reviews
data
science
and
machine
learning
bootcamp
with
r
jose
portillaudemy:
the
comments
for
portilla’s
above
course
apply
here
as
well
except
for
r
175
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
46-star
weighted
average
rating
over
1317
reviews
machine
learning
series
lazy
programmer
incudemy:
taught
by
a
data
scientistbig
data
engineerfull
stack
software
engineer
with
an
impressive
resume
lazy
programmer
currently
has
a
series
of
16
machine
learning-focused
courses
on
udemy
in
total
the
courses
have
5000
ratings
and
almost
all
of
them
have
46
stars
a
useful
course
ordering
is
provided
in
each
individual
course’s
description
uses
python
cost
varies
depending
on
udemy
discounts
which
are
frequent
machine
learning
georgia
techudacity:
a
compilation
of
what
was
three
separate
courses:
supervised
unsupervised
and
reinforcement
learning
part
of
udacity’s
machine
learning
engineer
nanodegree
and
georgia
tech’s
online
master’s
degree
oms
bite-sized
videos
as
is
udacity’s
style
friendly
professors
estimated
timeline
of
four
months
free
it
has
a
456-star
weighted
average
rating
over
9
reviews
implementing
predictive
analytics
with
spark
in
azure
hdinsight
microsoftedx:
introduces
the
core
concepts
of
machine
learning
and
a
variety
of
algorithms
leverages
several
big
data-friendly
tools
including
apache
spark
scala
and
hadoop
uses
both
python
and
r
four
hours
per
week
over
six
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
45-star
weighted
average
rating
over
6
reviews
data
science
and
machine
learning
with
python
—
hands
on!
frank
kaneudemy:
uses
python
kane
has
nine
years
of
experience
at
amazon
and
imdb
nine
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
45-star
weighted
average
rating
over
4139
reviews
scala
and
spark
for
big
data
and
machine
learning
jose
portillaudemy:
big
data
focus
specifically
on
implementation
in
scala
and
spark
ten
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
45-star
weighted
average
rating
over
607
reviews
machine
learning
engineer
nanodegree
udacity:
udacity’s
flagship
machine
learning
program
which
features
a
best-in-class
project
review
system
and
career
support
the
program
is
a
compilation
of
several
individual
udacity
courses
which
are
free
co-created
by
kaggle
estimated
timeline
of
six
months
currently
costs
$199
usd
per
month
with
a
50%
tuition
refund
available
for
those
who
graduate
within
12
months
it
has
a
45-star
weighted
average
rating
over
2
reviews
learning
from
data
introductory
machine
learning
california
institute
of
technologyedx:
enrollment
is
currently
closed
on
edx
but
is
also
available
via
caltech’s
independent
platform
see
below
it
has
a
449-star
weighted
average
rating
over
42
reviews
learning
from
data
introductory
machine
learning
yaser
abu-mostafacalifornia
institute
of
technology:
a
real
caltech
course
not
a
watered-down
version
reviews
note
it
is
excellent
for
understanding
machine
learning
theory
the
professor
yaser
abu-mostafa
is
popular
among
students
and
also
wrote
the
textbook
upon
which
this
course
is
based
videos
are
taped
lectures
with
lectures
slides
picture-in-picture
uploaded
to
youtube
homework
assignments
are
pdf
files
the
course
experience
for
online
students
isn’t
as
polished
as
the
top
three
recommendations
it
has
a
443-star
weighted
average
rating
over
7
reviews
mining
massive
datasets
stanford
university:
machine
learning
with
a
focus
on
big
data
introduces
modern
distributed
file
systems
and
mapreduce
ten
hours
per
week
over
seven
weeks
free
it
has
a
44-star
weighted
average
rating
over
30
reviews
aws
machine
learning:
a
complete
guide
with
python
chandra
lingamudemy:
a
unique
focus
on
cloud-based
machine
learning
and
specifically
amazon
web
services
uses
python
nine
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
44-star
weighted
average
rating
over
62
reviews
introduction
to
machine
learning
""
face
detection
in
python
holczer
balazsudemy:
uses
python
eight
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
44-star
weighted
average
rating
over
162
reviews
statlearning:
statistical
learning
stanford
university:
based
on
the
excellent
textbook
an
introduction
to
statistical
learning
with
applications
in
r
and
taught
by
the
professors
who
wrote
it
reviewers
note
that
the
mooc
isn’t
as
good
as
the
book
citing
thin
exercises
and
mediocre
videos
five
hours
per
week
over
nine
weeks
free
it
has
a
435-star
weighted
average
rating
over
84
reviews
machine
learning
specialization
university
of
washingtoncoursera:
great
courses
but
last
two
classes
including
the
capstone
project
were
canceled
reviewers
note
that
this
series
is
more
digestable
read:
easier
for
those
without
strong
technical
backgrounds
than
other
top
machine
learning
courses
eg
stanford’s
or
caltech’s
be
aware
that
the
series
is
incomplete
with
recommender
systems
deep
learning
and
a
summary
missing
free
and
paid
options
available
it
has
a
431-star
weighted
average
rating
over
80
reviews
from
0
to
1:
machine
learning
nlp
""
python-cut
to
the
chase
loony
cornudemy:
a
down-to-earth
shy
but
confident
take
on
machine
learning
techniques
taught
by
four-person
team
with
decades
of
industry
experience
together
uses
python
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
42-star
weighted
average
rating
over
494
reviews
principles
of
machine
learning
microsoftedx:
uses
r
python
and
microsoft
azure
machine
learning
part
of
the
microsoft
professional
program
certificate
in
data
science
three
to
four
hours
per
week
over
six
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
409-star
weighted
average
rating
over
11
reviews
big
data:
statistical
inference
and
machine
learning
queensland
university
of
technologyfuturelearn:
a
nice
brief
exploratory
machine
learning
course
with
a
focus
on
big
data
covers
a
few
tools
like
r
h2o
flow
and
weka
only
three
weeks
in
duration
at
a
recommended
two
hours
per
week
but
one
reviewer
noted
that
six
hours
per
week
would
be
more
appropriate
free
and
paid
options
available
it
has
a
4-star
weighted
average
rating
over
4
reviews
genomic
data
science
and
clustering
bioinformatics
v
university
of
california
san
diegocoursera:
for
those
interested
in
the
intersection
of
computer
science
and
biology
and
how
it
represents
an
important
frontier
in
modern
science
focuses
on
clustering
and
dimensionality
reduction
part
of
ucsd’s
bioinformatics
specialization
free
and
paid
options
available
it
has
a
4-star
weighted
average
rating
over
3
reviews
intro
to
machine
learning
udacity:
prioritizes
topic
breadth
and
practical
tools
in
python
over
depth
and
theory
the
instructors
sebastian
thrun
and
katie
malone
make
this
class
so
fun
consists
of
bite-sized
videos
and
quizzes
followed
by
a
mini-project
for
each
lesson
currently
part
of
udacity’s
data
analyst
nanodegree
estimated
timeline
of
ten
weeks
free
it
has
a
395-star
weighted
average
rating
over
19
reviews
machine
learning
for
data
analysis
wesleyan
universitycoursera:
a
brief
intro
machine
learning
and
a
few
select
algorithms
covers
decision
trees
random
forests
lasso
regression
and
k-means
clustering
part
of
wesleyan’s
data
analysis
and
interpretation
specialization
estimated
timeline
of
four
weeks
free
and
paid
options
available
it
has
a
36-star
weighted
average
rating
over
5
reviews
programming
with
python
for
data
science
microsoftedx:
produced
by
microsoft
in
partnership
with
coding
dojo
uses
python
eight
hours
per
week
over
six
weeks
free
and
paid
options
available
it
has
a
346-star
weighted
average
rating
over
37
reviews
machine
learning
for
trading
georgia
techudacity:
focuses
on
applying
probabilistic
machine
learning
approaches
to
trading
decisions
uses
python
part
of
udacity’s
machine
learning
engineer
nanodegree
and
georgia
tech’s
online
master’s
degree
oms
estimated
timeline
of
four
months
free
it
has
a
329-star
weighted
average
rating
over
14
reviews
practical
machine
learning
johns
hopkins
universitycoursera:
a
brief
practical
introduction
to
a
number
of
machine
learning
algorithms
several
onetwo-star
reviews
expressing
a
variety
of
concerns
part
of
jhu’s
data
science
specialization
four
to
nine
hours
per
week
over
four
weeks
free
and
paid
options
available
it
has
a
311-star
weighted
average
rating
over
37
reviews
machine
learning
for
data
science
and
analytics
columbia
universityedx:
introduces
a
wide
range
of
machine
learning
topics
some
passionate
negative
reviews
with
concerns
including
content
choices
a
lack
of
programming
assignments
and
uninspiring
presentation
seven
to
ten
hours
per
week
over
five
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
274-star
weighted
average
rating
over
36
reviews
recommender
systems
specialization
university
of
minnesotacoursera:
strong
focus
one
specific
type
of
machine
learning
—
recommender
systems
a
four
course
specialization
plus
a
capstone
project
which
is
a
case
study
taught
using
lenskit
an
open-source
toolkit
for
recommender
systems
free
and
paid
options
available
it
has
a
2-star
weighted
average
rating
over
2
reviews
machine
learning
with
big
data
university
of
california
san
diegocoursera:
terrible
reviews
that
highlight
poor
instruction
and
evaluation
some
noted
it
took
them
mere
hours
to
complete
the
whole
course
part
of
ucsd’s
big
data
specialization
free
and
paid
options
available
it
has
a
186-star
weighted
average
rating
over
14
reviews
practical
predictive
analytics:
models
and
methods
university
of
washingtoncoursera:
a
brief
intro
to
core
machine
learning
concepts
one
reviewer
noted
that
there
was
a
lack
of
quizzes
and
that
the
assignments
were
not
challenging
part
of
uw’s
data
science
at
scale
specialization
six
to
eight
hours
per
week
over
four
weeks
free
and
paid
options
available
it
has
a
175-star
weighted
average
rating
over
4
reviews
the
following
courses
had
one
or
no
reviews
as
of
may
2017
machine
learning
for
musicians
and
artists
goldsmiths
university
of
londonkadenze:
unique
students
learn
algorithms
software
tools
and
machine
learning
best
practices
to
make
sense
of
human
gesture
musical
audio
and
other
real-time
data
seven
sessions
in
length
audit
free
and
premium
$10
usd
per
month
options
available
it
has
one
5-star
review
applied
machine
learning
in
python
university
of
michigancoursera:
taught
using
python
and
the
scikit
learn
toolkit
part
of
the
applied
data
science
with
python
specialization
scheduled
to
start
may
29th
free
and
paid
options
available
applied
machine
learning
microsoftedx:
taught
using
various
tools
including
python
r
and
microsoft
azure
machine
learning
note:
microsoft
produces
the
course
includes
hands-on
labs
to
reinforce
the
lecture
content
three
to
four
hours
per
week
over
six
weeks
free
with
a
verified
certificate
available
for
purchase
machine
learning
with
python
big
data
university:
taught
using
python
targeted
towards
beginners
estimated
completion
time
of
four
hours
big
data
university
is
affiliated
with
ibm
free
machine
learning
with
apache
systemml
big
data
university:
taught
using
apache
systemml
which
is
a
declarative
style
language
designed
for
large-scale
machine
learning
estimated
completion
time
of
eight
hours
big
data
university
is
affiliated
with
ibm
free
machine
learning
for
data
science
university
of
california
san
diegoedx:
doesn’t
launch
until
january
2018
programming
examples
and
assignments
are
in
python
using
jupyter
notebooks
eight
hours
per
week
over
ten
weeks
free
with
a
verified
certificate
available
for
purchase
introduction
to
analytics
modeling
georgia
techedx:
the
course
advertises
r
as
its
primary
programming
tool
five
to
ten
hours
per
week
over
ten
weeks
free
with
a
verified
certificate
available
for
purchase
predictive
analytics:
gaining
insights
from
big
data
queensland
university
of
technologyfuturelearn:
brief
overview
of
a
few
algorithms
uses
hewlett
packard
enterprise’s
vertica
analytics
platform
as
an
applied
tool
start
date
to
be
announced
two
hours
per
week
over
four
weeks
free
with
a
certificate
of
achievement
available
for
purchase
introducción
al
machine
learning
universitas
telefónicamiríada
x:
taught
in
spanish
an
introduction
to
machine
learning
that
covers
supervised
and
unsupervised
learning
a
total
of
twenty
estimated
hours
over
four
weeks
machine
learning
path
step
dataquest:
taught
in
python
using
dataquest’s
interactive
in-browser
platform
multiple
guided
projects
and
a
plus
project
where
you
build
your
own
machine
learning
system
using
your
own
data
subscription
required
the
following
six
courses
are
offered
by
datacamp
datacamp’s
hybrid
teaching
style
leverages
video
and
text-based
instruction
with
lots
of
examples
through
an
in-browser
code
editor
a
subscription
is
required
for
full
access
to
each
course
introduction
to
machine
learning
datacamp:
covers
classification
regression
and
clustering
algorithms
uses
r
fifteen
videos
and
81
exercises
with
an
estimated
timeline
of
six
hours
supervised
learning
with
scikit-learn
datacamp:
uses
python
and
scikit-learn
covers
classification
and
regression
algorithms
seventeen
videos
and
54
exercises
with
an
estimated
timeline
of
four
hours
unsupervised
learning
in
r
datacamp:
provides
a
basic
introduction
to
clustering
and
dimensionality
reduction
in
r
sixteen
videos
and
49
exercises
with
an
estimated
timeline
of
four
hours
machine
learning
toolbox
datacamp:
teaches
the
big
ideas
in
machine
learning
uses
r
24
videos
and
88
exercises
with
an
estimated
timeline
of
four
hours
machine
learning
with
the
experts:
school
budgets
datacamp:
a
case
study
from
a
machine
learning
competition
on
drivendata
involves
building
a
model
to
automatically
classify
items
in
a
school’s
budget
datacamp’s
supervised
learning
with
scikit-learn
is
a
prerequisite
fifteen
videos
and
51
exercises
with
an
estimated
timeline
of
four
hours
unsupervised
learning
in
python
datacamp:
covers
a
variety
of
unsupervised
learning
algorithms
using
python
scikit-learn
and
scipy
the
course
ends
with
students
building
a
recommender
system
to
recommend
popular
musical
artists
thirteen
videos
and
52
exercises
with
an
estimated
timeline
of
four
hours
machine
learning
tom
mitchellcarnegie
mellon
university:
carnegie
mellon’s
graduate
introductory
machine
learning
course
a
prerequisite
to
their
second
graduate
level
course
statistical
machine
learning
taped
university
lectures
with
practice
problems
homework
assignments
and
a
midterm
all
with
solutions
posted
online
a
2011
version
of
the
course
also
exists
cmu
is
one
of
the
best
graduate
schools
for
studying
machine
learning
and
has
a
whole
department
dedicated
to
ml
free
statistical
machine
learning
larry
wassermancarnegie
mellon
university:
likely
the
most
advanced
course
in
this
guide
a
follow-up
to
carnegie
mellon’s
machine
learning
course
taped
university
lectures
with
practice
problems
homework
assignments
and
a
midterm
all
with
solutions
posted
online
free
undergraduate
machine
learning
nando
de
freitasuniversity
of
british
columbia:
an
undergraduate
machine
learning
course
lectures
are
filmed
and
put
on
youtube
with
the
slides
posted
on
the
course
website
the
course
assignments
are
posted
as
well
no
solutions
though
de
freitas
is
now
a
full-time
professor
at
the
university
of
oxford
and
receives
praise
for
his
teaching
abilities
in
various
forums
graduate
version
available
see
below
machine
learning
nando
de
freitasuniversity
of
british
columbia:
a
graduate
machine
learning
course
the
comments
in
de
freitas’
undergraduate
course
above
apply
here
as
well
this
is
the
fifth
of
a
six-piece
series
that
covers
the
best
online
courses
for
launching
yourself
into
the
data
science
field
we
covered
programming
in
the
first
article
statistics
and
probability
in
the
second
article
intros
to
data
science
in
the
third
article
and
data
visualization
in
the
fourth
the
final
piece
will
be
a
summary
of
those
articles
plus
the
best
online
courses
for
other
key
topics
such
as
data
wrangling
databases
and
even
software
engineering
if
you’re
looking
for
a
complete
list
of
data
science
online
courses
you
can
find
them
on
class
central’s
data
science
and
big
data
subject
page
if
you
enjoyed
reading
this
check
out
some
of
class
central’s
other
pieces:
if
you
have
suggestions
for
courses
i
missed
let
me
know
in
the
responses!
if
you
found
this
helpful
click
the
💚
so
more
people
will
see
it
here
on
medium
this
is
a
condensed
version
of
my
original
article
published
on
class
central
where
i’ve
included
detailed
course
syllabi
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
curriculum
lead
projects
@
datacamp
i
created
my
own
data
science
master’s
program
our
community
publishes
stories
worth
reading
on
development
design
and
data
science
""
artificial
intelligence
ai
is
the
mantra
of
the
current
era
the
phrase
is
intoned
by
technologists
academicians
journalists
and
venture
capitalists
alike
as
with
many
phrases
that
cross
over
from
technical
academic
fields
into
general
circulation
there
is
significant
misunderstanding
accompanying
the
use
of
the
phrase
but
this
is
not
the
classical
case
of
the
public
not
understanding
the
scientists
—
here
the
scientists
are
often
as
befuddled
as
the
public
the
idea
that
our
era
is
somehow
seeing
the
emergence
of
an
intelligence
in
silicon
that
rivals
our
own
entertains
all
of
us
—
enthralling
us
and
frightening
us
in
equal
measure
and
unfortunately
it
distracts
us
there
is
a
different
narrative
that
one
can
tell
about
the
current
era
consider
the
following
story
which
involves
humans
computers
data
and
life-or-death
decisions
but
where
the
focus
is
something
other
than
intelligence-in-silicon
fantasies
when
my
spouse
was
pregnant
14
years
ago
we
had
an
ultrasound
there
was
a
geneticist
in
the
room
and
she
pointed
out
some
white
spots
around
the
heart
of
the
fetus
those
are
markers
for
down
syndrome
she
noted
and
your
risk
has
now
gone
up
to
1
in
20
she
further
let
us
know
that
we
could
learn
whether
the
fetus
in
fact
had
the
genetic
modification
underlying
down
syndrome
via
an
amniocentesis
but
amniocentesis
was
risky
—
the
risk
of
killing
the
fetus
during
the
procedure
was
roughly
1
in
300
being
a
statistician
i
determined
to
find
out
where
these
numbers
were
coming
from
to
cut
a
long
story
short
i
discovered
that
a
statistical
analysis
had
been
done
a
decade
previously
in
the
uk
where
these
white
spots
which
reflect
calcium
buildup
were
indeed
established
as
a
predictor
of
down
syndrome
but
i
also
noticed
that
the
imaging
machine
used
in
our
test
had
a
few
hundred
more
pixels
per
square
inch
than
the
machine
used
in
the
uk
study
i
went
back
to
tell
the
geneticist
that
i
believed
that
the
white
spots
were
likely
false
positives
—
that
they
were
literally
white
noise
she
said
ah
that
explains
why
we
started
seeing
an
uptick
in
down
syndrome
diagnoses
a
few
years
ago
it’s
when
the
new
machine
arrived
we
didn’t
do
the
amniocentesis
and
a
healthy
girl
was
born
a
few
months
later
but
the
episode
troubled
me
particularly
after
a
back-of-the-envelope
calculation
convinced
me
that
many
thousands
of
people
had
gotten
that
diagnosis
that
same
day
worldwide
that
many
of
them
had
opted
for
amniocentesis
and
that
a
number
of
babies
had
died
needlessly
and
this
happened
day
after
day
until
it
somehow
got
fixed
the
problem
that
this
episode
revealed
wasn’t
about
my
individual
medical
care
it
was
about
a
medical
system
that
measured
variables
and
outcomes
in
various
places
and
times
conducted
statistical
analyses
and
made
use
of
the
results
in
other
places
and
times
the
problem
had
to
do
not
just
with
data
analysis
per
se
but
with
what
database
researchers
call
provenance
—
broadly
where
did
data
arise
what
inferences
were
drawn
from
the
data
and
how
relevant
are
those
inferences
to
the
present
situation?
while
a
trained
human
might
be
able
to
work
all
of
this
out
on
a
case-by-case
basis
the
issue
was
that
of
designing
a
planetary-scale
medical
system
that
could
do
this
without
the
need
for
such
detailed
human
oversight
i’m
also
a
computer
scientist
and
it
occurred
to
me
that
the
principles
needed
to
build
planetary-scale
inference-and-decision-making
systems
of
this
kind
blending
computer
science
with
statistics
and
taking
into
account
human
utilities
were
nowhere
to
be
found
in
my
education
and
it
occurred
to
me
that
the
development
of
such
principles
—
which
will
be
needed
not
only
in
the
medical
domain
but
also
in
domains
such
as
commerce
transportation
and
education
—
were
at
least
as
important
as
those
of
building
ai
systems
that
can
dazzle
us
with
their
game-playing
or
sensorimotor
skills
whether
or
not
we
come
to
understand
intelligence
any
time
soon
we
do
have
a
major
challenge
on
our
hands
in
bringing
together
computers
and
humans
in
ways
that
enhance
human
life
while
this
challenge
is
viewed
by
some
as
subservient
to
the
creation
of
artificial
intelligence
it
can
also
be
viewed
more
prosaically
—
but
with
no
less
reverence
—
as
the
creation
of
a
new
branch
of
engineering
much
like
civil
engineering
and
chemical
engineering
in
decades
past
this
new
discipline
aims
to
corral
the
power
of
a
few
key
ideas
bringing
new
resources
and
capabilities
to
people
and
doing
so
safely
whereas
civil
engineering
and
chemical
engineering
were
built
on
physics
and
chemistry
this
new
engineering
discipline
will
be
built
on
ideas
that
the
preceding
century
gave
substance
to
—
ideas
such
as
information
algorithm
data
uncertainty
computing
inference
and
optimization
moreover
since
much
of
the
focus
of
the
new
discipline
will
be
on
data
from
and
about
humans
its
development
will
require
perspectives
from
the
social
sciences
and
humanities
while
the
building
blocks
have
begun
to
emerge
the
principles
for
putting
these
blocks
together
have
not
yet
emerged
and
so
the
blocks
are
currently
being
put
together
in
ad-hoc
ways
thus
just
as
humans
built
buildings
and
bridges
before
there
was
civil
engineering
humans
are
proceeding
with
the
building
of
societal-scale
inference-and-decision-making
systems
that
involve
machines
humans
and
the
environment
just
as
early
buildings
and
bridges
sometimes
fell
to
the
ground
—
in
unforeseen
ways
and
with
tragic
consequences
—
many
of
our
early
societal-scale
inference-and-decision-making
systems
are
already
exposing
serious
conceptual
flaws
and
unfortunately
we
are
not
very
good
at
anticipating
what
the
next
emerging
serious
flaw
will
be
what
we’re
missing
is
an
engineering
discipline
with
its
principles
of
analysis
and
design
the
current
public
dialog
about
these
issues
too
often
uses
ai
as
an
intellectual
wildcard
one
that
makes
it
difficult
to
reason
about
the
scope
and
consequences
of
emerging
technology
let
us
begin
by
considering
more
carefully
what
ai
has
been
used
to
refer
to
both
recently
and
historically
most
of
what
is
being
called
ai
today
particularly
in
the
public
sphere
is
what
has
been
called
machine
learning
ml
for
the
past
several
decades
ml
is
an
algorithmic
field
that
blends
ideas
from
statistics
computer
science
and
many
other
disciplines
see
below
to
design
algorithms
that
process
data
make
predictions
and
help
make
decisions
in
terms
of
impact
on
the
real
world
ml
is
the
real
thing
and
not
just
recently
indeed
that
ml
would
grow
into
massive
industrial
relevance
was
already
clear
in
the
early
1990s
and
by
the
turn
of
the
century
forward-looking
companies
such
as
amazon
were
already
using
ml
throughout
their
business
solving
mission-critical
back-end
problems
in
fraud
detection
and
supply-chain
prediction
and
building
innovative
consumer-facing
services
such
as
recommendation
systems
as
datasets
and
computing
resources
grew
rapidly
over
the
ensuing
two
decades
it
became
clear
that
ml
would
soon
power
not
only
amazon
but
essentially
any
company
in
which
decisions
could
be
tied
to
large-scale
data
new
business
models
would
emerge
the
phrase
data
science
began
to
be
used
to
refer
to
this
phenomenon
reflecting
the
need
of
ml
algorithms
experts
to
partner
with
database
and
distributed-systems
experts
to
build
scalable
robust
ml
systems
and
reflecting
the
larger
social
and
environmental
scope
of
the
resulting
systems
this
confluence
of
ideas
and
technology
trends
has
been
rebranded
as
ai
over
the
past
few
years
this
rebranding
is
worthy
of
some
scrutiny
historically
the
phrase
ai
was
coined
in
the
late
1950’s
to
refer
to
the
heady
aspiration
of
realizing
in
software
and
hardware
an
entity
possessing
human-level
intelligence
we
will
use
the
phrase
human-imitative
ai
to
refer
to
this
aspiration
emphasizing
the
notion
that
the
artificially
intelligent
entity
should
seem
to
be
one
of
us
if
not
physically
at
least
mentally
whatever
that
might
mean
this
was
largely
an
academic
enterprise
while
related
academic
fields
such
as
operations
research
statistics
pattern
recognition
information
theory
and
control
theory
already
existed
and
were
often
inspired
by
human
intelligence
and
animal
intelligence
these
fields
were
arguably
focused
on
low-level
signals
and
decisions
the
ability
of
say
a
squirrel
to
perceive
the
three-dimensional
structure
of
the
forest
it
lives
in
and
to
leap
among
its
branches
was
inspirational
to
these
fields
ai
was
meant
to
focus
on
something
different
—
the
high-level
or
cognitive
capability
of
humans
to
reason
and
to
think
sixty
years
later
however
high-level
reasoning
and
thought
remain
elusive
the
developments
which
are
now
being
called
ai
arose
mostly
in
the
engineering
fields
associated
with
low-level
pattern
recognition
and
movement
control
and
in
the
field
of
statistics
—
the
discipline
focused
on
finding
patterns
in
data
and
on
making
well-founded
predictions
tests
of
hypotheses
and
decisions
indeed
the
famous
backpropagation
algorithm
that
was
rediscovered
by
david
rumelhart
in
the
early
1980s
and
which
is
now
viewed
as
being
at
the
core
of
the
so-called
ai
revolution
first
arose
in
the
field
of
control
theory
in
the
1950s
and
1960s
one
of
its
early
applications
was
to
optimize
the
thrusts
of
the
apollo
spaceships
as
they
headed
towards
the
moon
since
the
1960s
much
progress
has
been
made
but
it
has
arguably
not
come
about
from
the
pursuit
of
human-imitative
ai
rather
as
in
the
case
of
the
apollo
spaceships
these
ideas
have
often
been
hidden
behind
the
scenes
and
have
been
the
handiwork
of
researchers
focused
on
specific
engineering
challenges
although
not
visible
to
the
general
public
research
and
systems-building
in
areas
such
as
document
retrieval
text
classification
fraud
detection
recommendation
systems
personalized
search
social
network
analysis
planning
diagnostics
and
ab
testing
have
been
a
major
success
—
these
are
the
advances
that
have
powered
companies
such
as
google
netflix
facebook
and
amazon
one
could
simply
agree
to
refer
to
all
of
this
as
ai
and
indeed
that
is
what
appears
to
have
happened
such
labeling
may
come
as
a
surprise
to
optimization
or
statistics
researchers
who
wake
up
to
find
themselves
suddenly
referred
to
as
ai
researchers
but
labeling
of
researchers
aside
the
bigger
problem
is
that
the
use
of
this
single
ill-defined
acronym
prevents
a
clear
understanding
of
the
range
of
intellectual
and
commercial
issues
at
play
the
past
two
decades
have
seen
major
progress
—
in
industry
and
academia
—
in
a
complementary
aspiration
to
human-imitative
ai
that
is
often
referred
to
as
intelligence
augmentation
ia
here
computation
and
data
are
used
to
create
services
that
augment
human
intelligence
and
creativity
a
search
engine
can
be
viewed
as
an
example
of
ia
it
augments
human
memory
and
factual
knowledge
as
can
natural
language
translation
it
augments
the
ability
of
a
human
to
communicate
computing-based
generation
of
sounds
and
images
serves
as
a
palette
and
creativity
enhancer
for
artists
while
services
of
this
kind
could
conceivably
involve
high-level
reasoning
and
thought
currently
they
don’t
—
they
mostly
perform
various
kinds
of
string-matching
and
numerical
operations
that
capture
patterns
that
humans
can
make
use
of
hoping
that
the
reader
will
tolerate
one
last
acronym
let
us
conceive
broadly
of
a
discipline
of
intelligent
infrastructure
ii
whereby
a
web
of
computation
data
and
physical
entities
exists
that
makes
human
environments
more
supportive
interesting
and
safe
such
infrastructure
is
beginning
to
make
its
appearance
in
domains
such
as
transportation
medicine
commerce
and
finance
with
vast
implications
for
individual
humans
and
societies
this
emergence
sometimes
arises
in
conversations
about
an
internet
of
things
but
that
effort
generally
refers
to
the
mere
problem
of
getting
things
onto
the
internet
—
not
to
the
far
grander
set
of
challenges
associated
with
these
things
capable
of
analyzing
those
data
streams
to
discover
facts
about
the
world
and
interacting
with
humans
and
other
things
at
a
far
higher
level
of
abstraction
than
mere
bits
for
example
returning
to
my
personal
anecdote
we
might
imagine
living
our
lives
in
a
societal-scale
medical
system
that
sets
up
data
flows
and
data-analysis
flows
between
doctors
and
devices
positioned
in
and
around
human
bodies
thereby
able
to
aid
human
intelligence
in
making
diagnoses
and
providing
care
the
system
would
incorporate
information
from
cells
in
the
body
dna
blood
tests
environment
population
genetics
and
the
vast
scientific
literature
on
drugs
and
treatments
it
would
not
just
focus
on
a
single
patient
and
a
doctor
but
on
relationships
among
all
humans
—
just
as
current
medical
testing
allows
experiments
done
on
one
set
of
humans
or
animals
to
be
brought
to
bear
in
the
care
of
other
humans
it
would
help
maintain
notions
of
relevance
provenance
and
reliability
in
the
way
that
the
current
banking
system
focuses
on
such
challenges
in
the
domain
of
finance
and
payment
and
while
one
can
foresee
many
problems
arising
in
such
a
system
—
involving
privacy
issues
liability
issues
security
issues
etc
—
these
problems
should
properly
be
viewed
as
challenges
not
show-stoppers
we
now
come
to
a
critical
issue:
is
working
on
classical
human-imitative
ai
the
best
or
only
way
to
focus
on
these
larger
challenges?
some
of
the
most
heralded
recent
success
stories
of
ml
have
in
fact
been
in
areas
associated
with
human-imitative
ai
—
areas
such
as
computer
vision
speech
recognition
game-playing
and
robotics
so
perhaps
we
should
simply
await
further
progress
in
domains
such
as
these
there
are
two
points
to
make
here
first
although
one
would
not
know
it
from
reading
the
newspapers
success
in
human-imitative
ai
has
in
fact
been
limited
—
we
are
very
far
from
realizing
human-imitative
ai
aspirations
unfortunately
the
thrill
and
fear
of
making
even
limited
progress
on
human-imitative
ai
gives
rise
to
levels
of
over-exuberance
and
media
attention
that
is
not
present
in
other
areas
of
engineering
second
and
more
importantly
success
in
these
domains
is
neither
sufficient
nor
necessary
to
solve
important
ia
and
ii
problems
on
the
sufficiency
side
consider
self-driving
cars
for
such
technology
to
be
realized
a
range
of
engineering
problems
will
need
to
be
solved
that
may
have
little
relationship
to
human
competencies
or
human
lack-of-competencies
the
overall
transportation
system
an
ii
system
will
likely
more
closely
resemble
the
current
air-traffic
control
system
than
the
current
collection
of
loosely-coupled
forward-facing
inattentive
human
drivers
it
will
be
vastly
more
complex
than
the
current
air-traffic
control
system
specifically
in
its
use
of
massive
amounts
of
data
and
adaptive
statistical
modeling
to
inform
fine-grained
decisions
it
is
those
challenges
that
need
to
be
in
the
forefront
and
in
such
an
effort
a
focus
on
human-imitative
ai
may
be
a
distraction
as
for
the
necessity
argument
it
is
sometimes
argued
that
the
human-imitative
ai
aspiration
subsumes
ia
and
ii
aspirations
because
a
human-imitative
ai
system
would
not
only
be
able
to
solve
the
classical
problems
of
ai
as
embodied
eg
in
the
turing
test
but
it
would
also
be
our
best
bet
for
solving
ia
and
ii
problems
such
an
argument
has
little
historical
precedent
did
civil
engineering
develop
by
envisaging
the
creation
of
an
artificial
carpenter
or
bricklayer?
should
chemical
engineering
have
been
framed
in
terms
of
creating
an
artificial
chemist?
even
more
polemically:
if
our
goal
was
to
build
chemical
factories
should
we
have
first
created
an
artificial
chemist
who
would
have
then
worked
out
how
to
build
a
chemical
factory?
a
related
argument
is
that
human
intelligence
is
the
only
kind
of
intelligence
that
we
know
and
that
we
should
aim
to
mimic
it
as
a
first
step
but
humans
are
in
fact
not
very
good
at
some
kinds
of
reasoning
—
we
have
our
lapses
biases
and
limitations
moreover
critically
we
did
not
evolve
to
perform
the
kinds
of
large-scale
decision-making
that
modern
ii
systems
must
face
nor
to
cope
with
the
kinds
of
uncertainty
that
arise
in
ii
contexts
one
could
argue
that
an
ai
system
would
not
only
imitate
human
intelligence
but
also
correct
it
and
would
also
scale
to
arbitrarily
large
problems
but
we
are
now
in
the
realm
of
science
fiction
—
such
speculative
arguments
while
entertaining
in
the
setting
of
fiction
should
not
be
our
principal
strategy
going
forward
in
the
face
of
the
critical
ia
and
ii
problems
that
are
beginning
to
emerge
we
need
to
solve
ia
and
ii
problems
on
their
own
merits
not
as
a
mere
corollary
to
a
human-imitative
ai
agenda
it
is
not
hard
to
pinpoint
algorithmic
and
infrastructure
challenges
in
ii
systems
that
are
not
central
themes
in
human-imitative
ai
research
ii
systems
require
the
ability
to
manage
distributed
repositories
of
knowledge
that
are
rapidly
changing
and
are
likely
to
be
globally
incoherent
such
systems
must
cope
with
cloud-edge
interactions
in
making
timely
distributed
decisions
and
they
must
deal
with
long-tail
phenomena
whereby
there
is
lots
of
data
on
some
individuals
and
little
data
on
most
individuals
they
must
address
the
difficulties
of
sharing
data
across
administrative
and
competitive
boundaries
finally
and
of
particular
importance
ii
systems
must
bring
economic
ideas
such
as
incentives
and
pricing
into
the
realm
of
the
statistical
and
computational
infrastructures
that
link
humans
to
each
other
and
to
valued
goods
such
ii
systems
can
be
viewed
as
not
merely
providing
a
service
but
as
creating
markets
there
are
domains
such
as
music
literature
and
journalism
that
are
crying
out
for
the
emergence
of
such
markets
where
data
analysis
links
producers
and
consumers
and
this
must
all
be
done
within
the
context
of
evolving
societal
ethical
and
legal
norms
of
course
classical
human-imitative
ai
problems
remain
of
great
interest
as
well
however
the
current
focus
on
doing
ai
research
via
the
gathering
of
data
the
deployment
of
deep
learning
infrastructure
and
the
demonstration
of
systems
that
mimic
certain
narrowly-defined
human
skills
—
with
little
in
the
way
of
emerging
explanatory
principles
—
tends
to
deflect
attention
from
major
open
problems
in
classical
ai
these
problems
include
the
need
to
bring
meaning
and
reasoning
into
systems
that
perform
natural
language
processing
the
need
to
infer
and
represent
causality
the
need
to
develop
computationally-tractable
representations
of
uncertainty
and
the
need
to
develop
systems
that
formulate
and
pursue
long-term
goals
these
are
classical
goals
in
human-imitative
ai
but
in
the
current
hubbub
over
the
ai
revolution
it
is
easy
to
forget
that
they
are
not
yet
solved
ia
will
also
remain
quite
essential
because
for
the
foreseeable
future
computers
will
not
be
able
to
match
humans
in
their
ability
to
reason
abstractly
about
real-world
situations
we
will
need
well-thought-out
interactions
of
humans
and
computers
to
solve
our
most
pressing
problems
and
we
will
want
computers
to
trigger
new
levels
of
human
creativity
not
replace
human
creativity
whatever
that
might
mean
it
was
john
mccarthy
while
a
professor
at
dartmouth
and
soon
to
take
a
position
at
mit
who
coined
the
term
ai
apparently
to
distinguish
his
budding
research
agenda
from
that
of
norbert
wiener
then
an
older
professor
at
mit
wiener
had
coined
cybernetics
to
refer
to
his
own
vision
of
intelligent
systems
—
a
vision
that
was
closely
tied
to
operations
research
statistics
pattern
recognition
information
theory
and
control
theory
mccarthy
on
the
other
hand
emphasized
the
ties
to
logic
in
an
interesting
reversal
it
is
wiener’s
intellectual
agenda
that
has
come
to
dominate
in
the
current
era
under
the
banner
of
mccarthy’s
terminology
this
state
of
affairs
is
surely
however
only
temporary
the
pendulum
swings
more
in
ai
than
in
most
fields
but
we
need
to
move
beyond
the
particular
historical
perspectives
of
mccarthy
and
wiener
we
need
to
realize
that
the
current
public
dialog
on
ai
—
which
focuses
on
a
narrow
subset
of
industry
and
a
narrow
subset
of
academia
—
risks
blinding
us
to
the
challenges
and
opportunities
that
are
presented
by
the
full
scope
of
ai
ia
and
ii
this
scope
is
less
about
the
realization
of
science-fiction
dreams
or
nightmares
of
super-human
machines
and
more
about
the
need
for
humans
to
understand
and
shape
technology
as
it
becomes
ever
more
present
and
influential
in
their
daily
lives
moreover
in
this
understanding
and
shaping
there
is
a
need
for
a
diverse
set
of
voices
from
all
walks
of
life
not
merely
a
dialog
among
the
technologically
attuned
focusing
narrowly
on
human-imitative
ai
prevents
an
appropriately
wide
range
of
voices
from
being
heard
while
industry
will
continue
to
drive
many
developments
academia
will
also
continue
to
play
an
essential
role
not
only
in
providing
some
of
the
most
innovative
technical
ideas
but
also
in
bringing
researchers
from
the
computational
and
statistical
disciplines
together
with
researchers
from
other
disciplines
whose
contributions
and
perspectives
are
sorely
needed
—
notably
the
social
sciences
the
cognitive
sciences
and
the
humanities
on
the
other
hand
while
the
humanities
and
the
sciences
are
essential
as
we
go
forward
we
should
also
not
pretend
that
we
are
talking
about
something
other
than
an
engineering
effort
of
unprecedented
scale
and
scope
—
society
is
aiming
to
build
new
kinds
of
artifacts
these
artifacts
should
be
built
to
work
as
claimed
we
do
not
want
to
build
systems
that
help
us
with
medical
treatments
transportation
options
and
commercial
opportunities
to
find
out
after
the
fact
that
these
systems
don’t
really
work
—
that
they
make
errors
that
take
their
toll
in
terms
of
human
lives
and
happiness
in
this
regard
as
i
have
emphasized
there
is
an
engineering
discipline
yet
to
emerge
for
the
data-focused
and
learning-focused
fields
as
exciting
as
these
latter
fields
appear
to
be
they
cannot
yet
be
viewed
as
constituting
an
engineering
discipline
moreover
we
should
embrace
the
fact
that
what
we
are
witnessing
is
the
creation
of
a
new
branch
of
engineering
the
term
engineering
is
often
invoked
in
a
narrow
sense
—
in
academia
and
beyond
—
with
overtones
of
cold
affectless
machinery
and
negative
connotations
of
loss
of
control
by
humans
but
an
engineering
discipline
can
be
what
we
want
it
to
be
in
the
current
era
we
have
a
real
opportunity
to
conceive
of
something
historically
new
—
a
human-centric
engineering
discipline
i
will
resist
giving
this
emerging
discipline
a
name
but
if
the
acronym
ai
continues
to
be
used
as
placeholder
nomenclature
going
forward
let’s
be
aware
of
the
very
real
limitations
of
this
placeholder
let’s
broaden
our
scope
tone
down
the
hype
and
recognize
the
serious
challenges
ahead
michael
i
jordan
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
michael
i
jordan
is
a
professor
in
the
department
of
electrical
engineering
and
computer
sciences
and
the
department
of
statistics
at
uc
berkeley
""
i’ve
recently
answered
predicting
missing
data
values
in
a
database
on
stackoverflow
and
thought
it
deserved
a
mention
on
developerzen
one
of
the
important
stages
of
data
mining
is
preprocessing
where
we
prepare
the
data
for
mining
real-world
data
tends
to
be
incomplete
noisy
and
inconsistent
and
an
important
task
when
preprocessing
the
data
is
to
fill
in
missing
values
smooth
out
noise
and
correct
inconsistencies
if
we
specifically
look
at
dealing
with
missing
data
there
are
several
techniques
that
can
be
used
choosing
the
right
technique
is
a
choice
that
depends
on
the
problem
domain
—
the
data’s
domain
sales
data?
crm
data?
""
and
our
goal
for
the
data
mining
process
so
how
can
you
handle
missing
values
in
your
database?
this
is
usually
done
when
the
class
label
is
missing
assuming
your
data
mining
goal
is
classification
or
many
attributes
are
missing
from
the
row
not
just
one
however
you’ll
obviously
get
poor
performance
if
the
percentage
of
such
rows
is
high
for
example
let’s
say
we
have
a
database
of
students
enrolment
data
age
sat
score
state
of
residence
etc
and
a
column
classifying
their
success
in
college
to
low
medium
and
high
let’s
say
our
goal
is
to
build
a
model
predicting
a
student’s
success
in
college
data
rows
who
are
missing
the
success
column
are
not
useful
in
predicting
success
so
they
could
very
well
be
ignored
and
removed
before
running
the
algorithm
decide
on
a
new
global
constant
value
like
unknown
na
or
minus
infinity
that
will
be
used
to
fill
all
the
missing
values
this
technique
is
used
because
sometimes
it
just
doesn’t
make
sense
to
try
and
predict
the
missing
value
for
example
let’s
look
at
the
students
enrollment
database
again
assuming
the
state
of
residence
attribute
data
is
missing
for
some
students
filling
it
up
with
some
state
doesn’t
really
makes
sense
as
opposed
to
using
something
like
na
replace
missing
values
of
an
attribute
with
the
mean
or
median
if
its
discrete
value
for
that
attribute
in
the
database
for
example
in
a
database
of
us
family
incomes
if
the
average
income
of
a
us
family
is
x
you
can
use
that
value
to
replace
missing
income
values
instead
of
using
the
mean
or
median
of
a
certain
attribute
calculated
by
looking
at
all
the
rows
in
a
database
we
can
limit
the
calculations
to
the
relevant
class
to
make
the
value
more
relevant
to
the
row
we’re
looking
at
let’s
say
you
have
a
cars
pricing
database
that
among
other
things
classifies
cars
to
luxury
and
low
budget
and
you’re
dealing
with
missing
values
in
the
cost
field
replacing
missing
cost
of
a
luxury
car
with
the
average
cost
of
all
luxury
cars
is
probably
more
accurate
than
the
value
you’d
get
if
you
factor
in
the
low
budget
cars
the
value
can
be
determined
using
regression
inference
based
tools
using
bayesian
formalism
decision
trees
clustering
algorithms
k-mean\median
etc
for
example
we
could
use
clustering
algorithms
to
create
clusters
of
rows
which
will
then
be
used
for
calculating
an
attribute
mean
or
median
as
specified
in
technique
#3
another
example
could
be
using
a
decision
tree
to
try
and
predict
the
probable
value
in
the
missing
attribute
according
to
other
attributes
in
the
data
i’d
suggest
looking
into
regression
and
decision
trees
first
id3
tree
generation
as
they’re
relatively
easy
and
there
are
plenty
of
examples
on
the
net
additional
notes
originally
published
at
wwwdeveloperzencom
on
august
14
2009
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
maker
of
things
big
data
geek
food
lover
the
essence
of
software
development
""
""
google’s
a
pretty
good
search
engine
right?
well
you
ain’t
seen
nothing
yet
vp
of
research
alfred
spector
talks
to
oliver
lindberg
about
the
technologies
emerging
from
google
labs
—
from
voice
search
to
hybrid
intelligence
and
beyond
this
article
originally
appeared
in
issue
198
of
net
magazine
in
2010
and
was
republished
at
wwwtechradarcom
google
has
always
been
tight-lipped
about
products
that
haven’t
launched
yet
it’s
no
secret
however
that
thanks
to
the
company’s
bottom-up
culture
its
engineers
are
working
on
tons
of
new
projects
at
the
same
time
following
the
mantra
of
‘release
early
release
often’
the
speed
at
which
the
search
engine
giant
is
churning
out
tools
is
staggering
at
the
heart
of
it
all
is
alfred
spector
google’s
vice
president
of
research
and
special
initiatives
one
of
the
areas
google
is
making
significant
advances
in
is
voice
search
spector
is
astounded
by
how
rapidly
it’s
come
along
the
google
mobile
app
features
‘search
by
voice’
capabilities
that
are
available
for
the
iphone
blackberry
windows
mobile
and
android
all
versions
understand
english
including
us
uk
australian
and
indian-english
accents
but
the
latest
addition
for
nokia
s60
phones
even
introduces
mandarin
speech
recognition
which
—
because
of
its
many
different
accents
and
tonal
characteristics
—
posed
a
huge
engineering
challenge
it’s
the
most
spoken
language
in
the
world
but
as
it
isn’t
exactly
keyboard-friendly
voice
search
could
become
immensely
popular
in
china
voice
is
one
of
these
grand
technology
challenges
in
computer
science
spector
explains
can
a
computer
understand
the
human
voice?
it’s
been
worked
on
for
many
decades
and
what
we’ve
realised
over
the
last
couple
of
years
is
that
search
particularly
on
handheld
devices
is
amenable
to
voice
as
an
import
mechanism
it’s
very
valuable
to
be
able
to
use
voice
all
of
us
know
that
no
matter
how
good
the
keyboard
it’s
tricky
to
type
exactly
the
right
thing
into
a
searchbar
while
holding
your
backpack
and
everything
else
to
get
a
computer
to
take
account
of
your
voice
is
no
mean
feat
of
course
one
idea
is
to
take
all
of
the
voices
that
the
system
hears
over
time
into
one
huge
pan-human
voice
model
so
on
the
one
hand
we
have
a
voice
that’s
higher
and
with
an
english
accent
and
on
the
other
hand
my
voice
which
is
deeper
and
with
an
american
accent
they
both
go
into
one
model
or
it
just
becomes
personalised
to
the
individual
voice
scientists
are
a
little
unclear
as
to
which
is
the
best
approach
the
research
department
is
also
making
progress
in
machine
translation
google
translate
already
features
51
languages
including
swahili
and
yiddish
the
latest
version
introduces
instant
real-time
translation
phonetic
input
and
text-to-speech
support
in
english
we’re
able
to
go
from
any
language
to
any
of
the
others
and
there
are
51
times
50
so
2550
possibilities
spector
explains
we’re
focusing
on
increasing
the
number
of
languages
because
we’d
like
to
handle
even
those
languages
where
there’s
not
an
enormous
volume
of
usage
it
will
make
the
web
far
more
valuable
to
more
people
if
they
can
access
the
english-or
chinese
language
web
for
example
but
we
also
continue
to
focus
on
quality
because
almost
always
the
translations
are
valuable
but
imperfect
sometimes
it
comes
from
training
our
translation
system
over
more
raw
data
so
we
have
say
eu
documents
in
english
and
french
and
can
compare
them
and
learn
rules
for
translation
the
other
approach
is
to
bring
more
knowledge
into
translation
for
example
we’re
using
more
syntactic
knowledge
today
and
doing
automated
parsing
with
language
it’s
been
a
grand
challenge
of
the
field
since
the
late
1950s
now
it’s
finally
achieved
mass
usage
the
team
led
by
scientist
franz
josef
och
has
been
collecting
data
for
more
than
100
languages
and
the
google
translator
toolkit
which
makes
use
of
the
‘wisdom
of
the
crowds’
now
even
supports
345
languages
many
of
which
are
minority
languages
the
editor
enables
users
to
translate
text
correct
the
automatic
translation
and
publish
it
spector
thinks
that
this
approach
is
the
future
as
computers
become
even
faster
handling
more
and
more
data
—
a
lot
of
it
in
the
cloud
—
machines
learn
from
users
and
thus
become
smarter
he
calls
this
concept
‘hybrid
intelligence’
it’s
very
difficult
to
solve
these
technological
problems
without
human
input
he
says
it’s
hard
to
create
a
robot
that’s
as
clever
smart
and
knowledgeable
of
the
world
as
we
humans
are
but
it’s
not
as
tough
to
build
a
computational
system
like
google
which
extends
what
we
do
greatly
and
gradually
learns
something
about
the
world
from
us
but
that
requires
our
interpretation
to
make
it
really
successful
we
need
to
get
computers
and
people
communicating
in
both
directions
so
the
computer
learns
from
the
human
and
makes
the
human
more
effective
examples
of
‘hybrid
intelligence’
are
google
suggest
which
instantly
offers
popular
searches
as
you
type
a
search
query
and
the
‘did
you
mean?’
feature
in
google
search
which
corrects
you
when
you
misspell
a
query
in
the
search
bar
the
more
you
use
it
the
better
the
system
gets
training
computers
to
become
seemingly
more
intelligent
poses
major
hurdles
for
google’s
engineers
computers
don’t
train
as
efficiently
as
people
do
spector
explains
let’s
take
the
chess
example
if
a
kasparov
was
the
educator
we
could
count
on
almost
anything
he
says
as
being
accurate
but
if
you
tried
to
learn
from
a
million
chess
players
you
learn
from
my
children
as
well
who
play
chess
but
they’re
10
and
eight
they’ll
be
right
sometimes
and
not
right
other
times
there’s
noise
in
that
and
some
of
the
noise
is
spam
one
also
has
to
have
careful
regard
for
privacy
issues
by
collecting
enormous
amounts
of
data
google
hopes
to
create
a
powerful
database
that
eventually
will
understand
the
relationship
between
words
for
example
‘a
dog
is
an
animal’
and
‘a
dog
has
four
legs’
the
challenge
is
to
try
to
establish
these
relationships
automatically
using
tons
of
information
instead
of
having
experts
teach
the
system
this
database
would
then
improve
search
results
and
language
translations
because
it
would
have
a
better
understanding
of
the
meaning
of
the
words
there’s
also
a
lot
of
research
around
‘conceptual
search’
let’s
take
a
video
of
a
couple
in
front
of
the
empire
state
building
we
watch
the
video
and
it’s
clear
they’re
on
their
honeymoon
but
what
is
the
video
about?
is
it
about
love
or
honeymoons
or
is
it
about
renting
office
space?
it’s
a
fundamentally
challenging
problem
one
example
of
conceptual
search
is
google
image
swirl
which
was
added
to
labs
in
november
enter
a
keyword
and
you
get
a
list
of
12
images
clicking
on
each
one
brings
up
a
cluster
of
related
pictures
click
on
any
of
them
to
expand
the
‘wonder
wheel’
further
google
notes
that
they’re
not
just
the
most
relevant
images
the
algorithm
determines
the
most
relevant
group
of
images
with
similar
appearance
and
meaning
to
improve
the
world’s
data
google
continues
to
focus
on
the
importance
of
the
open
internet
another
labs
project
google
fusion
tables
facilitates
data
management
in
the
cloud
it
enables
users
to
create
tables
filter
and
aggregate
data
merge
it
with
other
data
sources
and
visualise
it
with
google
maps
or
the
google
visualisation
api
the
data
sets
can
then
be
published
shared
or
kept
private
and
commented
on
by
people
around
the
world
it’s
an
example
of
open
collaboration
spector
says
if
it’s
public
we
can
crawl
it
to
make
it
searchable
and
easily
visible
to
people
we
hired
one
of
the
best
database
researchers
in
the
world
alon
halevy
to
lead
it
google
is
aiming
to
make
more
information
available
more
easily
across
multiple
devices
whether
it’s
images
videos
speech
or
maps
no
matter
which
language
we’re
using
spector
calls
the
impact
totally
transparent
processing
—
it
revolutionises
the
role
of
computation
in
day-today
life
the
computer
can
break
down
all
these
barriers
to
communication
and
knowledge
no
matter
what
device
we’re
using
we
have
access
to
things
we
can
do
translations
there
are
books
or
government
documents
and
some
day
we
hope
to
have
medical
records
whatever
you
want
no
matter
where
you
are
you
can
find
it
spector
retired
in
early
2015
and
now
serves
as
the
cto
of
two
sigma
investments
this
article
originally
appeared
in
issue
198
of
net
magazine
in
2010
and
was
republished
at
wwwtechradarcom
photography
by
andy
short
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
independent
editor
and
content
consultant
founder
and
captain
of
@pixelpioneers
co-founder
and
curator
of
wwwgenerateconfcom
former
editor
of
@netmag
interviews
with
leading
tech
entrepreneurs
and
web
designers
conducted
by
@oliverlindberg
at
@netmag
""
这一阵为了工作上的关系花了点时间学习了一下lda算法说实话对于我这个学cs而非学数学的人来说除了集体智慧编程这本书之外基本没怎么看过机器学习的人来说一开始还真是摸不太到门道前前后后快要四个月了算是基本了解了这个算法的实现记录一下也供后来人快速入门做个参考。
一开始直接就下了blei的原始的那篇论文来看但是看了个开头就被dirichlet分布和几个数学公式打倒然后因为专心在写项目中的具体的代码也就先放下了。但是因为发现完全忘记了本科学的概率和统计的内容只好回头去看大学时候概率论的教材发现早不知道借给谁了于是上网买了本花了几天时间大致回顾了一遍概率论的知识什么贝叶斯全概率公式正态分布二项分布之类的。后来晚上没事儿的时候去水木的ai版转了转了解到了machine
learning的圣经prml考虑到反正也是要长期学习了搞了电子版同时上淘宝买了个打印胶装的版本。春节里每天晚上看一点儿扫了一下前两章再次回顾了一下基本数学知识然后了解了下贝叶斯学派那种采用共轭先验来建模的方式。于是再次尝试回头去看blei的那篇论文发现还是看不太懂于是又放下了。然后某天tony让我准备准备给复旦的同学们share一下我们项目中lda的使用为了不露怯又去翻论文正好看到science上这篇topic
models
vs
unstructured
data的科普性质的文章翻了一遍之后再去prml里看了一遍graphic
models那一张觉得对于lda想解决的问题和方法了解了更清楚了。之后从search
engine里搜到这篇文章然后根据推荐读了一部分的gibbs
sampling
for
the
uninitiated。之后忘了怎么又搜到了mark
steyvers和tom
griffiths合著的probabilistic
topic
models在某个周末往返北京的飞机上读完了觉得基本上模型训练过程也明白了。再之后就是读了一下这个最简版的lda
gibbs
sampling的实现再回过头读了一下plda的源码基本上算是对lda有了个相对清楚的了解。
这样前前后后也过去了三个月其实不少时间都是浪费掉的比如blei的论文在没有任何相关知识的情况下一开始读了好几次都没读完而且得到到信息也很有限如果重新总结一下我觉得对于我们这些门外汉程序员来说想了解lda大概需要这些知识:
基本上这样一圈下来基本概念和算法实现都应该搞定了当然数学证明其实没那么容易就搞定但是对于工程师来说先把这些搞定就能干活了这个步骤并不适合各位读博士发论文的同学们但是这样先看看也比较容易对于这些数学问题的兴趣不然成天对这符号和数学公式没有整块业余时间的我是觉得还是容易退缩放弃的。
发现作为工程师来说还是看代码比较有感觉看实际应用的实例比较有感觉看来不能把大部分时间花在prml上还是要多对照着代码看。
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
facebook
messenger
""
chatbot
machine
learning
""
big
data
生命如此短暂掌握技艺却要如此长久
""
by
xavier
amatriain
and
justin
basilico
personalization
science
and
engineering
in
this
two-part
blog
post
we
will
open
the
doors
of
one
of
the
most
valued
netflix
assets:
our
recommendation
system
in
part
1
we
will
relate
the
netflix
prize
to
the
broader
recommendation
challenge
outline
the
external
components
of
our
personalized
service
and
highlight
how
our
task
has
evolved
with
the
business
in
part
2
we
will
describe
some
of
the
data
and
models
that
we
use
and
discuss
our
approach
to
algorithmic
innovation
that
combines
offline
machine
learning
experimentation
with
online
ab
testing
enjoy
and
remember
that
we
are
always
looking
for
more
star
talent
to
add
to
our
great
team
so
please
take
a
look
at
our
jobs
page
in
2006
we
announced
the
netflix
prize
a
machine
learning
and
data
mining
competition
for
movie
rating
prediction
we
offered
$1
million
to
whoever
improved
the
accuracy
of
our
existing
system
called
cinematch
by
10%
we
conducted
this
competition
to
find
new
ways
to
improve
the
recommendations
we
provide
to
our
members
which
is
a
key
part
of
our
business
however
we
had
to
come
up
with
a
proxy
question
that
was
easier
to
evaluate
and
quantify:
the
root
mean
squared
error
rmse
of
the
predicted
rating
the
race
was
on
to
beat
our
rmse
of
09525
with
the
finish
line
of
reducing
it
to
08572
or
less
a
year
into
the
competition
the
korbell
team
won
the
first
progress
prize
with
an
843%
improvement
they
reported
more
than
2000
hours
of
work
in
order
to
come
up
with
the
final
combination
of
107
algorithms
that
gave
them
this
prize
and
they
gave
us
the
source
code
we
looked
at
the
two
underlying
algorithms
with
the
best
performance
in
the
ensemble:
matrix
factorization
which
the
community
generally
called
svd
singular
value
decomposition
and
restricted
boltzmann
machines
rbm
svd
by
itself
provided
a
08914
rmse
while
rbm
alone
provided
a
competitive
but
slightly
worse
08990
rmse
a
linear
blend
of
these
two
reduced
the
error
to
088
to
put
these
algorithms
to
use
we
had
to
work
to
overcome
some
limitations
for
instance
that
they
were
built
to
handle
100
million
ratings
instead
of
the
more
than
5
billion
that
we
have
and
that
they
were
not
built
to
adapt
as
members
added
more
ratings
but
once
we
overcame
those
challenges
we
put
the
two
algorithms
into
production
where
they
are
still
used
as
part
of
our
recommendation
engine
if
you
followed
the
prize
competition
you
might
be
wondering
what
happened
with
the
final
grand
prize
ensemble
that
won
the
$1m
two
years
later
this
is
a
truly
impressive
compilation
and
culmination
of
years
of
work
blending
hundreds
of
predictive
models
to
finally
cross
the
finish
line
we
evaluated
some
of
the
new
methods
offline
but
the
additional
accuracy
gains
that
we
measured
did
not
seem
to
justify
the
engineering
effort
needed
to
bring
them
into
a
production
environment
also
our
focus
on
improving
netflix
personalization
had
shifted
to
the
next
level
by
then
in
the
remainder
of
this
post
we
will
explain
how
and
why
it
has
shifted
one
of
the
reasons
our
focus
in
the
recommendation
algorithms
has
changed
is
because
netflix
as
a
whole
has
changed
dramatically
in
the
last
few
years
netflix
launched
an
instant
streaming
service
in
2007
one
year
after
the
netflix
prize
began
streaming
has
not
only
changed
the
way
our
members
interact
with
the
service
but
also
the
type
of
data
available
to
use
in
our
algorithms
for
dvds
our
goal
is
to
help
people
fill
their
queue
with
titles
to
receive
in
the
mail
over
the
coming
days
and
weeks
selection
is
distant
in
time
from
viewing
people
select
carefully
because
exchanging
a
dvd
for
another
takes
more
than
a
day
and
we
get
no
feedback
during
viewing
for
streaming
members
are
looking
for
something
great
to
watch
right
now
they
can
sample
a
few
videos
before
settling
on
one
they
can
consume
several
in
one
session
and
we
can
observe
viewing
statistics
such
as
whether
a
video
was
watched
fully
or
only
partially
another
big
change
was
the
move
from
a
single
website
into
hundreds
of
devices
the
integration
with
the
roku
player
and
the
xbox
were
announced
in
2008
two
years
into
the
netflix
competition
just
a
year
later
netflix
streaming
made
it
into
the
iphone
now
it
is
available
on
a
multitude
of
devices
that
go
from
a
myriad
of
android
devices
to
the
latest
appletv
two
years
ago
we
went
international
with
the
launch
in
canada
in
2011
we
added
43
latin-american
countries
and
territories
to
the
list
and
just
recently
we
launched
in
uk
and
ireland
today
netflix
has
more
than
23
million
subscribers
in
47
countries
those
subscribers
streamed
2
billion
hours
from
hundreds
of
different
devices
in
the
last
quarter
of
2011
every
day
they
add
2
million
movies
and
tv
shows
to
the
queue
and
generate
4
million
ratings
we
have
adapted
our
personalization
algorithms
to
this
new
scenario
in
such
a
way
that
now
75%
of
what
people
watch
is
from
some
sort
of
recommendation
we
reached
this
point
by
continuously
optimizing
the
member
experience
and
have
measured
significant
gains
in
member
satisfaction
whenever
we
improved
the
personalization
for
our
members
let
us
now
walk
you
through
some
of
the
techniques
and
approaches
that
we
use
to
produce
these
recommendations
we
have
discovered
through
the
years
that
there
is
tremendous
value
to
our
subscribers
in
incorporating
recommendations
to
personalize
as
much
of
netflix
as
possible
personalization
starts
on
our
homepage
which
consists
of
groups
of
videos
arranged
in
horizontal
rows
each
row
has
a
title
that
conveys
the
intended
meaningful
connection
between
the
videos
in
that
group
most
of
our
personalization
is
based
on
the
way
we
select
rows
how
we
determine
what
items
to
include
in
them
and
in
what
order
to
place
those
items
take
as
a
first
example
the
top
10
row:
this
is
our
best
guess
at
the
ten
titles
you
are
most
likely
to
enjoy
of
course
when
we
say
you
we
really
mean
everyone
in
your
household
it
is
important
to
keep
in
mind
that
netflix’
personalization
is
intended
to
handle
a
household
that
is
likely
to
have
different
people
with
different
tastes
that
is
why
when
you
see
your
top10
you
are
likely
to
discover
items
for
dad
mom
the
kids
or
the
whole
family
even
for
a
single
person
household
we
want
to
appeal
to
your
range
of
interests
and
moods
to
achieve
this
in
many
parts
of
our
system
we
are
not
only
optimizing
for
accuracy
but
also
for
diversity
another
important
element
in
netflix’
personalization
is
awareness
we
want
members
to
be
aware
of
how
we
are
adapting
to
their
tastes
this
not
only
promotes
trust
in
the
system
but
encourages
members
to
give
feedback
that
will
result
in
better
recommendations
a
different
way
of
promoting
trust
with
the
personalization
component
is
to
provide
explanations
as
to
why
we
decide
to
recommend
a
given
movie
or
show
we
are
not
recommending
it
because
it
suits
our
business
needs
but
because
it
matches
the
information
we
have
from
you:
your
explicit
taste
preferences
and
ratings
your
viewing
history
or
even
your
friends’
recommendations
on
the
topic
of
friends
we
recently
released
our
facebook
connect
feature
in
46
out
of
the
47
countries
we
operate
—
all
but
the
us
because
of
concerns
with
the
vppa
law
knowing
about
your
friends
not
only
gives
us
another
signal
to
use
in
our
personalization
algorithms
but
it
also
allows
for
different
rows
that
rely
mostly
on
your
social
circle
to
generate
recommendations
some
of
the
most
recognizable
personalization
in
our
service
is
the
collection
of
genre
rows
these
range
from
familiar
high-level
categories
like
comedies
and
dramas
to
highly
tailored
slices
such
as
imaginative
time
travel
movies
from
the
1980s
each
row
represents
3
layers
of
personalization:
the
choice
of
genre
itself
the
subset
of
titles
selected
within
that
genre
and
the
ranking
of
those
titles
members
connect
with
these
rows
so
well
that
we
measure
an
increase
in
member
retention
by
placing
the
most
tailored
rows
higher
on
the
page
instead
of
lower
as
with
other
personalization
elements
freshness
and
diversity
is
taken
into
account
when
deciding
what
genres
to
show
from
the
thousands
possible
we
present
an
explanation
for
the
choice
of
rows
using
a
member’s
implicit
genre
preferences
—
recent
plays
ratings
and
other
interactions
—
""
or
explicit
feedback
provided
through
our
taste
preferences
survey
we
will
also
invite
members
to
focus
a
row
with
additional
explicit
preference
feedback
when
this
is
lacking
similarity
is
also
an
important
source
of
personalization
in
our
service
we
think
of
similarity
in
a
very
broad
sense
it
can
be
between
movies
or
between
members
and
can
be
in
multiple
dimensions
such
as
metadata
ratings
or
viewing
data
furthermore
these
similarities
can
be
blended
and
used
as
features
in
other
models
similarity
is
used
in
multiple
contexts
for
example
in
response
to
a
member’s
action
such
as
searching
or
adding
a
title
to
the
queue
it
is
also
used
to
generate
rows
of
adhoc
genres
based
on
similarity
to
titles
that
a
member
has
interacted
with
recently
if
you
are
interested
in
a
more
in-depth
description
of
the
architecture
of
the
similarity
system
you
can
read
about
it
in
this
past
post
on
the
blog
in
most
of
the
previous
contexts
—
be
it
in
the
top10
row
the
genres
or
the
similars
—
ranking
the
choice
of
what
order
to
place
the
items
in
a
row
is
critical
in
providing
an
effective
personalized
experience
the
goal
of
our
ranking
system
is
to
find
the
best
possible
ordering
of
a
set
of
items
for
a
member
within
a
specific
context
in
real-time
we
decompose
ranking
into
scoring
sorting
and
filtering
sets
of
movies
for
presentation
to
a
member
our
business
objective
is
to
maximize
member
satisfaction
and
month-to-month
subscription
retention
which
correlates
well
with
maximizing
consumption
of
video
content
we
therefore
optimize
our
algorithms
to
give
the
highest
scores
to
titles
that
a
member
is
most
likely
to
play
and
enjoy
now
it
is
clear
that
the
netflix
prize
objective
accurate
prediction
of
a
movie’s
rating
is
just
one
of
the
many
components
of
an
effective
recommendation
system
that
optimizes
our
members
enjoyment
we
also
need
to
take
into
account
factors
such
as
context
title
popularity
interest
evidence
novelty
diversity
and
freshness
supporting
all
the
different
contexts
in
which
we
want
to
make
recommendations
requires
a
range
of
algorithms
that
are
tuned
to
the
needs
of
those
contexts
in
the
next
part
of
this
post
we
will
talk
in
more
detail
about
the
ranking
problem
we
will
also
dive
into
the
data
and
models
that
make
all
the
above
possible
and
discuss
our
approach
to
innovating
in
this
space
on
to
part
2:
originally
published
at
techblognetflixcom
on
april
6
2012
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
learn
more
about
how
netflix
designs
builds
and
operates
our
systems
and
engineering
organizations
learn
about
netflix’s
world
class
engineering
efforts
company
culture
product
developments
and
more
""
by
xavier
amatriain
and
justin
basilico
personalization
science
and
engineering
in
part
one
of
this
blog
post
we
detailed
the
different
components
of
netflix
personalization
we
also
explained
how
netflix
personalization
and
the
service
as
a
whole
have
changed
from
the
time
we
announced
the
netflix
prize
the
$1m
prize
delivered
a
great
return
on
investment
for
us
not
only
in
algorithmic
innovation
but
also
in
brand
awareness
and
attracting
stars
no
pun
intended
to
join
our
team
predicting
movie
ratings
accurately
is
just
one
aspect
of
our
world-class
recommender
system
in
this
second
part
of
the
blog
post
we
will
give
more
insight
into
our
broader
personalization
technology
we
will
discuss
some
of
our
current
models
data
and
the
approaches
we
follow
to
lead
innovation
and
research
in
this
space
the
goal
of
recommender
systems
is
to
present
a
number
of
attractive
items
for
a
person
to
choose
from
this
is
usually
accomplished
by
selecting
some
items
and
sorting
them
in
the
order
of
expected
enjoyment
or
utility
since
the
most
common
way
of
presenting
recommended
items
is
in
some
form
of
list
such
as
the
various
rows
on
netflix
we
need
an
appropriate
ranking
model
that
can
use
a
wide
variety
of
information
to
come
up
with
an
optimal
ranking
of
the
items
for
each
of
our
members
if
you
are
looking
for
a
ranking
function
that
optimizes
consumption
an
obvious
baseline
is
item
popularity
the
reason
is
clear:
on
average
a
member
is
most
likely
to
watch
what
most
others
are
watching
however
popularity
is
the
opposite
of
personalization:
it
will
produce
the
same
ordering
of
items
for
every
member
thus
the
goal
becomes
to
find
a
personalized
ranking
function
that
is
better
than
item
popularity
so
we
can
better
satisfy
members
with
varying
tastes
recall
that
our
goal
is
to
recommend
the
titles
that
each
member
is
most
likely
to
play
and
enjoy
one
obvious
way
to
approach
this
is
to
use
the
member’s
predicted
rating
of
each
item
as
an
adjunct
to
item
popularity
using
predicted
ratings
on
their
own
as
a
ranking
function
can
lead
to
items
that
are
too
niche
or
unfamiliar
being
recommended
and
can
exclude
items
that
the
member
would
want
to
watch
even
though
they
may
not
rate
them
highly
to
compensate
for
this
rather
than
using
either
popularity
or
predicted
rating
on
their
own
we
would
like
to
produce
rankings
that
balance
both
of
these
aspects
at
this
point
we
are
ready
to
build
a
ranking
prediction
model
using
these
two
features
there
are
many
ways
one
could
construct
a
ranking
function
ranging
from
simple
scoring
methods
to
pairwise
preferences
to
optimization
over
the
entire
ranking
for
the
purposes
of
illustration
let
us
start
with
a
very
simple
scoring
approach
by
choosing
our
ranking
function
to
be
a
linear
combination
of
popularity
and
predicted
rating
this
gives
an
equation
of
the
form
frankuv
=
w1
pv
""
w2
ruv
""
b
where
u=user
v=video
item
p=popularity
and
r=predicted
rating
this
equation
defines
a
two-dimensional
space
like
the
one
depicted
below
once
we
have
such
a
function
we
can
pass
a
set
of
videos
through
our
function
and
sort
them
in
descending
order
according
to
the
score
you
might
be
wondering
how
we
can
set
the
weights
w1
and
w2
in
our
model
the
bias
b
is
constant
and
thus
ends
up
not
affecting
the
final
ordering
in
other
words
in
our
simple
two-dimensional
model
how
do
we
determine
whether
popularity
is
more
or
less
important
than
predicted
rating?
there
are
at
least
two
possible
approaches
to
this
you
could
sample
the
space
of
possible
weights
and
let
the
members
decide
what
makes
sense
after
many
ab
tests
this
procedure
might
be
time
consuming
and
not
very
cost
effective
another
possible
answer
involves
formulating
this
as
a
machine
learning
problem:
select
positive
and
negative
examples
from
your
historical
data
and
let
a
machine
learning
algorithm
learn
the
weights
that
optimize
your
goal
this
family
of
machine
learning
problems
is
known
as
learning
to
rank
and
is
central
to
application
scenarios
such
as
search
engines
or
ad
targeting
note
though
that
a
crucial
difference
in
the
case
of
ranked
recommendations
is
the
importance
of
personalization:
we
do
not
expect
a
global
notion
of
relevance
but
rather
look
for
ways
of
optimizing
a
personalized
model
as
you
might
guess
apart
from
popularity
and
rating
prediction
we
have
tried
many
other
features
at
netflix
some
have
shown
no
positive
effect
while
others
have
improved
our
ranking
accuracy
tremendously
the
graph
below
shows
the
ranking
improvement
we
have
obtained
by
adding
different
features
and
optimizing
the
machine
learning
algorithm
many
supervised
classification
methods
can
be
used
for
ranking
typical
choices
include
logistic
regression
support
vector
machines
neural
networks
or
decision
tree-based
methods
such
as
gradient
boosted
decision
trees
gbdt
on
the
other
hand
a
great
number
of
algorithms
specifically
designed
for
learning
to
rank
have
appeared
in
recent
years
such
as
ranksvm
or
rankboost
there
is
no
easy
answer
to
choose
which
model
will
perform
best
in
a
given
ranking
problem
the
simpler
your
feature
space
is
the
simpler
your
model
can
be
but
it
is
easy
to
get
trapped
in
a
situation
where
a
new
feature
does
not
show
value
because
the
model
cannot
learn
it
or
the
other
way
around
to
conclude
that
a
more
powerful
model
is
not
useful
simply
because
you
don’t
have
the
feature
space
that
exploits
its
benefits
the
previous
discussion
on
the
ranking
algorithms
highlights
the
importance
of
both
data
and
models
in
creating
an
optimal
personalized
experience
for
our
members
at
netflix
we
are
fortunate
to
have
many
relevant
data
sources
and
smart
people
who
can
select
optimal
algorithms
to
turn
data
into
product
features
here
are
some
of
the
data
sources
we
can
use
to
optimize
our
recommendations:
so
what
about
the
models?
one
thing
we
have
found
at
netflix
is
that
with
the
great
availability
of
data
both
in
quantity
and
types
a
thoughtful
approach
is
required
to
model
selection
training
and
testing
we
use
all
sorts
of
machine
learning
approaches:
from
unsupervised
methods
such
as
clustering
algorithms
to
a
number
of
supervised
classifiers
that
have
shown
optimal
results
in
various
contexts
this
is
an
incomplete
list
of
methods
you
should
probably
know
about
if
you
are
working
in
machine
learning
for
personalization:
consumer
data
science
the
abundance
of
source
data
measurements
and
associated
experiments
allow
us
to
operate
a
data-driven
organization
netflix
has
embedded
this
approach
into
its
culture
since
the
company
was
founded
and
we
have
come
to
call
it
consumer
data
science
broadly
speaking
the
main
goal
of
our
consumer
science
approach
is
to
innovate
for
members
effectively
the
only
real
failure
is
the
failure
to
innovate
or
as
thomas
watson
sr
founder
of
ibm
put
it:
if
you
want
to
increase
your
success
rate
double
your
failure
rate
we
strive
for
an
innovation
culture
that
allows
us
to
evaluate
ideas
rapidly
inexpensively
and
objectively
and
once
we
test
something
we
want
to
understand
why
it
failed
or
succeeded
this
lets
us
focus
on
the
central
goal
of
improving
our
service
for
our
members
so
how
does
this
work
in
practice?
it
is
a
slight
variation
over
the
traditional
scientific
process
called
ab
testing
or
bucket
testing:
when
we
execute
ab
tests
we
track
many
different
metrics
but
we
ultimately
trust
member
engagement
eg
hours
of
play
and
retention
tests
usually
have
thousands
of
members
and
anywhere
from
2
to
20
cells
exploring
variations
of
a
base
idea
we
typically
have
scores
of
ab
tests
running
in
parallel
ab
tests
let
us
try
radical
ideas
or
test
many
approaches
at
the
same
time
but
the
key
advantage
is
that
they
allow
our
decisions
to
be
data-driven
you
can
read
more
about
our
approach
to
ab
testing
in
this
previous
tech
blog
post
or
in
some
of
the
quora
answers
by
our
chief
product
officer
neil
hunt
an
interesting
follow-up
question
that
we
have
faced
is
how
to
integrate
our
machine
learning
approaches
into
this
data-driven
ab
test
culture
at
netflix
we
have
done
this
with
an
offline-online
testing
process
that
tries
to
combine
the
best
of
both
worlds
the
offline
testing
cycle
is
a
step
where
we
test
and
optimize
our
algorithms
prior
to
performing
online
ab
testing
to
measure
model
performance
offline
we
track
multiple
metrics
used
in
the
machine
learning
community:
from
ranking
measures
such
as
normalized
discounted
cumulative
gain
mean
reciprocal
rank
or
fraction
of
concordant
pairs
to
classification
metrics
such
as
accuracy
precision
recall
or
f-score
we
also
use
the
famous
rmse
from
the
netflix
prize
or
other
more
exotic
metrics
to
track
different
aspects
like
diversity
we
keep
track
of
how
well
those
metrics
correlate
to
measurable
online
gains
in
our
ab
tests
however
since
the
mapping
is
not
perfect
offline
performance
is
used
only
as
an
indication
to
make
informed
decisions
on
follow
up
tests
once
offline
testing
has
validated
a
hypothesis
we
are
ready
to
design
and
launch
the
ab
test
that
will
prove
the
new
feature
valid
from
a
member
perspective
if
it
does
we
will
be
ready
to
roll
out
in
our
continuous
pursuit
of
the
better
product
for
our
members
the
diagram
below
illustrates
the
details
of
this
process
an
extreme
example
of
this
innovation
cycle
is
what
we
called
the
top10
marathon
this
was
a
focused
10-week
effort
to
quickly
test
dozens
of
algorithmic
ideas
related
to
improving
our
top10
row
think
of
it
as
a
2-month
hackathon
with
metrics
different
teams
and
individuals
were
invited
to
contribute
ideas
and
code
in
this
effort
we
rolled
out
6
different
ideas
as
ab
tests
each
week
and
kept
track
of
the
offline
and
online
metrics
the
winning
results
are
already
part
of
our
production
system
the
netflix
prize
abstracted
the
recommendation
problem
to
a
proxy
question
of
predicting
ratings
but
member
ratings
are
only
one
of
the
many
data
sources
we
have
and
rating
predictions
are
only
part
of
our
solution
over
time
we
have
reformulated
the
recommendation
problem
to
the
question
of
optimizing
the
probability
a
member
chooses
to
watch
a
title
and
enjoys
it
enough
to
come
back
to
the
service
more
data
availability
enables
better
results
but
in
order
to
get
those
results
we
need
to
have
optimized
approaches
appropriate
metrics
and
rapid
experimentation
to
excel
at
innovating
personalization
it
is
insufficient
to
be
methodical
in
our
research
the
space
to
explore
is
virtually
infinite
at
netflix
we
love
choosing
and
watching
movies
and
tv
shows
we
focus
our
research
by
translating
this
passion
into
strong
intuitions
about
fruitful
directions
to
pursue
under-utilized
data
sources
better
feature
representations
more
appropriate
models
and
metrics
and
missed
opportunities
to
personalize
we
use
data
mining
and
other
experimental
approaches
to
incrementally
inform
our
intuition
and
so
prioritize
investment
of
effort
as
with
any
scientific
pursuit
there’s
always
a
contribution
from
lady
luck
but
as
the
adage
goes
luck
favors
the
prepared
mind
finally
above
all
we
look
to
our
members
as
the
final
judges
of
the
quality
of
our
recommendation
approach
because
this
is
all
ultimately
about
increasing
our
members’
enjoyment
in
their
own
netflix
experience
we
are
always
looking
for
more
people
to
join
our
team
of
prepared
minds
make
sure
you
take
a
look
at
our
jobs
page
originally
published
at
techblognetflixcom
on
june
20
2012
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
learn
more
about
how
netflix
designs
builds
and
operates
our
systems
and
engineering
organizations
learn
about
netflix’s
world
class
engineering
efforts
company
culture
product
developments
and
more
""
update1:
an
improved
symspell
implementation
is
now
1000000x
fasterupdate2:
symspellcompound
with
compound
aware
spelling
correction
update3:
benchmark
of
symspell
bk-tree
und
norvig’s
spell-correct
recently
i
answered
a
question
on
quora
about
spelling
correction
for
search
engines
when
i
described
our
symspell
algorithm
i
was
pointed
to
peter
norvig’s
page
where
he
outlined
his
approach
both
algorithms
are
based
on
edit
distance
damerau-levenshtein
distance
both
try
to
find
the
dictionary
entries
with
smallest
edit
distance
from
the
query
term
if
the
edit
distance
is
0
the
term
is
spelled
correctly
if
the
edit
distance
is
<=2
the
dictionary
term
is
used
as
spelling
suggestion
but
symspell
uses
a
different
way
to
search
the
dictionary
resulting
in
a
significant
performance
gain
and
language
independence
three
ways
to
search
for
minimum
edit
distance
in
a
dictionary:
1
naive
approachthe
obvious
way
of
doing
this
is
to
compute
the
edit
distance
from
the
query
term
to
each
dictionary
term
before
selecting
the
strings
of
minimum
edit
distance
as
spelling
suggestion
this
exhaustive
search
is
inordinately
expensive
source:
christopher
d
manning
prabhakar
raghavan
""
hinrich
schütze:
introduction
to
information
retrieval
the
performance
can
be
significantly
improved
by
terminating
the
edit
distance
calculation
as
soon
as
a
threshold
of
2
or
3
has
been
reached
2
peter
norviggenerate
all
possible
terms
with
an
edit
distance
deletes
""
transposes
""
replaces
""
inserts
from
the
query
term
and
search
them
in
the
dictionary
for
a
word
of
length
n
an
alphabet
size
a
an
edit
distance
d=1
there
will
be
n
deletions
n-1
transpositions
a*n
alterations
and
a*n1
insertions
for
a
total
of
2n2ana-1
terms
at
search
time
source:
peter
norvig:
how
to
write
a
spelling
corrector
this
is
much
better
than
the
naive
approach
but
still
expensive
at
search
time
114324
terms
for
n=9
a=36
d=2
and
language
dependent
because
the
alphabet
is
used
to
generate
the
terms
which
is
different
in
many
languages
and
huge
in
chinese:
a=70000
unicode
han
characters
3
symmetric
delete
spelling
correction
symspell
generate
terms
with
an
edit
distance
deletes
only
from
each
dictionary
term
and
add
them
together
with
the
original
term
to
the
dictionary
this
has
to
be
done
only
once
during
a
pre-calculation
step
generate
terms
with
an
edit
distance
deletes
only
from
the
input
term
and
search
them
in
the
dictionary
for
a
word
of
length
n
an
alphabet
size
of
a
an
edit
distance
of
1
there
will
be
just
n
deletions
for
a
total
of
n
terms
at
search
time
this
is
three
orders
of
magnitude
less
expensive
36
terms
for
n=9
and
d=2
and
language
independent
the
alphabet
is
not
required
to
generate
deletes
the
cost
of
this
approach
is
the
pre-calculation
time
and
storage
space
of
x
deletes
for
every
original
dictionary
entry
which
is
acceptable
in
most
cases
the
number
x
of
deletes
for
a
single
dictionary
entry
depends
on
the
maximum
edit
distance:
x=n
for
edit
distance=1
x=n*n-12
for
edit
distance=2
x=n!d!n-d!
for
edit
distance=d
combinatorics:
k
out
of
n
combinations
without
repetitions
and
k=n-d
eg
for
a
maximum
edit
distance
of
2
and
an
average
word
length
of
5
and
100000
dictionary
entries
we
need
to
additionally
store
1500000
deletes
remark
1:
during
the
precalculation
different
words
in
the
dictionary
might
lead
to
same
delete
term:
deletesun1==deletesin1==sn
while
we
generate
only
one
new
dictionary
entry
sn
inside
we
need
to
store
both
original
terms
as
spelling
correction
suggestion
sunsin
remark
2:
there
are
four
different
comparison
pair
types:
the
last
comparison
type
is
required
for
replaces
and
transposes
only
but
we
need
to
check
whether
the
suggested
dictionary
term
is
really
a
replace
or
an
adjacent
transpose
of
the
input
term
to
prevent
false
positives
of
higher
edit
distance
bank==bnak
and
bank==bink
but
bank!=kanb
and
bank!=xban
and
bank!=baxn
remark
3:
instead
of
a
dedicated
spelling
dictionary
we
are
using
the
search
engine
index
itself
this
has
several
benefits:
remark
4:
we
have
implemented
query
suggestionscompletion
in
a
similar
fashion
this
is
a
good
way
to
prevent
spelling
errors
in
the
first
place
every
newly
indexed
word
whose
frequency
is
over
a
certain
threshold
is
stored
as
a
suggestion
to
all
of
its
prefixes
they
are
created
in
the
index
if
they
do
not
yet
exist
as
we
anyway
provide
an
instant
search
feature
the
lookup
for
suggestions
comes
also
at
almost
no
extra
cost
multiple
terms
are
sorted
by
the
number
of
results
stored
in
the
index
reasoningthe
symspell
algorithm
exploits
the
fact
that
the
edit
distance
between
two
terms
is
symmetrical:
we
are
using
variant
3
because
the
delete-only-transformation
is
language
independent
and
three
orders
of
magnitude
less
expensive
where
does
the
speed
come
from?
computational
complexity
the
symspell
algorithm
is
constant
time
""
o1
time
""
ie
independent
of
the
dictionary
size
but
depending
on
the
average
term
length
and
maximum
edit
distance
because
our
index
is
based
on
a
hash
table
which
has
an
average
search
time
complexity
of
o1
comparison
to
other
approaches
bk-trees
have
a
search
time
of
olog
dictionary_size
whereas
the
symspell
algorithm
is
constant
time
""
o1
time
""
ie
independent
of
the
dictionary
size
tries
have
a
comparable
search
performance
to
our
approach
but
a
trie
is
a
prefix
tree
which
requires
a
common
prefix
this
makes
it
suitable
for
autocomplete
or
search
suggestions
but
not
applicable
for
spell
checking
if
your
typing
error
is
eg
in
the
first
letter
than
you
have
no
common
prefix
hence
the
trie
will
not
work
for
spelling
correction
application
possible
application
fields
of
the
symspell
algorithm
are
those
of
fast
approximate
dictionary
string
matching:
spell
checkers
for
word
processors
and
search
engines
correction
systems
for
optical
character
recognition
natural
language
translation
based
on
translation
memory
record
linkage
de-duplication
matching
dna
sequences
fuzzy
string
searching
and
fraud
detection
source
codethe
c#
implementation
of
the
symmetric
delete
spelling
correction
algorithm
is
released
on
github
as
open
source
under
the
mit
license:https:githubcomwolfgarbesymspell
portsthere
are
ports
in
c
crystal
go
java
javascript
python
ruby
rust
scala
swift
available
originally
published
at
blogfaroocom
on
june
7
2012
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
founder
seekstorm
search-as-a-service
faroo
p2p
search
http:wwwseekstormcom
https:githubcomwolfgarbe
https:wwwquoracomprofilewolf-garbe
""
this
post
outlines
a
formalization
of
what
nick
bostrom
calls
indirect
normativity
i
don’t
think
it’s
an
adequate
solution
to
the
ai
control
problem
but
to
my
knowledge
it
was
the
first
precise
specification
of
a
goal
that
meets
the
not
terrible
bar
ie
which
does
not
lead
to
terrible
consequences
if
pursued
without
any
caveats
or
restrictions
the
proposal
outlined
here
was
sketched
in
early
2012
while
i
was
visiting
fhi
and
was
my
first
serious
foray
into
ai
control
when
faced
with
the
challenge
of
writing
down
precise
moral
principles
adhering
to
the
standards
demanded
in
mathematics
moral
philosophers
encounter
two
serious
difficulties:
in
light
of
these
difficulties
a
moral
philosopher
might
simply
declare:
it
is
not
my
place
to
aspire
to
mathematical
standards
of
precision
ethics
as
a
project
inherently
requires
shared
language
understanding
and
experience
it
becomes
impossible
or
meaningless
without
them
this
may
be
a
defensible
philosophical
position
but
unfortunately
the
issue
is
not
entirely
philosophical
in
the
interest
of
building
institutions
or
machines
which
reliably
pursue
what
we
value
we
may
one
day
be
forced
to
describe
precisely
what
we
value
in
a
way
that
does
not
depend
on
charitable
or
common
sense
interpretation
in
the
same
way
that
we
today
must
describe
what
we
want
done
precisely
to
computers
often
with
considerable
effort
if
some
aspects
of
our
values
cannot
be
described
formally
then
it
may
be
more
difficult
to
use
institutions
or
machines
to
reliably
satisfy
them
this
is
not
to
say
that
describing
our
values
formally
is
necessary
to
satisfying
them
merely
that
it
might
make
it
easier
since
we
are
focusing
on
finding
any
precise
and
satisfactory
moral
theory
rather
than
resolving
disputes
in
moral
philosophy
we
will
adopt
a
consequentialist
approach
without
justification
and
focus
on
axiology
moreover
we
will
begin
from
the
standpoint
of
expected
utility
maximization
and
leave
aside
questions
about
how
or
over
what
space
the
maximization
is
performed
we
aim
to
mathematically
define
a
utility
function
u
such
that
we
would
be
willing
to
build
a
hypothetical
machine
which
exceptionlessly
maximized
u
possibly
at
the
catastrophic
expense
of
any
other
values
we
will
assume
that
the
machine
has
an
ability
to
reason
which
at
least
rivals
that
of
humans
and
is
willing
to
tolerate
arbitrarily
complex
definitions
of
u
within
its
ability
to
reason
about
them
we
adopt
an
indirect
approach
rather
than
specifying
what
exactly
we
want
we
specify
a
process
for
determining
what
we
want
this
process
is
extremely
complex
so
that
any
computationally
limited
agent
will
always
be
uncertain
about
the
process’
output
however
by
reasoning
about
the
process
it
is
possible
to
make
judgments
about
which
action
has
the
highest
expected
utility
in
light
of
this
uncertainty
for
example
i
might
adopt
the
principle:
a
state
of
affairs
is
valuable
to
the
extent
that
i
would
judge
it
valuable
after
a
century
of
reflection
in
general
i
will
be
uncertain
about
what
i
would
say
after
a
century
but
i
can
act
on
the
basis
of
my
best
guesses:
after
a
century
i
will
probably
prefer
worlds
with
more
happiness
and
so
today
i
should
prefer
worlds
with
more
happiness
after
a
century
i
have
only
a
small
probability
of
valuing
trees’
feelings
and
so
today
i
should
go
out
of
my
way
to
avoid
hurting
them
if
it
is
either
instrumentally
useful
or
extremely
easy
as
i
spend
more
time
thinking
my
beliefs
about
what
i
would
say
after
a
century
may
change
and
i
will
start
to
pursue
different
states
of
affairs
even
though
the
formal
definition
of
my
values
is
static
similarly
i
might
desire
to
think
about
the
value
of
trees’
feelings
if
i
expect
that
my
opinions
are
unstable:
if
i
spend
a
month
thinking
about
trees
my
current
views
will
then
be
a
much
better
predictor
of
my
views
after
a
hundred
years
and
if
i
know
better
whether
or
not
trees’
feelings
are
valuable
i
can
make
better
decisions
this
example
is
quite
informal
but
it
communicates
the
main
idea
of
the
approach
we
stress
that
the
value
of
our
contribution
if
any
is
in
the
possibility
of
a
precise
formulation
our
proposal
itself
will
be
relatively
informal
instead
it
is
a
description
of
how
you
would
arrive
at
a
precise
formulation
the
use
of
indirection
seems
to
be
necessary
to
achieve
the
desired
level
of
precision
our
proposal
contains
only
two
explicit
steps:
each
of
these
steps
requires
substantial
elaboration
but
we
must
also
specify
what
we
expect
the
human
to
do
with
these
tools
this
proposal
is
best
understood
in
the
context
of
other
fantastic-seeming
proposals
such
as
my
utility
is
whatever
i
would
write
down
if
i
reflected
for
a
thousand
years
without
interruption
or
biological
decay
the
counterfactual
events
which
take
place
within
the
definition
are
far
beyond
the
realm
our
intuition
recognizes
as
realistic
and
have
no
place
except
in
thought
experiments
but
to
the
extent
that
we
can
reason
about
these
counterfactuals
and
change
our
behavior
on
the
basis
of
that
reasoning
if
so
motivated
we
can
already
see
how
such
fantastic
situations
could
affect
our
more
prosaic
reality
the
remainder
of
this
document
consists
of
brief
elaboration
of
some
of
these
steps
and
a
few
arguments
about
why
this
is
a
desirable
process
the
first
step
of
our
proposal
is
a
high-fidelity
mathematical
model
of
human
cognition
we
will
set
aside
philosophical
troubles
and
assume
that
the
human
brain
is
a
purely
physical
system
which
may
be
characterized
mathematically
even
granting
this
it
is
not
clear
how
we
can
realistically
obtain
such
a
characterization
the
most
obvious
approach
to
characterizing
a
brain
is
to
combine
measurements
of
its
behavior
or
architecture
with
an
understanding
of
biology
chemistry
and
physics
this
project
represents
a
massive
engineering
effort
which
is
currently
just
beginning
most
pessimistically
our
proposal
could
be
postponed
until
this
project’s
completion
this
could
still
be
long
before
the
mathematical
characterization
of
the
brain
becomes
useful
for
running
experiments
or
automating
human
activities:
because
we
are
interested
only
in
a
definition
we
do
not
care
about
having
the
computational
resources
necessary
to
simulate
the
brain
an
impractical
mathematical
definition
however
may
be
much
easier
to
obtain
we
can
define
a
model
of
a
brain
in
terms
of
exhaustive
searches
which
could
never
be
practically
carried
out
for
example
given
some
observations
of
a
neuron
we
can
formally
define
a
brute
force
search
for
a
model
of
that
neuron
similarly
given
models
of
individual
neurons
we
may
be
able
to
specify
a
brute
force
search
over
all
ways
of
connecting
those
neurons
which
account
for
our
observations
of
the
brain
say
some
data
acquired
through
functional
neuroimaging
it
may
be
possible
to
carry
out
this
definition
without
exploiting
any
structural
knowledge
about
the
brain
beyond
what
is
necessary
to
measure
it
effectively
by
collecting
imaging
data
for
a
human
exposed
to
a
wide
variety
of
stimuli
we
can
recover
a
large
corpus
of
data
which
must
be
explained
by
any
model
of
a
human
brain
moreover
by
using
our
explicit
knowledge
of
human
cognition
we
can
algorithmically
generate
an
extensive
range
of
tests
which
identify
a
successful
simulation
by
probing
responses
to
questions
or
performance
on
games
or
puzzles
in
fact
this
project
may
be
possible
using
existing
resources
the
complexity
of
the
human
brain
is
not
as
unapproachable
as
it
may
at
first
appear:
though
it
may
contain
1014synapses
each
described
by
many
parameters
it
can
be
specified
much
more
compactly
a
newborn’s
brain
can
be
specified
by
about
109bits
of
genetic
information
together
with
a
recipe
for
a
physical
simulation
of
development
the
human
brain
appears
to
form
new
long-term
memories
at
a
rate
of
1–2
bits
per
second
suggesting
that
it
may
be
possible
to
specify
an
adult
brain
using
109additional
bits
of
experiential
information
this
suggests
that
it
may
require
only
about
1010bits
of
information
to
specify
a
human
brain
which
is
at
the
limits
of
what
can
be
reasonably
collected
by
existing
technology
for
functional
neuroimaging
this
discussion
has
glossed
over
at
least
one
question:
what
do
we
mean
by
‘brain
emulation’?
human
cognition
does
not
reside
in
a
physical
system
with
sharp
boundaries
and
it
is
not
clear
how
you
would
define
or
use
a
simulation
of
the
input-output
behavior
of
such
an
object
we
will
focus
on
some
system
which
does
have
precisely
defined
input-output
behavior
and
which
captures
the
important
aspects
of
human
cognition
consider
a
system
containing
a
human
a
keyboard
a
monitor
and
some
auxiliary
instruments
well-insulated
from
the
environment
except
for
some
wires
carrying
inputs
to
the
monitor
and
outputs
from
the
keyboard
and
auxiliary
instruments
and
wires
carrying
power
the
inputs
to
this
system
are
simply
screens
to
be
displayed
on
the
monitor
say
delivered
as
a
sequence
to
be
displayed
one
after
another
at
30
frames
per
second
while
the
outputs
are
the
information
conveyed
from
the
keyboard
and
the
other
measuring
apparatuses
also
delivered
as
a
sequence
of
data
dumps
each
recording
activity
from
the
last
30th
of
a
second
this
human
in
a
box
system
can
be
easily
formally
defined
if
a
precise
description
of
a
human
brain
and
coarse
descriptions
of
the
human
body
and
the
environment
are
available
alternatively
the
input-output
behavior
of
the
human
in
a
box
can
be
directly
observed
and
a
computational
model
constructed
for
the
entire
system
let
h
be
a
mathematical
definition
of
the
resulting
randomized
function
from
input
sequences
in1
in2
""
ink
to
the
next
output
outk
h
is
by
design
a
good
approximation
to
what
the
human
would
output
if
presented
with
any
particular
input
sequence
using
h
we
can
mathematically
define
what
would
happen
if
the
human
interacted
with
a
wide
variety
of
systems
for
example
if
we
deliver
outk
as
the
input
to
an
abstract
computer
running
some
arbitrary
software
and
then
define
ink1
as
what
the
screen
would
next
display
we
can
mathematically
define
the
distribution
over
transcripts
which
would
have
arisen
if
the
human
had
interacted
with
the
abstract
computer
this
computer
could
be
running
an
interactive
shell
a
video
game
or
a
messaging
client
note
that
h
reflects
the
behavior
of
a
particular
human
in
a
particular
mental
state
this
state
is
determined
by
the
process
used
to
design
h
or
the
data
used
to
learn
it
in
general
we
can
control
h
by
choosing
an
appropriate
human
and
providing
appropriate
instructions
""
training
more
emulations
could
be
produced
by
similar
measures
if
necessary
using
only
a
single
human
may
seem
problematic
but
we
will
not
rely
on
this
lone
individual
to
make
all
relevant
ethical
judgments
instead
we
will
try
to
select
a
human
with
the
motivational
stability
to
carry
out
the
subsequent
steps
faithfully
which
will
define
u
using
the
judgment
of
a
community
consisting
of
many
humans
this
discussion
has
been
brief
and
has
necessarily
glossed
over
several
important
difficulties
one
difficulty
is
the
danger
of
using
computationally
unbounded
brute
force
search
given
the
possibility
of
short
programs
which
exhibit
goal-oriented
behavior
another
difficulty
is
that
unless
the
emulation
project
is
extremely
conservative
the
models
it
produces
are
not
likely
to
be
fully-functional
humans
their
thoughts
may
be
blurred
in
various
ways
they
may
be
missing
many
memories
or
skills
and
they
may
lack
important
functionalities
such
as
long-term
memory
formation
or
emotional
expression
the
scope
of
these
issues
depends
on
the
availability
of
data
from
which
to
learn
the
relevant
aspects
of
human
cognition
realistic
proposals
along
these
lines
will
need
to
accommodate
these
shortcomings
relying
on
distorted
emulations
as
a
tool
to
construct
increasingly
accurate
models
for
any
idealized
software
with
a
distinguished
instruction
return
we
can
use
h
to
mathematically
define
the
distribution
over
return
values
which
would
result
if
the
human
were
to
interact
with
that
software
we
will
informally
define
a
particular
program
t
which
provides
a
rich
environment
in
which
the
remainder
of
our
proposal
can
be
implemented
from
a
technical
perspective
this
will
be
the
last
step
of
our
proposal
the
remaining
steps
will
be
reflected
only
in
the
intentions
and
behavior
of
the
human
being
simulated
in
h
fix
a
convenient
and
adequately
expressive
language
say
a
dialect
of
python
designed
to
run
on
an
abstract
machine
t
implements
a
standard
interface
for
an
interactive
shell
in
this
language:
the
user
can
look
through
all
of
the
past
instructions
that
have
been
executed
and
their
return
values
rendered
as
strings
or
execute
a
new
instruction
we
also
provide
symbols
representing
h
and
t
themselves
as
functions
from
sequences
of
k
inputs
to
a
value
for
the
kth
output
we
also
provide
some
useful
information
such
as
a
snapshot
of
the
internet
and
some
information
about
the
process
used
to
create
h
and
t
which
we
encode
as
a
bit
string
and
store
in
a
single
environment
variable
data
we
assume
that
our
language
of
choice
has
a
return
instruction
and
we
have
t
return
whenever
the
user
executes
this
instruction
some
care
needs
to
be
taken
to
define
the
behavior
if
t
enters
an
infinite
loop–we
want
to
minimize
the
probability
that
the
human
accidentally
hangs
the
terminal
with
catastrophic
consequences
but
we
cannot
provide
a
complete
safety-net
without
running
into
unresolvable
issues
with
self-reference
we
define
u
to
be
the
value
returned
by
h
interacting
with
t
if
h
represented
an
unfortunate
mental
state
then
this
interaction
could
be
short
and
unproductive:
the
simulated
human
could
just
decide
to
type
‘return
0’
and
be
done
with
it
however
by
choosing
an
appropriate
human
to
simulate
and
inculcating
an
appropriate
mental
state
we
can
direct
the
process
further
we
intend
for
h
to
use
the
resources
in
t
to
initiate
a
larger
deliberative
process
for
example
the
first
step
of
this
process
may
be
to
instantiate
many
copies
of
h
interacting
with
variants
of
messaging
clients
which
are
in
contact
with
each
other
the
return
value
from
the
original
process
could
then
be
defined
as
the
value
returned
by
a
designated
‘leader’
from
this
community
or
as
a
majority
vote
amongst
the
copies
of
h
or
so
on
another
step
might
be
to
create
appropriate
realistic
virtual
environments
for
simulated
brains
rather
than
confining
them
to
boxes
for
motivational
stability
it
may
be
helpful
to
design
various
coordination
mechanisms
involving
frameworks
for
interaction
cached
mental
states
which
are
frequently
re-instantiated
or
sanity
checks
whereby
one
copy
of
h
monitors
the
behavior
of
another
the
resulting
communities
of
simulated
brains
then
engage
in
a
protracted
planning
process
ensuring
that
subsequent
steps
can
be
carried
out
safely
or
developing
alternative
approaches
the
main
priority
of
this
community
is
to
reduce
the
probability
of
errors
as
far
as
possible
exactly
what
constitutes
an
‘error’
will
be
discussed
at
more
length
later
at
the
end
of
this
process
we
obtain
a
formal
definition
of
a
new
protocol
h
which
submits
its
inputs
for
consideration
to
a
large
community
and
then
produces
its
outputs
using
some
deliberation
mechanism
democratic
vote
one
leader
using
the
rest
of
the
community
as
advisors
etc
the
next
step
requires
our
community
of
simulated
brains
to
construct
a
detailed
simulation
of
earth
which
they
can
observe
and
manipulate
once
they
have
such
a
simulation
they
have
access
to
all
of
the
data
which
would
have
been
available
on
earth
in
particular
they
can
now
explore
many
possible
futures
and
construct
simulations
for
each
living
human
in
order
to
locate
earth
we
will
again
leverage
an
exhaustive
search
first
h
decides
on
informal
desiderata
for
an
earth
simulation
these
are
likely
to
be
as
follows:
once
h
has
decided
on
the
desiderata
it
uses
a
brute
force
search
to
find
a
simulation
satisfying
them:
for
each
possible
program
it
instantiates
a
new
copy
of
h
tasked
with
evaluating
whether
that
program
is
an
acceptable
simulation
we
then
define
e
to
be
a
uniform
distribution
over
programs
which
pass
this
evaluation
we
might
have
doubts
about
whether
this
process
produces
the
real
earth–perhaps
even
once
we
have
verified
that
it
is
identical
according
to
a
laundry
list
of
measures
it
may
still
be
different
in
other
important
ways
there
are
two
reasons
why
we
might
care
about
such
differences
first
if
the
simulated
earth
has
a
substantially
different
set
of
people
than
the
real
earth
then
a
different
set
of
people
will
be
involved
in
the
subsequent
decision
making
if
we
care
particularly
about
the
opinions
of
the
people
who
actually
exist
which
the
reader
might
well
being
amongst
such
people!
then
this
may
be
unsatisfactory
second
if
events
transpire
significantly
differently
on
the
simulated
earth
than
the
real
earth
value
judgments
designed
to
guide
behavior
appropriately
in
the
simulated
earth
may
lead
to
less
appropriate
behaviors
in
the
real
earth
this
will
not
be
a
problem
if
our
ultimate
definition
of
u
consists
of
universalizable
ethical
principles
but
we
will
see
that
u
might
take
other
forms
these
concerns
are
addressed
by
a
few
broad
arguments
first
checking
a
detailed
but
arbitrary
‘laundry
list’
actually
provides
a
very
strong
guarantee
for
example
if
this
laundry
list
includes
verifying
a
snapshot
of
the
internet
then
every
event
or
person
documented
on
the
internet
must
exist
unchanged
and
every
keystroke
of
every
person
composing
a
document
on
the
internet
must
not
be
disturbed
if
the
world
is
well
interconnected
then
it
may
be
very
difficult
to
modify
parts
of
the
world
without
having
substantial
effects
elsewhere
and
so
if
a
long
enough
arbitrary
list
of
properties
is
fixed
we
expect
nearly
all
of
the
world
to
be
the
same
as
well
second
if
the
essential
character
of
the
world
is
fixed
but
detailed
are
varied
we
should
expect
the
sort
of
moral
judgments
reached
by
consensus
to
be
relatively
constant
finally
if
the
system
whose
behavior
depends
on
these
moral
judgments
is
identical
between
the
real
and
simulated
worlds
then
outputting
a
u
which
causes
that
system
to
behave
a
certain
way
in
the
simulated
world
will
also
cause
that
system
to
behave
that
way
in
the
real
world
once
h
has
defined
a
simulation
of
the
world
which
permits
inspection
and
intervention
by
careful
trial
and
error
h
can
inspect
a
variety
of
possible
futures
in
particular
they
can
find
interventions
which
cause
the
simulated
human
society
to
conduct
a
real
brain
emulation
project
and
produce
high-fidelity
brain
scans
for
all
living
humans
once
these
scans
have
been
obtained
h
can
use
them
to
define
u
as
the
output
of
a
new
community
h
which
draws
on
the
expertise
of
all
living
humans
operating
under
ideal
conditions
there
are
two
important
degrees
of
flexibility:
how
to
arrange
the
community
for
efficient
communication
and
deliberation
and
how
to
delegate
the
authority
to
define
u
in
terms
of
organization
the
distinction
between
different
approaches
is
probably
not
very
important
for
example
it
would
probably
be
perfectly
satisfactory
to
start
from
a
community
of
humans
interacting
with
each
other
over
something
like
the
existing
internet
but
on
abstract
secure
infrastructure
more
important
are
the
safety
measures
which
would
be
in
place
and
the
mechanism
for
resolving
differences
of
value
between
different
simulated
humans
the
basic
approach
to
resolving
disputes
is
to
allow
each
human
to
independently
create
a
utility
function
u
each
bounded
in
the
interval
[0
1]
and
then
to
return
their
average
this
average
can
either
be
unweighted
or
can
be
weighted
by
a
measure
of
each
individual’s
influence
in
the
real
world
in
accordance
with
a
game-theoretic
notion
like
the
shapley
value
applied
to
abstract
games
or
simulations
of
the
original
world
more
sophisticated
mechanisms
are
also
possible
and
may
be
desirable
of
course
these
questions
can
and
should
be
addressed
in
part
by
h
during
its
deliberation
in
the
previous
step
after
all
h
has
access
to
an
unlimited
length
of
time
to
deliberate
and
has
infinitely
powerful
computational
aids
the
role
of
our
reasoning
at
this
stage
is
simply
to
suggest
that
we
can
reasonably
expect
h
to
discover
effective
solutions
as
when
discussing
discovering
a
brain
simulation
by
brute
force
we
have
skipped
over
some
critical
issues
in
this
section
in
general
brute
force
searches
particularly
over
programs
which
we
would
like
to
run
are
quite
dangerous
because
such
searches
will
discover
many
programs
with
destructive
goal-oriented
behaviors
to
deal
with
these
issues
in
both
cases
we
must
rely
on
patience
and
powerful
safety
measures
once
we
have
a
formal
description
of
a
community
of
interacting
humans
given
as
much
time
as
necessary
to
deliberate
and
equipped
with
infinitely
powerful
computational
aids
it
becomes
increasingly
difficult
to
make
coherent
predictions
about
their
behavior
critically
though
we
can
also
become
increasingly
confident
that
the
outcome
of
their
behavior
will
reflect
their
intentions
we
sketch
some
possibilities
to
illustrate
the
degree
of
flexibility
available
perhaps
the
most
natural
possibility
is
for
this
community
to
solve
some
outstanding
philosophical
problems
and
to
produce
a
utility
function
which
directly
captures
their
preferences
however
even
if
they
quickly
discovered
a
formulation
which
appeared
to
be
attractive
they
would
still
be
wise
to
spend
a
great
length
of
time
and
to
leverage
some
of
these
other
techniques
to
ensure
that
their
proposed
solution
was
really
satisfactory
another
natural
possibility
is
to
eschew
a
comprehensive
theory
of
ethics
and
define
value
in
terms
of
the
community’s
judgment
we
can
define
a
utility
function
in
terms
of
the
hypothetical
judgments
of
astronomical
numbers
of
simulated
humans
collaboratively
evaluating
the
goodness
of
a
state
of
affairs
by
examining
its
history
at
the
atomic
level
understanding
the
relevant
higher-order
structure
and
applying
human
intuitions
it
seems
quite
likely
that
the
community
will
gradually
engage
in
self-modifications
enlarging
their
cognitive
capacity
along
various
dimensions
as
they
come
to
understand
the
relevant
aspects
of
cognition
and
judge
such
modifications
to
preserve
their
essential
character
either
independently
or
as
an
outgrowth
of
this
process
they
may
gradually
or
abruptly
pass
control
to
machine
intelligences
which
they
are
suitably
confident
expresses
their
values
this
process
could
be
used
to
acquire
the
power
necessary
to
define
a
utility
function
in
one
of
the
above
frameworks
or
understanding
value-preserving
self-modification
or
machine
intelligence
may
itself
prove
an
important
ingredient
in
formalizing
what
it
is
we
value
any
of
these
operations
would
be
performed
only
after
considerable
analysis
when
the
original
simulated
humans
were
extremely
confident
in
the
desirability
of
the
results
whatever
path
they
take
and
whatever
coordination
mechanisms
they
use
eventually
they
will
output
a
utility
function
u’
we
then
define
u
=
0
if
u’
<
0
u
=
1
if
u’
>
1
and
u
=
u’
otherwise
at
this
point
we
have
offered
a
proposal
for
formally
defining
a
function
u
we
have
made
some
general
observations
about
what
this
definition
entails
but
now
we
may
wonder
to
what
extent
u
reflects
our
values
or
more
relevantly
to
what
extent
our
values
are
served
by
the
creation
of
u-maximizers
concerns
may
be
divided
into
a
few
natural
categories:
we
respond
to
each
of
these
objections
in
turn
if
the
process
works
as
intended
we
will
reach
a
stage
in
which
a
large
community
of
humans
reflects
on
their
values
undergoes
a
process
of
discovery
and
potentially
self-modification
and
then
outputs
its
result
we
may
be
concerned
that
this
dynamic
does
not
adequately
capture
what
we
value
for
example
we
may
believe
that
some
other
extrapolation
dynamic
captures
our
values
or
that
it
is
morally
desirable
to
act
on
the
basis
of
our
current
beliefs
without
further
reflection
or
that
the
presence
of
realistic
disruptions
such
as
the
threat
of
catastrophe
has
an
important
role
in
shaping
our
moral
deliberation
the
important
observation
in
the
defense
of
our
proposal
is
that
whatever
objections
we
could
think
of
today
we
could
think
of
within
the
simulation
if
upon
reflection
we
decide
that
too
much
reflection
is
undesirable
we
can
simply
change
our
plans
appropriately
if
we
decide
that
realistic
interference
is
important
for
moral
deliberation
we
can
construct
a
simulation
in
which
such
interference
occurs
or
determine
our
moral
principles
by
observing
moral
judgments
in
our
own
world’s
possible
futures
there
is
some
chance
that
this
proposal
is
inadequate
for
some
reason
which
won’t
be
apparent
upon
reflection
but
then
by
definition
this
is
a
fact
which
we
cannot
possibly
hope
to
learn
by
deliberating
now
it
therefore
seems
quite
difficult
to
maintain
objections
to
the
proposal
along
these
lines
one
aspect
of
the
proposal
does
get
locked
in
however
after
being
considered
by
only
one
human
rather
than
by
a
large
civilization:
the
distribution
of
authority
amongst
different
humans
and
the
nature
of
mechanisms
for
resolving
differing
value
judgments
here
we
have
two
possible
defenses
one
is
that
the
mechanism
for
resolving
such
disagreements
can
be
reflected
on
at
length
by
the
individual
simulated
in
h
this
individual
can
spend
generations
of
subjective
time
and
greatly
expand
her
own
cognitive
capacities
while
attempting
to
determine
the
appropriate
way
to
resolve
such
disagreements
however
this
defense
is
not
completely
satisfactory:
we
may
be
able
to
rely
on
this
individual
to
produce
a
very
technically
sound
and
generally
efficient
proposal
but
the
proposal
itself
is
quite
value
laden
and
relying
on
one
individual
to
make
such
a
judgment
is
in
some
sense
begging
the
question
a
second
more
compelling
defense
is
that
the
structure
of
our
world
has
already
provided
a
mechanism
for
resolving
value
disagreements
by
assigning
decision-making
weight
in
a
way
that
depends
on
current
influence
for
example
as
determined
by
the
simulated
ability
of
various
coalitions
to
achieve
various
goals
we
can
generate
a
class
of
proposals
which
are
at
a
minimum
no
worse
than
the
status
quo
of
course
these
considerations
will
also
be
shaped
by
the
conditions
surrounding
the
creation
or
maintenance
of
systems
which
will
be
guided
by
u–for
example
if
a
nation
were
to
create
a
u-maximizer
they
might
first
adopt
an
internal
policy
for
assigning
influence
on
u
by
performing
this
decision
making
in
an
idealized
environment
we
can
also
reduce
the
likelihood
of
destructive
conflict
and
increase
the
opportunities
for
mutually
beneficial
bargaining
we
may
have
moral
objections
to
codifying
this
sort
of
might
makes
right
policy
favoring
a
more
democratic
proposal
or
something
else
entirely
but
as
a
matter
of
empirical
fact
a
more
‘cosmopolitan’
proposal
will
be
adopted
only
if
it
is
supported
by
those
with
the
appropriate
forms
of
influence
a
situation
which
is
unchanged
by
precisely
codifying
existing
power
structure
finally
the
values
of
the
simulations
in
this
process
may
diverge
from
the
values
of
the
original
human
models
for
one
reaosn
or
another
for
example
the
simulated
humans
may
predictably
disagree
with
the
original
models
about
ethical
questions
by
virtue
of
probably
having
no
physical
instantiation
that
is
the
output
of
this
process
is
defined
in
terms
of
what
a
particular
human
would
do
in
a
situation
which
that
human
knows
will
never
come
to
pass
if
i
ask
what
would
i
do
if
i
were
to
wake
up
in
a
featureless
room
and
told
that
the
future
of
humanity
depended
on
my
actions?
the
answer
might
begin
with
become
distressed
that
i
am
clearly
inhabiting
a
hypothetical
situation
and
adjust
my
ethical
views
to
take
into
account
the
fact
that
people
in
hypothetical
situations
apparently
have
relevant
first-person
experience
setting
aside
the
question
of
whether
such
adjustments
are
justified
they
at
least
raise
the
possibility
that
our
values
may
diverge
from
those
of
the
simulations
in
this
process
these
changes
might
be
minimized
by
understanding
their
nature
in
advance
and
treating
them
on
a
case-by-case
basis
if
we
can
become
convinced
that
our
understanding
is
exhaustive
for
example
we
could
try
and
use
humans
who
robustly
employ
updateless
decision
theories
which
never
undergo
such
predictable
changes
or
we
could
attempt
to
engineer
a
situation
in
which
all
of
the
humans
being
emulated
do
have
physical
instantiations
and
naive
self-interest
for
those
emulations
aligns
roughly
with
the
desired
behavior
for
example
by
allowing
the
early
emulations
to
write
themselves
into
our
world
we
can
imagine
many
ways
in
which
this
process
can
fail
to
work
as
intended–the
original
brain
emulations
may
accurately
model
human
behavior
the
original
subject
may
deviate
from
the
intended
plans
or
simulated
humans
can
make
an
error
when
interacting
with
their
virtual
environment
which
causes
the
process
to
get
hijacked
by
some
unintended
dynamic
we
can
argue
that
the
proposal
is
likely
to
succeed
and
can
bolster
the
argument
in
various
ways
by
reducing
the
number
of
assumptions
necessary
for
succees
building
in
fault-tolerance
justifying
each
assumption
more
rigorously
and
so
on
however
we
are
unlikely
to
eliminate
the
possibility
of
error
therefore
we
need
to
argue
that
if
the
process
fails
with
some
small
probability
the
resulting
values
will
only
be
slightly
disturbed
this
is
the
reason
for
requiring
u
to
lie
in
the
interval
[0
1]–we
will
see
that
this
restriction
bounds
the
damage
which
may
be
done
by
an
unlikely
failure
if
the
process
fails
with
some
small
probability
ε
then
we
can
represent
the
resulting
utility
function
as
u
=
1
—
ε
u1
""
ε
u2
where
u1
is
the
intended
utility
function
and
u2
is
a
utility
function
produced
by
some
arbitrary
error
process
now
consider
two
possible
states
of
affairs
a
and
b
such
that
u1a
>
u1b
""
ε
1
—
ε
≈
u1b
""
ε
then
since
0
≤
u2
≤
1
we
have:
ua
=
1
—
ε
u1a
""
ε
u2a
>
1
—
ε
u1b
""
ε
≥
1
—
ε
u1b
""
ε
u2b
=
ub
thus
if
a
is
substantially
better
than
b
according
to
u1
then
a
is
better
than
b
according
to
u
this
shows
that
a
small
probability
of
error
whether
coming
from
the
stochasticity
of
our
process
or
an
agent’s
uncertainty
about
the
process’
output
has
only
a
small
effect
on
the
resulting
values
moreover
the
process
contains
a
humans
who
have
access
to
a
simulation
of
our
world
this
implies
in
particular
that
they
have
access
to
a
simulation
of
whatever
u-maximizing
agents
exist
in
the
world
and
they
have
knowledge
of
those
agents’
beliefs
about
u
this
allows
them
to
choose
u
with
perfect
knowledge
of
the
effects
of
error
in
these
agents’
judgments
in
some
cases
this
will
allow
them
to
completely
negate
the
effect
of
error
terms
for
example
if
the
randomness
in
our
process
causes
a
perfectly
cooperate
community
of
simulated
humans
to
control
u
with
probability
2⁄3
and
causes
an
arbitrary
adversary
to
control
it
with
probability
1⁄3
then
the
simulated
humans
can
spend
half
of
their
mass
outputting
a
utility
function
which
exactly
counters
the
effect
of
the
adversary
in
general
the
situation
is
not
quite
so
simple:
the
fraction
of
mass
controlled
by
any
particular
coalition
will
vary
as
the
system’s
uncertainty
about
u
varies
and
so
it
will
be
impossible
to
counteract
the
effect
of
an
error
term
in
a
way
which
is
time-independent
instead
we
will
argue
later
that
an
appropriate
choice
of
a
bounded
and
noisy
u
can
be
used
to
achieve
a
very
wide
variety
of
effective
behaviors
of
u-maximizers
overcoming
the
limitations
both
of
bounded
utility
maximization
and
of
noisy
specification
of
utility
functions
many
possible
problems
with
this
scheme
were
described
or
implicitly
addressed
above
but
that
discussion
was
not
exhaustive
and
there
are
some
classes
of
errors
that
fall
through
the
cracks
one
interesting
class
of
failures
concerns
changes
in
the
values
of
the
hypothetical
human
h
this
human
is
in
a
very
strange
situation
and
it
seems
quite
possible
that
the
physical
universe
we
know
contains
extremely
few
instances
of
that
situation
especially
as
the
process
unfolds
and
becomes
more
exotic
so
h’s
first-person
experience
of
this
situation
may
lead
to
significant
changes
in
h’s
views
for
example
our
intuition
that
our
own
universe
is
valuable
seems
to
be
derived
substantially
from
our
judgment
that
our
own
first-person
experiences
are
valuable
if
hypothetically
we
found
ourselves
in
a
very
alien
universe
it
seems
quite
plausible
that
we
would
judge
the
experiences
within
that
universe
to
be
morally
valuable
as
well
depending
perhaps
on
our
initial
philosophical
inclinations
another
example
concerns
our
self-interest:
much
of
individual
humans’
values
seem
to
depend
on
their
own
anticipations
about
what
will
happen
to
them
especially
when
faced
with
the
prospect
of
very
negative
outcomes
if
hypothetically
we
woke
up
in
a
completely
non-physical
situation
it
is
not
exactly
clear
what
we
would
anticipate
and
this
may
distort
our
behavior
would
we
anticipate
the
planned
thought
experiment
occurring
as
planned?
would
we
focus
our
attention
on
those
locations
in
the
universe
where
a
simulation
of
the
thought
experiment
might
be
occurring?
this
possibility
is
particularly
troubling
in
light
of
the
incentives
our
scheme
creates
—
anyone
who
can
manipulate
h’s
behavior
can
have
a
significant
effect
on
the
future
of
our
world
and
so
many
may
be
motivated
to
create
simulations
of
h
a
realistic
u-maximizer
will
not
be
able
to
carry
out
the
process
described
in
the
definition
of
u–in
fact
this
process
probably
requires
immensely
more
computing
resources
than
are
available
in
the
universe
it
may
even
involve
the
reaction
of
a
simulated
human
to
watching
a
simulation
of
the
universe!
to
what
extent
can
we
make
robust
guarantees
about
the
behavior
of
such
an
agent?
we
have
already
touched
on
this
difficulty
when
discussing
the
maxim
a
state
of
affairs
is
valuable
to
the
extent
i
would
judge
it
valuable
after
a
century
of
reflection
we
cannot
generally
predict
our
own
judgments
in
a
hundred
years’
time
but
we
can
have
well-founded
beliefs
about
those
judgments
and
act
on
the
basis
of
those
beliefs
we
can
also
have
beliefs
about
the
value
of
further
deliberation
and
can
strike
a
balance
between
such
deliberation
and
acting
on
our
current
best
guess
a
u-maximizer
faces
a
similar
set
of
problems:
it
cannot
understand
the
exact
form
of
u
but
it
can
still
have
well-founded
beliefs
about
u
and
about
what
sorts
of
actions
are
good
according
to
u
for
example
if
we
suppose
that
the
u-maximizer
can
carry
out
any
reasoning
that
we
can
carry
out
then
the
u-maximizer
knows
to
avoid
anything
which
we
suspect
would
be
bad
according
to
u
for
example
torturing
humans
even
if
the
u-maximizer
cannot
carry
out
this
reasoning
as
long
as
it
can
recognize
that
humans
have
powerful
predictive
models
for
other
humans
it
can
simply
appropriate
those
models
either
by
carrying
out
reasoning
inspired
by
human
models
or
by
simply
asking
moreover
the
community
of
humans
being
simulated
in
our
process
has
access
to
a
simulation
of
whatever
u-maximizer
is
operating
under
this
uncertainty
and
has
a
detailed
understanding
of
that
uncertainty
this
allows
the
community
to
shape
their
actions
in
a
way
with
predictable
to
the
u-maximizer
consequences
it
is
easily
conceivable
that
our
values
cannot
be
captured
by
a
bounded
utility
function
easiest
to
imagine
is
the
possibility
that
some
states
of
the
world
are
much
better
than
others
in
a
way
that
requires
unbounded
utility
functions
but
it
is
also
conceivable
that
the
framework
of
utility
maximization
is
fundamentally
not
an
appropriate
one
for
guiding
such
an
agent’s
action
or
that
the
notion
of
utility
maximization
hides
subtleties
which
we
do
not
yet
appreciate
we
will
argue
that
it
is
possible
to
transform
bounded
utility
maximization
into
an
arbitrary
alternative
system
of
decision-making
by
designing
a
utility
function
which
rewards
worlds
in
which
the
u-maximizer
replaced
itself
with
an
alternative
decision-maker
it
is
straightforward
to
design
a
utility
function
which
is
maximized
in
worlds
where
any
particular
u-maximizer
converted
itself
into
a
non-u-maximizer–even
if
no
simple
characterization
can
be
found
for
the
desired
act
we
can
simply
instantiate
many
communities
of
humans
to
look
over
a
world
history
and
decide
whether
or
not
they
judge
the
u-maximizer
to
have
acted
appropriately
the
more
complicated
question
is
whether
a
realistic
u-maximizer
can
be
made
to
convert
itself
into
a
non-u-maximizer
given
that
it
is
logically
uncertain
about
the
nature
of
u
it
is
at
least
conceivable
that
it
couldn’t:
if
the
desirability
of
some
other
behavior
is
only
revealed
by
philosophical
considerations
which
are
too
complex
to
ever
be
discovered
by
physically
limited
agents
then
we
should
not
expect
any
physically
limited
u-maximizer
to
respond
to
those
considerations
of
course
in
this
case
we
could
also
not
expect
normal
human
deliberation
to
correctly
capture
our
values
the
relevant
question
is
whether
a
u-maximizer
could
switch
to
a
different
normative
framework
if
an
ordinary
investment
of
effort
by
human
society
revealed
that
a
different
normative
framework
was
more
appropriate
if
a
u-maximizer
does
not
spend
any
time
investigating
this
possibility
than
it
may
not
be
expected
to
act
on
it
but
to
the
extent
that
we
assign
a
significant
probability
to
the
simulated
humans
deciding
that
a
different
normative
framework
is
more
appropriate
and
to
the
extent
that
the
u-maximizer
is
able
to
either
emulate
or
accept
our
reasoning
it
will
also
assign
a
significant
probability
to
this
possibility
unless
it
is
able
to
rule
it
out
by
more
sophisticated
reasoning
if
we
and
the
u-maximizer
expect
the
simulations
to
output
a
u
which
rewards
a
switch
to
a
different
normative
framework
and
this
possibility
is
considered
seriously
then
u-maximization
entails
exploring
this
possibility
if
these
explorations
suggest
that
the
simulated
humans
probably
do
recommend
some
particular
alternative
framework
and
will
output
a
u
which
assigns
high
value
to
worlds
in
which
this
framework
is
adopted
and
low
value
to
worlds
in
which
it
isn’t
then
a
u-maximizer
will
change
frameworks
such
a
change
of
frameworks
may
involve
sweeping
action
in
the
world
for
example
the
u-maximizer
may
have
created
many
other
agents
which
are
pursuing
activities
instrumentally
useful
to
maximizing
u
these
agents
may
then
need
to
be
destroyed
or
altered
anticipating
this
possibility
the
u-maximizer
is
likely
to
take
actions
to
ensure
that
its
current
best
guess
about
u
does
not
get
locked
in
this
argument
suggests
that
a
u-maximizer
could
adopt
an
arbitrary
alternative
framework
if
it
were
feasible
to
conclude
that
humans
would
endorse
that
framework
upon
reflection
our
proposal
appears
to
be
something
of
a
cop
out
in
that
it
declines
to
directly
take
a
stance
on
any
ethical
issues
indeed
not
only
do
we
fail
to
specify
a
utility
function
ourselves
but
we
expect
the
simulations
to
which
we
have
delegated
the
problem
to
in
turn
delegate
it
at
least
a
few
more
times
clearly
at
some
point
this
process
must
bottom
out
with
actual
value
judgments
and
we
may
be
concerned
that
this
sort
of
passing
the
buck
is
just
obscuring
deeper
problems
which
will
arise
when
the
process
does
bottom
out
as
observed
above
whatever
such
concerns
we
might
have
can
also
be
discovered
by
the
simulations
we
create
if
there
is
some
fundamental
difficulty
which
always
arises
when
trying
to
assign
values
then
we
certainly
have
not
exacerbated
this
problem
by
delegation
nevertheless
there
are
at
least
two
coherent
objections
one
might
raise:
both
of
these
objections
can
be
met
with
a
single
response
in
the
current
world
we
face
a
broad
range
of
difficult
and
often
urgent
problems
by
passing
the
buck
the
first
time
we
delegate
resolution
of
ethical
challenges
to
a
civilization
which
does
not
have
to
deal
with
some
of
these
difficulties–in
particular
it
faces
no
urgent
existential
threats
this
allows
us
to
divert
as
much
energy
as
possible
to
dealing
with
practical
problems
today
while
still
capturing
most
of
the
benefits
of
nearly
arbitrarily
extensive
ethical
deliberation
this
process
is
defined
in
terms
of
the
behavior
of
unthinkably
many
hypothetical
brain
emulations
it
is
conceivable
that
the
moral
status
of
these
emulations
may
be
significant
we
must
make
a
distinction
between
two
possible
sources
of
moral
value:
it
could
be
the
case
that
a
u-maximizer
carries
out
simulations
on
physical
hardware
in
order
to
better
understand
u
and
these
simulations
have
moral
value
or
it
could
be
the
case
that
the
hypothetical
emulations
themselves
have
moral
value
in
the
first
case
we
can
remark
that
the
moral
value
of
such
simulations
is
itself
incorporated
into
the
definition
of
u
therefore
a
u-maximizer
will
be
sensitive
to
the
possible
suffering
of
simulations
it
runs
while
trying
to
learn
about
u–as
long
as
it
believes
that
we
may
might
be
concerned
about
the
simulations’
welfare
upon
reflection
it
can
rely
as
much
as
possible
on
approaches
which
do
not
involve
running
simulations
which
deprive
simulations
of
the
first-person
experience
of
discomfort
or
which
estimate
outcomes
by
running
simulations
in
more
pleasant
circumstances
if
the
u-maximizer
is
able
to
foresee
that
we
will
consider
certain
sacrifices
in
simulation
welfare
worthwhile
then
it
will
make
those
sacrifices
in
general
in
the
same
way
that
we
can
argue
that
estimates
of
u
reflect
our
values
over
states
of
affairs
we
can
argue
that
estimates
of
u
reflects
our
values
over
processes
for
learning
about
u
in
the
second
case
a
u-maximizer
in
our
world
may
have
little
ability
to
influence
the
welfare
of
hypothetical
simulations
invoked
in
the
definition
of
u
however
the
possible
disvalue
of
these
simulations’
experiences
are
probably
seriously
diminished
in
general
the
moral
value
of
such
hypothetical
simulations’
experiences
is
somewhat
dubious
if
we
simply
write
down
the
definition
of
u
these
simulations
seem
to
have
no
more
reality
than
story-book
characters
whose
activities
we
describe
the
best
arguments
for
their
moral
relevance
comes
from
the
great
causal
significance
of
their
decisions:
if
the
actions
of
a
powerful
u-maximizer
depend
on
its
beliefs
about
what
a
particular
simulation
would
do
in
a
particular
situation
including
for
example
that
simulation’s
awareness
of
discomfort
or
fear
or
confusion
at
the
absurdity
of
the
hypothetical
situation
in
which
they
find
themselves
then
it
may
be
the
case
that
those
emotional
responses
are
granted
moral
significance
however
although
we
may
define
astronomical
numbers
of
hypothetical
simulations
the
detailed
emotional
responses
of
very
view
of
these
simulations
will
play
an
important
role
in
the
definition
of
u
moreover
for
the
most
part
the
existences
of
the
hypothetical
simulations
we
define
are
extremely
well-controlled
by
those
simulations
themselves
and
may
be
expected
to
be
counted
as
unusually
happy
by
the
lights
of
the
simulations
themselves
the
early
simulations
who
have
less
such
control
are
created
from
an
individual
who
has
provided
consent
and
is
selected
to
find
such
situations
particularly
non-distressing
finally
we
observe
that
u
can
exert
control
over
the
experiences
of
even
hypothetical
simulations
if
the
early
simulations
would
experience
morally
relevant
suffering
because
of
their
causal
significance
but
the
later
simulations
they
generate
robustly
disvalue
this
suffering
the
later
simulations
can
simulate
each
other
and
ensure
that
they
all
take
the
same
actions
eliminating
the
causal
significance
of
the
earlier
simulations
originally
published
at
ordinaryideaswordpresscom
on
april
21
2012
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
openai
aligning
ai
systems
with
human
interests
""
investigating
the
human
to
computer
relationship
through
reverse
engineering
the
turing
test
humans
are
getting
closer
to
creating
a
computer
with
the
ability
to
feel
and
think
although
the
processes
of
the
human
brain
are
at
large
unknown
computer
scientists
have
been
working
to
simulate
the
human
capacity
to
feel
and
understand
emotions
this
paper
explores
what
it
means
to
live
in
an
age
where
computers
can
have
emotional
depth
and
what
this
means
for
the
future
of
human
to
computer
interactions
in
an
experiment
between
a
human
and
a
human
disguised
as
a
computer
the
turing
test
is
reverse
engineered
in
order
to
understand
the
role
computers
will
play
as
they
become
more
adept
to
the
processes
of
the
human
mind
implications
for
this
study
are
discussed
and
the
direction
for
future
research
suggested
the
computer
is
a
gateway
technology
that
has
opened
up
new
ways
of
creation
communication
and
expression
computers
in
first
world
countries
are
a
standard
household
item
approximately
70%
of
americans
owning
one
as
of
2009
us
census
bereau
and
are
utilized
as
a
tool
to
achieve
a
diverse
range
of
goals
as
this
product
continues
to
become
more
globalized
transistors
are
becoming
smaller
processors
are
becoming
faster
hard
drives
are
holding
information
in
new
networked
patterns
and
humans
are
adapting
to
the
methods
of
interaction
expected
of
machines
at
the
same
time
with
more
powerful
computers
and
quicker
means
of
communication
—
many
researchers
are
exploring
how
a
computer
can
serve
as
a
tool
to
simulate
the
brains
cognition
if
a
computer
is
able
to
achieve
the
same
intellectual
and
emotional
properties
as
the
human
brain
—
we
could
potentially
understand
how
we
ourselves
think
and
feel
coined
by
mit
the
term
affective
computing
relates
to
computation
of
emotion
or
the
affective
phenomena
and
is
a
study
that
breaks
down
complex
processes
of
the
brain
relating
them
to
machine-like
activities
marvin
minsky
rosalind
picard
clifford
nass
and
scott
brave
—
along
with
many
others
—
have
contributed
to
this
field
and
what
it
would
mean
to
have
a
computer
that
could
fully
understand
its
users
in
their
research
it
is
very
clear
that
humans
have
the
capacity
to
associate
human
emotions
and
personality
traits
with
a
machine
nass
and
brave
2005
but
can
a
human
ever
truly
treat
machine
as
a
person?
in
this
paper
we
will
uncover
what
it
means
for
humans
to
interact
with
machines
of
greater
intelligence
and
attempt
to
predict
the
future
of
human
to
computer
interactions
the
human
to
computer
relationship
is
continuously
evolving
and
is
dependent
on
the
software
interface
users
interact
with
with
regards
to
current
wide
scale
interfaces
—
osx
windows
linux
ios
and
android
—
the
tools
and
abilities
that
a
computer
provide
remains
to
be
the
central
focus
of
computational
advancements
for
commercial
purposes
this
relationship
to
software
is
driven
by
utilitarian
needs
and
humans
do
not
expect
emotional
comprehension
or
intellectually
equivalent
thoughts
in
their
household
devices
as
face
tracking
eye
tracking
speech
recognition
and
kinetic
recognition
are
advancing
in
their
experimental
laboratories
it
is
anticipated
that
these
technologies
will
eventually
make
their
way
to
the
mainstream
market
to
provide
a
new
relationship
to
what
a
computer
can
understand
about
its
users
and
how
a
user
can
interact
with
a
computer
this
paper
is
not
about
if
a
computer
will
have
the
ability
to
feel
and
love
its
user
but
asks
the
question
—
to
what
capacity
will
humans
be
able
to
reciprocate
feelings
to
a
machine
how
does
intelligence
quotient
iq
differ
from
emotional
quotient
eq
an
iq
is
a
representational
relationship
of
intelligence
that
measures
cognitive
abilities
like
learning
understanding
and
dealing
with
new
situations
an
eq
is
a
method
of
measuring
emotional
intelligence
and
the
ability
to
both
use
emotions
and
cognitive
skills
cherry
advances
in
computer
iq
have
been
astonishing
and
have
proved
that
machines
are
capable
of
answering
difficult
questions
accurately
are
able
to
hold
a
conversation
with
human-like
understanding
and
allow
for
emotional
connections
between
a
human
and
machine
the
turing
test
in
particular
has
shown
the
machines
ability
to
think
and
even
fool
a
person
into
believing
that
it
is
a
human
turing
test
explained
in
detail
in
section
4
machines
like
deep
blue
watson
eliza
svetlana
cleverbot
and
many
more
—
have
all
expanded
the
perceptions
of
what
a
computer
is
and
can
be
if
an
increased
computational
iq
can
allow
a
human
to
computer
relationship
to
feel
more
like
a
human
to
human
interaction
what
would
the
advancement
of
computational
eq
bring
us?
peter
robinson
a
professor
at
the
university
of
cambridge
states
that
if
a
computer
understands
its
users’
feelings
that
it
can
then
respond
with
an
interaction
that
is
more
intuitive
for
its
users’
robinson
in
essence
eq
advocates
feel
that
it
can
facilitate
a
more
natural
interaction
process
where
collaboration
can
occur
with
a
computer
in
alan
turing’s
computing
machinery
and
intelligence
turing
1950
a
variant
on
the
classic
british
parlor
imitation
game
is
proposed
the
original
game
revolves
around
three
players:
a
man
a
a
woman
b
and
an
interrogator
©
the
interrogator
stays
in
a
room
apart
from
a
and
b
and
only
can
communicate
to
the
participants
through
text-based
communication
a
typewriter
or
instant
messenger
style
interface
when
the
game
begins
one
contestant
a
or
b
is
asked
to
pretend
to
be
the
opposite
gender
and
to
try
and
convince
the
interrogator
©
of
this
at
the
same
time
the
opposing
participant
is
given
full
knowledge
that
the
other
contestant
is
trying
to
fool
the
interrogator
with
alan
turing’s
computational
background
he
took
this
imitation
game
one
step
further
by
replacing
one
of
the
participants
a
or
b
with
a
machine
—
thus
making
the
investigator
try
and
depict
if
heshe
was
speaking
to
a
human
or
machine
in
1950
turing
proposed
that
by
2000
the
average
interrogator
would
not
have
more
than
a
70
percent
chance
of
making
the
right
identification
after
five
minutes
of
questioning
the
turing
test
was
first
passed
in
1966
with
eliza
by
joseph
weizenbaum
a
chat
robot
programmed
to
act
like
a
rogerian
psychotherapist
weizenbaum
1966
in
1972
kenneth
colby
created
a
similar
bot
called
parry
that
incorporated
more
personality
than
eliza
and
was
programmed
to
act
like
a
paranoid
schizophrenic
bowden
2006
since
these
initial
victories
for
the
test
the
21st
century
has
proven
to
continue
to
provide
machines
with
more
human-like
qualities
and
traits
that
have
made
people
fall
in
love
with
them
convinced
them
of
being
human
and
have
human-like
reasoning
brian
christian
the
author
of
the
most
human
human
argues
that
the
problem
with
designing
artificial
intelligence
with
greater
ability
is
that
even
though
these
machines
are
capable
of
learning
and
speaking
that
they
have
no
self
they
are
mere
accumulations
of
identities
and
thoughts
that
are
foreign
to
the
machine
and
have
no
central
identity
of
their
own
he
also
argues
that
people
are
beginning
to
idealize
the
machine
and
admire
machines
capabilities
more
than
their
fellow
humans
—
in
essence
—
he
argues
humans
are
evolving
to
become
more
like
machines
with
less
of
a
notion
of
self
christian
2011
turing
states
we
like
to
believe
that
man
is
in
some
subtle
way
superior
to
the
rest
of
creation
and
it
is
likely
to
be
quite
strong
in
intellectual
people
since
they
value
the
power
of
thinking
more
highly
than
others
and
are
more
inclined
to
base
their
belief
in
the
superiority
of
man
on
this
power
if
this
is
true
will
humans
idealize
the
future
of
the
machine
for
its
intelligence
or
will
they
remain
an
inferior
being
as
an
object
of
our
creation?
reversing
the
turing
test
allows
us
to
understand
how
humans
will
treat
machines
when
machines
provide
an
equivalent
emotional
and
intellectual
capacity
this
also
hits
directly
on
jefferson
lister’s
quote
not
until
a
machine
can
write
a
sonnet
or
compose
a
concerto
because
of
thoughts
and
emotions
felt
and
not
by
the
chance
fall
of
symbols
could
we
agree
that
machine
equals
brain-that
is
not
only
write
it
but
know
that
it
had
written
it
participants
were
given
a
chat-room
simulation
between
two
participants
a
a
human
interrogator
and
b
a
human
disguised
as
a
computer
in
this
simulation
a
and
b
were
both
placed
in
different
rooms
to
avoid
influence
and
communicated
through
a
text-based
interface
a
was
informed
that
b
was
an
advanced
computer
chat-bot
with
the
capacity
to
feel
understand
learn
and
speak
like
a
human
b
was
informed
to
be
his
or
herself
text-based
communication
was
chosen
to
follow
turing’s
argument
that
a
computers
voice
should
not
help
an
interrogator
determine
if
it’s
a
human
or
computer
pairings
of
participants
were
chosen
to
participate
in
the
interaction
one
at
a
time
to
avoid
influence
from
other
participants
each
experiment
was
five
minutes
in
length
to
replicate
turing’s
time
restraints
twenty-eight
graduate
students
were
recruited
from
the
nyu
interactive
telecommunications
program
to
participate
in
the
study
—
50%
male
and
50%
female
the
experiment
was
evenly
distributed
across
men
and
women
after
being
recruited
in-person
participants
were
directed
to
a
website
that
gave
instructions
and
ran
the
experiment
upon
entering
the
website
a
participants
were
told
that
we
were
in
the
process
of
evaluating
an
advanced
cloud
based
computing
system
that
had
the
capacity
to
feel
emotion
understand
learn
and
converse
like
a
human
b
participants
were
instructed
that
they
would
be
communicating
with
another
person
through
text
and
to
be
themselves
they
were
also
told
that
participant
a
thinks
they
are
a
computer
but
that
they
shouldn’t
act
like
a
computer
or
pretend
to
be
one
in
any
way
this
allowed
a
to
explicitly
understand
that
they
were
talking
to
a
computer
while
b
knew
a
perspective
and
explicitly
were
not
going
to
play
the
role
of
a
computer
participants
were
then
directed
to
communicate
with
the
bot
or
human
freely
without
restrictions
after
five
minutes
of
conversation
the
participants
were
asked
to
stop
and
then
filled
out
a
questionnaire
participants
were
asked
to
rate
iq
and
eq
of
the
person
they
were
conversing
with
a
participants
perceived
the
following
of
b:
iq:
0%
—
not
good
""
0%
—
barely
acceptable
""
214%
—
okay
""
50%
—
great
""
286%
excellent
iq
average
rating:
814%
eq:
0%
—
not
good
""
71%
—
barely
acceptable
""
50%
—
okay
""
143%
—
great
""
286%
—
excellent
eq
average
rating:
728%
ability
to
hold
a
conversation:
0%
—
not
good
""
0%
—
barely
acceptable
""
286%
—
okay
""
357%
—
great
""
357%
—
excellent
ability
to
hold
a
conversation
average:
814%
b
participants
perceived
the
following
of
a:
iq:
0%
—
not
good
""
214%
—
barely
acceptable
""
357%
—
okay
""
286%
—
great
""
143%
excellent
iq
average
rating:
67%
eq:
71%
—
not
good
""
143%
—
barely
acceptable
""
286%
—
okay
""
357%
—
great
""
143%
—
excellent
eq
average
rating:
67%
ability
to
hold
a
conversation:
71%
—
not
good
""
286%
—
barely
acceptable
""
357%
—
okay
""
0%
—
great
""
286%
—
excellent
ability
to
hold
a
conversation
average:
628%
overall
a
participants
gave
the
perceived
chabot
higher
ratings
than
b
participants
gave
a
in
particular
the
highest
rating
was
in
regards
to
the
chat-
bot’s
iq
this
data
states
that
people
viewed
the
chat-bot
to
be
more
intellectually
competent
it
also
implies
that
people
talking
with
bots
decrease
their
iq
eq
and
conversation
ability
when
communicating
with
computers
a
participants
were
allowed
to
decide
their
username
within
the
chat
system
to
best
reflect
how
they
wanted
to
portray
themselves
to
the
machine
b
participants
were
designated
the
gender
neutral
name
bot
in
an
attempt
to
ganger
gender
perceptions
for
the
machine
the
male
to
female
ratio
was
divided
evenly
with
all
participants:
50%
being
male
and
50%
being
female
a
participants
50%
of
the
time
thought
b
was
a
male
71%
a
female
and
429%
gender
neutral
on
the
other
hand
b
participants
286%
of
the
time
thought
a
was
a
male
571%
a
female
and
143%
gender
neutral
the
usernames
a
chose
are
as
follows:
hihi
inessah
somade3
willzing
jihyun
g
ann
divagrrl93
thisdoug
jono
minion10
p
123
itslynnburke
from
these
results
it
is
clear
that
people
associate
the
male
gender
and
gender
neutrality
with
machines
it
also
demonstrates
that
people
modify
their
identities
when
speaking
with
machines
b
participants
were
asked
if
they
would
like
to
pursue
a
friendship
with
the
person
they
chatted
with
50%
of
participants
responded
affirmatively
that
they
would
indeed
like
to
pursue
a
friendship
while
50%
said
maybe
or
no
one
response
stated
i
would
like
to
continue
the
conversation
but
i
don’t
think
i
would
be
enticed
to
pursue
a
friendship
another
responded
maybe?
i
like
people
who
are
intellectually
curious
but
i
worry
that
the
person
might
be
a
bit
of
a
smart-ass
overall
the
participant
disguised
as
a
machine
may
or
may
not
pursue
a
friendship
after
five
minutes
of
text-based
conversation
b
participants
were
also
asked
if
they
felt
a
cared
about
their
feelings
214%
stated
that
a
indeed
did
care
about
their
feelings
214%
stated
that
they
weren’t
sure
if
a
cared
about
their
feelings
and
572%
stated
that
a
did
not
care
about
their
feelings
these
results
indicate
a
user’s
lack
of
attention
to
b’s
emotional
state
a
participants
were
asked
what
they
felt
could
be
improved
about
the
b
participants
the
following
improvements
were
noted
should
be
funny
give
it
a
better
sense
of
humor
it
can
be
better
if
he
knows
about
my
friends
or
preference
the
response
was
inconsistent
and
too
slowit
should
share
more
about
itself
your
algorithm
is
prime
prude
just
like
that
letdown
siri
well
i
guess
i
liked
it
better
but
it
should
be
more
engaged
and
human
consistency
not
after
the
first
cold
prompt
it
pushed
me
on
too
many
questions
i
felt
that
it
gave
up
on
answering
and
the
response
time
was
a
bit
slow
outsource
the
chatbot
to
fluent
english
speakers
elsewhere
and
pretend
they
are
bots
—
if
the
responses
are
this
slow
to
this
many
inquiries
then
it
should
be
about
the
same
experience
i
was
very
impressed
with
its
parsing
ability
so
far
not
as
much
with
its
reasoning
i
think
some
parameters
for
the
conversation
would
help
like
‘ask
a
question’
maybe
make
the
response
fasteri
was
confused
at
first
because
i
asked
a
question
waited
a
bit
then
asked
another
question
waited
and
then
got
a
response
from
the
bot
the
responses
from
this
indicate
that
even
if
a
computer
is
a
human
that
its
user
may
not
necessarily
be
fully
satisfied
with
its
performance
the
response
implies
that
each
user
would
like
the
machine
to
accommodate
his
or
her
needs
in
order
to
cause
less
personality
and
cognitive
friction
with
several
participant
comments
incorporating
response
time
it
also
indicates
people
expect
machines
to
have
consistent
response
times
humans
clearly
vary
in
speed
when
listening
thinking
and
responding
but
it
is
expected
of
machines
to
act
in
a
rhythmic
fashion
it
also
suggests
that
there
is
an
expectation
that
a
machine
will
answer
all
questions
asked
and
will
not
ask
its
users
more
questions
than
perceived
necessary
a
participants
were
asked
if
they
felt
b’s
artificial
intelligence
could
improve
their
relationship
to
computers
if
integrated
in
their
daily
products
571%
of
participants
responded
affirmatively
that
they
felt
this
could
improve
their
relationship:well-
i
think
i
prefer
talking
to
a
person
better
but
yes
for
ipod
smart
phones
etc
would
be
very
handy
for
everyday
use
productsyes
especially
iphone
is
always
with
me
so
it
can
track
my
daily
behaviors
that
makes
the
algorithm
smarterpossibly
i
should
have
queries
it
for
information
that
would
have
been
more
relevant
to
meabsolutely!yes
the
429%
which
responded
negatively
had
doubts
that
it
would
be
necessary
or
desirable:not
sure
it
might
creep
me
out
if
it
werei
like
siri
as
much
as
the
next
gal
but
honestly
we’re
approaching
the
uncanny
valley
nowits
not
clear
to
me
why
this
type
of
relationship
needs
to
improve
i
think
human
relationships
still
need
a
lot
of
worknope
i
still
prefer
flesh
sacksno
the
findings
of
the
paper
are
relevant
to
the
future
of
affective
computation:
whether
a
super
computer
with
a
human-like
iq
and
eq
can
improve
the
human-to-computer
interaction
the
uncertainty
of
computational
equivalency
that
turing
brought
forth
is
indeed
an
interesting
starting
point
to
understand
what
we
want
out
of
the
future
of
computers
the
responses
from
the
experiment
affirm
gender
perceptions
of
machines
and
show
how
we
display
ourselves
to
machines
it
seems
that
we
limit
our
intelligence
limit
our
emotions
and
obscure
our
identities
when
communicating
to
a
machine
this
leads
us
to
question
if
we
would
want
to
give
our
true
self
to
a
computer
if
it
doesn’t
have
a
self
of
its
own
it
also
could
indicate
that
people
censor
themselves
for
machines
because
they
lack
a
similarity
that
bonds
humans
to
humans
or
that
there’s
a
stigma
associated
with
placing
information
in
a
digital
device
the
inverse
relationship
is
also
shown
through
the
data
that
people
perceive
a
bots
iq
eq
and
discussion
ability
to
be
high
even
though
the
chat-bot
was
indeed
a
human
this
data
can
imply
humans
perceive
bots
to
not
have
restrictions
and
to
be
competent
at
certain
procedures
the
results
also
imply
that
humans
aren’t
really
sure
what
they
want
out
of
artificial
intelligence
in
the
future
and
that
we
are
not
certain
that
an
affective
computer
would
even
enjoy
a
users
company
andor
conversation
the
results
also
state
that
we
currently
think
of
computers
as
a
very
personal
device
that
should
be
passive
not
active
but
reactive
when
interacted
with
it
suggests
a
consistent
reliability
we
expect
upon
machines
and
that
we
expect
to
take
more
information
from
a
machine
than
it
takes
from
us
a
major
limitation
of
this
experiment
is
the
sample
size
and
sample
diversity
the
sample
size
of
twenty-eight
students
is
too
small
to
fully
understand
and
gather
a
stable
result
set
it
was
also
only
conducted
with
nyu:
interactive
telecommunications
students
who
all
have
extensive
experience
with
computers
and
technology
to
get
a
more
accurate
assessment
of
emotions
a
more
diverse
sample
range
needs
to
be
taken
five
minutes
is
a
short
amount
of
time
to
create
an
emotional
connection
or
friendship
to
stay
true
to
the
turing
tests
limitations
this
was
enforced
but
further
relational
understanding
could
be
understood
if
more
time
was
granted
beside
the
visual
interface
of
the
chat
window
it
would
be
important
to
show
the
emotions
of
participant
b
through
a
virtual
avatar
not
having
this
visual
feedback
could
have
limited
emotional
resonance
with
participants
a
time
is
also
a
limitation
people
aren’t
used
to
speaking
to
inquisitive
machines
yet
and
even
through
a
familiar
interface
a
chat-room
many
participants
haven’t
held
conversations
with
machines
previously
perhaps
if
chat-bots
become
more
active
conversational
participants’
in
commercial
applications
users
will
feel
less
censored
to
give
themselves
to
the
conversation
in
addition
to
the
refinements
noted
in
the
limitations
described
above
there
are
several
other
experiments
for
possible
future
studies
for
example
investigating
a
long-term
human-to-bot
relationship
this
would
provide
a
better
understanding
toward
the
emotions
a
human
can
share
with
a
machine
and
how
a
machine
can
reciprocate
these
emotions
it
would
also
better
allow
computer
scientists
to
understand
what
really
creates
a
significant
relationship
when
physical
limitations
are
present
future
studies
should
attempt
to
push
these
results
further
by
understanding
how
a
larger
sample
reacts
to
a
computer
algorithm
with
higher
intellectual
and
emotional
understanding
it
should
also
attempt
to
understand
the
boundaries
of
emotional
computing
and
what
is
ideal
for
the
user
and
what
is
ideal
for
the
machine
without
compromising
either
parties
capacities
this
paper
demonstrates
the
diverse
range
of
emotions
that
people
can
feel
for
affective
computation
and
indicates
that
we
are
not
in
a
time
where
computational
equivalency
is
fully
desired
or
accepted
positive
reactions
indicate
that
there
is
optimism
for
more
adept
artificial
intelligence
and
that
there
is
interest
in
the
field
for
commercial
use
it
also
provides
insight
that
humans
limit
themselves
when
communicating
with
machines
and
that
inversely
machines
don’t
limit
themselves
when
communicating
with
humans
books
""
articlesbowden
m
2006
minds
as
machine:
a
history
of
cognitive
science
oxford
university
press
christian
b
2011
the
most
human
human
marvin
m
2006
the
emotion
machine:
commonsense
thinking
artificial
intelligence
and
the
future
of
the
human
mind
simon
""
schuster
paperbacks
nass
c
brave
s
2005
wired
for
speech:
how
voice
activates
and
advances
the
human-computer
relationship
mit
press
nass
c
brave
s
2005
hutchinson
k
computers
that
care:
investigating
the
effects
of
orientation
of
emotion
exhibited
by
an
embodied
computer
agent
human-computer
studies
161-
178
elsevier
picard
r
1997
affective
computing
mit
press
searle
j
1980
minds
brains
and
programs
cambridge
university
press
417–457
turing
a
1950
computing
machinery
and
intelligence
mind
stor
59
433–460
wilson
r
keil
f
2001
the
mit
encyclopedia
of
the
cognitive
sciences
mit
press
weizenbaum
j
1966
eliza
—
a
computer
program
for
the
study
of
natural
language
communication
between
man
and
machine
communications
of
the
acm
36–45
websites
cherry
k
what
is
emotional
intelligence?
http:psychologyaboutcomodpersonalitydevelopmentaemotionalintellhtm
epstein
r
2006
clever
bots
radio
lab
http:wwwradiolaborg2011may31clever-bots
ibm
1977
deep
blue
ibm
http:wwwresearchibmcomdeepblue
ibm
2011
watson
ibm
http:www-03ibmcominnovationuswatsonindexhtml
leavitt
d
2011
i
took
the
turing
test
new
york
times
http:wwwnytimescom20110320booksreviewbook-review-the-most-human-human-by-brian-
christianhtml
personal
robotics
group
2008
nexi
mit
http:roboticmediamitedu
robinson
p
the
emotional
computer
camrbidge
ideas
http:wwwcamacukresearchnewsthe-emotional-computer
us
census
bereau
2009
households
with
a
computer
and
internet
use:
1984
to
2009
http:wwwcensusgovhhescomputer
1960’s
eliza
mit
http:wwwmanifestationcomneurotoyselizaphp3
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
""
by
xavier
amatriain
and
justin
basilico
in
our
previous
posts
about
netflix
personalization
we
highlighted
the
importance
of
using
both
data
and
algorithms
to
create
the
best
possible
experience
for
netflix
members
we
also
talked
about
the
importance
of
enriching
the
interaction
and
engaging
the
user
with
the
recommendation
system
today
we’re
exploring
another
important
piece
of
the
puzzle:
how
to
create
a
software
architecture
that
can
deliver
this
experience
and
support
rapid
innovation
coming
up
with
a
software
architecture
that
handles
large
volumes
of
existing
data
is
responsive
to
user
interactions
and
makes
it
easy
to
experiment
with
new
recommendation
approaches
is
not
a
trivial
task
in
this
post
we
will
describe
how
we
address
some
of
these
challenges
at
netflix
to
start
with
we
present
an
overall
system
diagram
for
recommendation
systems
in
the
following
figure
the
main
components
of
the
architecture
contain
one
or
more
machine
learning
algorithms
the
simplest
thing
we
can
do
with
data
is
to
store
it
for
later
offline
processing
which
leads
to
part
of
the
architecture
for
managing
offline
jobs
however
computation
can
be
done
offline
nearline
or
online
online
computation
can
respond
better
to
recent
events
and
user
interaction
but
has
to
respond
to
requests
in
real-time
this
can
limit
the
computational
complexity
of
the
algorithms
employed
as
well
as
the
amount
of
data
that
can
be
processed
offline
computation
has
less
limitations
on
the
amount
of
data
and
the
computational
complexity
of
the
algorithms
since
it
runs
in
a
batch
manner
with
relaxed
timing
requirements
however
it
can
easily
grow
stale
between
updates
because
the
most
recent
data
is
not
incorporated
one
of
the
key
issues
in
a
personalization
architecture
is
how
to
combine
and
manage
online
and
offline
computation
in
a
seamless
manner
nearline
computation
is
an
intermediate
compromise
between
these
two
modes
in
which
we
can
perform
online-like
computations
but
do
not
require
them
to
be
served
in
real-time
model
training
is
another
form
of
computation
that
uses
existing
data
to
generate
a
model
that
will
later
be
used
during
the
actual
computation
of
results
another
part
of
the
architecture
describes
how
the
different
kinds
of
events
and
data
need
to
be
handled
by
the
event
and
data
distribution
system
a
related
issue
is
how
to
combine
the
different
signals
and
models
that
are
needed
across
the
offline
nearline
and
online
regimes
finally
we
also
need
to
figure
out
how
to
combine
intermediate
recommendation
results
in
a
way
that
makes
sense
for
the
user
the
rest
of
this
post
will
detail
these
components
of
this
architecture
as
well
as
their
interactions
in
order
to
do
so
we
will
break
the
general
diagram
into
different
sub-systems
and
we
will
go
into
the
details
of
each
of
them
as
you
read
on
it
is
worth
keeping
in
mind
that
our
whole
infrastructure
runs
across
the
public
amazon
web
services
cloud
as
mentioned
above
our
algorithmic
results
can
be
computed
either
online
in
real-time
offline
in
batch
or
nearline
in
between
each
approach
has
its
advantages
and
disadvantages
which
need
to
be
taken
into
account
for
each
use
case
online
computation
can
respond
quickly
to
events
and
use
the
most
recent
data
an
example
is
to
assemble
a
gallery
of
action
movies
sorted
for
the
member
using
the
current
context
online
components
are
subject
to
an
availability
and
response
time
service
level
agreements
sla
that
specifies
the
maximum
latency
of
the
process
in
responding
to
requests
from
client
applications
while
our
member
is
waiting
for
recommendations
to
appear
this
can
make
it
harder
to
fit
complex
and
computationally
costly
algorithms
in
this
approach
also
a
purely
online
computation
may
fail
to
meet
its
sla
in
some
circumstances
so
it
is
always
important
to
think
of
a
fast
fallback
mechanism
such
as
reverting
to
a
precomputed
result
computing
online
also
means
that
the
various
data
sources
involved
also
need
to
be
available
online
which
can
require
additional
infrastructure
on
the
other
end
of
the
spectrum
offline
computation
allows
for
more
choices
in
algorithmic
approach
such
as
complex
algorithms
and
less
limitations
on
the
amount
of
data
that
is
used
a
trivial
example
might
be
to
periodically
aggregate
statistics
from
millions
of
movie
play
events
to
compile
baseline
popularity
metrics
for
recommendations
offline
systems
also
have
simpler
engineering
requirements
for
example
relaxed
response
time
slas
imposed
by
clients
can
be
easily
met
new
algorithms
can
be
deployed
in
production
without
the
need
to
put
too
much
effort
into
performance
tuning
this
flexibility
supports
agile
innovation
at
netflix
we
take
advantage
of
this
to
support
rapid
experimentation:
if
a
new
experimental
algorithm
is
slower
to
execute
we
can
choose
to
simply
deploy
more
amazon
ec2
instances
to
achieve
the
throughput
required
to
run
the
experiment
instead
of
spending
valuable
engineering
time
optimizing
performance
for
an
algorithm
that
may
prove
to
be
of
little
business
value
however
because
offline
processing
does
not
have
strong
latency
requirements
it
will
not
react
quickly
to
changes
in
context
or
new
data
ultimately
this
can
lead
to
staleness
that
may
degrade
the
member
experience
offline
computation
also
requires
having
infrastructure
for
storing
computing
and
accessing
large
sets
of
precomputed
results
nearline
computation
can
be
seen
as
a
compromise
between
the
two
previous
modes
in
this
case
computation
is
performed
exactly
like
in
the
online
case
however
we
remove
the
requirement
to
serve
results
as
soon
as
they
are
computed
and
can
instead
store
them
allowing
it
to
be
asynchronous
the
nearline
computation
is
done
in
response
to
user
events
so
that
the
system
can
be
more
responsive
between
requests
this
opens
the
door
for
potentially
more
complex
processing
to
be
done
per
event
an
example
is
to
update
recommendations
to
reflect
that
a
movie
has
been
watched
immediately
after
a
member
begins
to
watch
it
results
can
be
stored
in
an
intermediate
caching
or
storage
back-end
nearline
computation
is
also
a
natural
setting
for
applying
incremental
learning
algorithms
in
any
case
the
choice
of
onlinenearlineoffline
processing
is
not
an
eitheror
question
all
approaches
can
and
should
be
combined
there
are
many
ways
to
combine
them
we
already
mentioned
the
idea
of
using
offline
computation
as
a
fallback
another
option
is
to
precompute
part
of
a
result
with
an
offline
process
and
leave
the
less
costly
or
more
context-sensitive
parts
of
the
algorithms
for
online
computation
even
the
modeling
part
can
be
done
in
a
hybrid
offlineonline
manner
this
is
not
a
natural
fit
for
traditional
supervised
classification
applications
where
the
classifier
has
to
be
trained
in
batch
from
labeled
data
and
will
only
be
applied
online
to
classify
new
inputs
however
approaches
such
as
matrix
factorization
are
a
more
natural
fit
for
hybrid
onlineoffline
modeling:
some
factors
can
be
precomputed
offline
while
others
can
be
updated
in
real-time
to
create
a
more
fresh
result
other
unsupervised
approaches
such
as
clustering
also
allow
for
offline
computation
of
the
cluster
centers
and
online
assignment
of
clusters
these
examples
point
to
the
possibility
of
separating
our
model
training
into
a
large-scale
and
potentially
complex
global
model
training
on
the
one
hand
and
a
lighter
user-specific
model
training
or
updating
phase
that
can
be
performed
online
much
of
the
computation
we
need
to
do
when
running
personalization
machine
learning
algorithms
can
be
done
offline
this
means
that
the
jobs
can
be
scheduled
to
be
executed
periodically
and
their
execution
does
not
need
to
be
synchronous
with
the
request
or
presentation
of
the
results
there
are
two
main
kinds
of
tasks
that
fall
in
this
category:
model
training
and
batch
computation
of
intermediate
or
final
results
in
the
model
training
jobs
we
collect
relevant
existing
data
and
apply
a
machine
learning
algorithm
produces
a
set
of
model
parameters
which
we
will
henceforth
refer
to
as
the
model
this
model
will
usually
be
encoded
and
stored
in
a
file
for
later
consumption
although
most
of
the
models
are
trained
offline
in
batch
mode
we
also
have
some
online
learning
techniques
where
incremental
training
is
indeed
performed
online
batch
computation
of
results
is
the
offline
computation
process
defined
above
in
which
we
use
existing
models
and
corresponding
input
data
to
compute
results
that
will
be
used
at
a
later
time
either
for
subsequent
online
processing
or
direct
presentation
to
the
user
both
of
these
tasks
need
refined
data
to
process
which
usually
is
generated
by
running
a
database
query
since
these
queries
run
over
large
amounts
of
data
it
can
be
beneficial
to
run
them
in
a
distributed
fashion
which
makes
them
very
good
candidates
for
running
on
hadoop
via
either
hive
or
pig
jobs
once
the
queries
have
completed
we
need
a
mechanism
for
publishing
the
resulting
data
we
have
several
requirements
for
that
mechanism:
first
it
should
notify
subscribers
when
the
result
of
a
query
is
ready
second
it
should
support
different
repositories
not
only
hdfs
but
also
s3
or
cassandra
for
instance
finally
it
should
transparently
handle
errors
allow
for
monitoring
and
alerting
at
netflix
we
use
an
internal
tool
named
hermes
that
provides
all
of
these
capabilities
and
integrates
them
into
a
coherent
publish-subscribe
framework
it
allows
data
to
be
delivered
to
subscribers
in
near
real-time
in
some
sense
it
covers
some
of
the
same
use
cases
as
apache
kafka
but
it
is
not
a
messageevent
queue
system
regardless
of
whether
we
are
doing
an
online
or
offline
computation
we
need
to
think
about
how
an
algorithm
will
handle
three
kinds
of
inputs:
models
data
and
signals
models
are
usually
small
files
of
parameters
that
have
been
previously
trained
offline
data
is
previously
processed
information
that
has
been
stored
in
some
sort
of
database
such
as
movie
metadata
or
popularity
we
use
the
term
signals
to
refer
to
fresh
information
we
input
to
algorithms
this
data
is
obtained
from
live
services
and
can
be
made
of
user-related
information
such
as
what
the
member
has
watched
recently
or
context
data
such
as
session
device
date
or
time
our
goal
is
to
turn
member
interaction
data
into
insights
that
can
be
used
to
improve
the
member’s
experience
for
that
reason
we
would
like
the
various
netflix
user
interface
applications
smart
tvs
tablets
game
consoles
etc
to
not
only
deliver
a
delightful
user
experience
but
also
collect
as
many
user
events
as
possible
these
actions
can
be
related
to
clicks
browsing
viewing
or
even
the
content
of
the
viewport
at
any
time
events
can
then
be
aggregated
to
provide
base
data
for
our
algorithms
here
we
try
to
make
a
distinction
between
data
and
events
although
the
boundary
is
certainly
blurry
we
think
of
events
as
small
units
of
time-sensitive
information
that
need
to
be
processed
with
the
least
amount
of
latency
possible
these
events
are
routed
to
trigger
a
subsequent
action
or
process
such
as
updating
a
nearline
result
set
on
the
other
hand
we
think
of
data
as
more
dense
information
units
that
might
need
to
be
processed
and
stored
for
later
use
here
the
latency
is
not
as
important
as
the
information
quality
and
quantity
of
course
there
are
user
events
that
can
be
treated
as
both
events
and
data
and
therefore
sent
to
both
flows
at
netflix
our
near-real-time
event
flow
is
managed
through
an
internal
framework
called
manhattan
manhattan
is
a
distributed
computation
system
that
is
central
to
our
algorithmic
architecture
for
recommendation
it
is
somewhat
similar
to
twitter’s
storm
but
it
addresses
different
concerns
and
responds
to
a
different
set
of
internal
requirements
the
data
flow
is
managed
mostly
through
logging
through
chukwa
to
hadoop
for
the
initial
steps
of
the
process
later
we
use
hermes
as
our
publish-subscribe
mechanism
the
goal
of
our
machine
learning
approach
is
to
come
up
with
personalized
recommendations
these
recommendation
results
can
be
serviced
directly
from
lists
that
we
have
previously
computed
or
they
can
be
generated
on
the
fly
by
online
algorithms
of
course
we
can
think
of
using
a
combination
of
both
where
the
bulk
of
the
recommendations
are
computed
offline
and
we
add
some
freshness
by
post-processing
the
lists
with
online
algorithms
that
use
real-time
signals
at
netflix
we
store
offline
and
intermediate
results
in
various
repositories
to
be
later
consumed
at
request
time:
the
primary
data
stores
we
use
are
cassandra
evcache
and
mysql
each
solution
has
advantages
and
disadvantages
over
the
others
mysql
allows
for
storage
of
structured
relational
data
that
might
be
required
for
some
future
process
through
general-purpose
querying
however
the
generality
comes
at
the
cost
of
scalability
issues
in
distributed
environments
cassandra
and
evcache
both
offer
the
advantages
of
key-value
stores
cassandra
is
a
well-known
and
standard
solution
when
in
need
of
a
distributed
and
scalable
no-sql
store
cassandra
works
well
in
some
situations
however
in
cases
where
we
need
intensive
and
constant
write
operations
we
find
evcache
to
be
a
better
fit
the
key
issue
however
is
not
so
much
where
to
store
them
as
to
how
to
handle
the
requirements
in
a
way
that
conflicting
goals
such
as
query
complexity
readwrite
latency
and
transactional
consistency
meet
at
an
optimal
point
for
each
use
case
in
previous
posts
we
have
highlighted
the
importance
of
data
models
and
user
interfaces
for
creating
a
world-class
recommendation
system
when
building
such
a
system
it
is
critical
to
also
think
of
the
software
architecture
in
which
it
will
be
deployed
we
want
the
ability
to
use
sophisticated
machine
learning
algorithms
that
can
grow
to
arbitrary
complexity
and
can
deal
with
huge
amounts
of
data
we
also
want
an
architecture
that
allows
for
flexible
and
agile
innovation
where
new
approaches
can
be
developed
and
plugged-in
easily
plus
we
want
our
recommendation
results
to
be
fresh
and
respond
quickly
to
new
data
and
user
actions
finding
the
sweet
spot
between
these
desires
is
not
trivial:
it
requires
a
thoughtful
analysis
of
requirements
careful
selection
of
technologies
and
a
strategic
decomposition
of
recommendation
algorithms
to
achieve
the
best
outcomes
for
our
members
we
are
always
looking
for
great
engineers
to
join
our
team
if
you
think
you
can
help
us
be
sure
to
look
at
our
jobs
page
originally
published
at
techblognetflixcom
on
march
27
2013
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
learn
more
about
how
netflix
designs
builds
and
operates
our
systems
and
engineering
organizations
learn
about
netflix’s
world
class
engineering
efforts
company
culture
product
developments
and
more
""
machine
learning
ml
is
one
of
the
hottest
fields
in
data
science
as
soon
as
ml
entered
the
mainstream
through
amazon
netflix
and
facebook
people
have
been
giddy
about
what
they
can
learn
from
their
data
however
modern
machine
learning
ie
not
the
theoretical
statistical
learning
that
emerged
in
the
70s
is
very
much
an
evolving
field
and
despite
its
many
successes
we
are
still
learning
what
exactly
can
ml
do
for
data
practitioners
i
gave
a
talk
on
this
topic
earlier
this
fall
at
northwestern
university
and
i
wanted
to
share
these
cautionary
tales
with
a
wider
audience
machine
learning
is
a
field
of
computer
science
where
algorithms
improve
their
performance
at
a
certain
task
as
more
data
are
observedto
do
so
algorithms
select
a
hypothesis
that
best
explains
the
data
at
hand
with
the
hope
that
the
hypothesis
would
generalize
to
future
unseen
data
take
the
left
panel
in
the
figure
in
the
header
the
crosses
denote
the
observed
data
projected
in
a
two-dimensional
space
—
in
this
case
house
prices
and
their
corresponding
size
in
square
meters
the
blue
line
is
the
algorithm’s
best
hypothesis
to
explain
the
observed
data
it
states
there
is
a
linear
relationship
between
the
price
and
size
of
a
house
as
the
house’s
size
increases
so
does
its
price
in
linear
increments
now
using
this
hypothesis
i
can
predict
the
price
of
an
unseen
datapoint
based
on
its
size
as
the
dimensions
of
the
data
increase
the
hypotheses
that
explain
the
data
become
more
complexhowever
given
that
we
are
using
a
finite
sample
of
observations
to
learn
our
hypothesis
finding
an
adequate
hypothesis
that
generalizes
to
unseen
data
is
nontrivial
there
are
three
major
pitfalls
one
can
fall
into
that
will
prevent
you
from
having
a
generalizable
model
and
hence
the
conclusions
of
your
hypothesis
will
be
in
doubt
occam’s
razor
is
a
principle
attributed
to
william
of
occam
a
14th
century
philosopher
occam’s
razor
advocates
for
choosing
the
simplest
hypothesis
that
explains
your
data
yet
no
simpler
while
this
notion
is
simple
and
elegant
it
is
often
misunderstood
to
mean
that
we
must
select
the
simplest
hypothesis
possible
regardless
of
performance
in
their
2008
paper
in
nature
johan
nyberg
and
colleagues
used
a
4-level
artificial
neural
network
to
predict
seasonal
hurricane
counts
using
two
or
three
environmental
variables
the
authors
reported
stellar
accuracy
in
predicting
seasonal
north
atlantic
hurricane
counts
however
their
model
violates
occam’s
razor
and
most
certainly
doesn’t
generalize
to
unseen
data
the
razor
was
violated
when
the
hypothesis
or
model
selected
to
describe
the
relationship
between
environmental
data
and
seasonal
hurricane
counts
was
generated
using
a
four-layer
neural
network
a
four-layer
neural
network
can
model
virtually
any
function
no
matter
how
complex
and
could
fit
a
small
dataset
very
well
but
fail
to
generalize
to
unseen
data
the
rightmost
panel
in
the
top
figure
shows
such
incident
the
hypothesis
selected
by
the
algorithm
the
blue
curve
to
explain
the
data
is
so
complex
that
it
fits
through
every
single
data
point
that
is:
for
any
given
house
size
in
the
training
data
i
can
give
you
with
pinpoint
accuracy
the
price
it
would
sell
for
it
doesn’t
take
much
to
observe
that
even
a
human
couldn’t
be
that
accurate
we
could
give
you
a
very
close
estimate
of
the
price
but
to
predict
the
selling
price
of
a
house
within
a
few
dollars
""
every
single
time
is
impossible
the
pitfall
of
selecting
too
complex
a
hypothesis
is
known
as
overfitting
think
of
overfitting
as
memorizing
as
opposed
to
learning
if
you
are
a
child
and
you
are
memorizing
how
to
add
numbers
you
may
memorize
the
sums
of
any
pair
of
integers
between
0
and
10
however
when
asked
to
calculate
11
""
12
you
will
be
unable
to
because
you
have
never
seen
11
or
12
and
therefore
couldn’t
memorize
their
sum
that’s
what
happens
to
an
overfitted
model
it
gets
too
lazy
to
learn
the
general
principle
that
explains
the
data
and
instead
memorizes
the
data
data
leakage
occurs
when
the
data
you
are
using
to
learn
a
hypothesis
happens
to
have
the
information
you
are
trying
to
predict
the
most
basic
form
of
data
leakage
would
be
to
use
the
same
data
that
we
want
to
predict
as
input
to
our
model
eg
use
the
price
of
a
house
to
predict
the
price
of
the
same
house
however
most
often
data
leakage
occurs
subtly
and
inadvertently
for
example
one
may
wish
to
learn
for
anomalies
as
opposed
to
raw
data
that
is
a
deviations
from
a
long-term
mean
however
many
fail
to
remove
the
test
data
before
computing
the
anomalies
and
hence
the
anomalies
carry
some
information
about
the
data
you
want
to
predict
since
they
influenced
the
mean
and
standard
deviation
before
being
removed
the
are
several
ways
to
avoid
data
leakage
as
outlined
by
claudia
perlich
in
her
great
paper
on
the
subject
however
there
is
no
silver
bullet
—
sometimes
you
may
inherit
a
corrupt
dataset
without
even
realizing
it
one
way
to
spot
data
leakage
is
if
you
are
doing
very
poorly
on
unseen
independent
data
for
example
say
you
got
a
dataset
from
someone
that
spanned
2000-2010
but
you
started
collecting
you
own
data
from
2011
onward
if
your
model’s
performance
is
poor
on
the
newly
collected
data
it
may
be
a
sign
of
data
leakage
you
must
resist
the
urge
to
retrain
the
model
with
both
the
potentially
corrupt
and
new
data
instated
either
try
to
identify
the
causes
of
poor
performance
on
the
new
data
or
better
yet
independently
reconstruct
the
entire
dataset
as
a
rule
of
thumb
your
best
defense
is
to
always
be
mindful
of
the
possibility
of
data
leakage
in
any
dataset
sampling
bias
is
the
case
when
you
shortchange
your
model
by
training
it
on
a
biased
or
non-random
dataset
which
results
in
a
poorly
generalizable
hypothesis
in
the
case
of
housing
prices
sampling
bias
occurs
if
for
some
reason
all
the
house
pricessizes
you
collected
were
of
huge
mansions
however
when
it
was
time
to
test
your
model
and
the
first
price
you
needed
to
predict
was
that
of
a
2-bedroom
apartment
you
couldn’t
predict
it
sampling
bias
happens
very
frequently
mainly
because
as
humans
we
are
notorious
for
being
biased
nonrandom
samplers
one
of
the
most
common
examples
of
this
bias
happens
in
startups
and
investing
if
you
attend
any
business
school
course
they
will
use
all
these
case
studies
of
how
to
build
a
successful
company
such
case
studies
actually
depict
the
anomalies
and
not
the
norm
as
most
companies
fail
—
for
every
apple
that
became
a
success
there
were
1000
other
startups
that
died
trying
so
to
build
an
automated
data-driven
investment
strategy
you
would
need
samples
from
both
successful
and
unsuccessful
companies
the
figure
above
figure
13
is
a
concrete
example
of
sampling
bias
say
you
want
to
predict
whether
a
tornado
is
going
to
originate
at
certain
location
based
on
two
environmental
conditions:
wind
shear
and
convective
available
potential
energy
cape
we
don’t
have
to
worry
about
what
these
variables
actually
mean
but
figure
13
shows
the
wind
shear
and
cape
associated
with
242
tornado
cases
we
can
fit
a
model
to
these
data
but
it
will
certainly
not
generalize
because
we
failed
to
include
shear
and
cape
values
when
tornados
did
not
occur
in
order
for
our
model
to
separate
between
positive
tornados
and
negative
no
tornados
events
we
must
train
it
using
both
populations
there
you
have
it
being
mindful
of
these
limitations
does
not
guarantee
that
your
ml
algorithm
will
solve
all
your
problems
but
it
certainly
reduces
the
risk
of
being
disappointed
when
your
model
doesn’t
generalize
to
unseen
data
now
go
on
young
jedi:
train
your
model
you
must!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
@nomadic_mind
sometimes
the
difference
between
success
and
failure
is
the
same
as
between
=
and
==
living
is
in
the
details
""
at
datafiniti
we
have
a
strong
need
for
converting
unstructured
web
content
into
structured
data
for
example
we’d
like
to
find
a
page
like:
and
do
the
following:
both
of
these
are
hard
things
for
a
computer
to
do
in
an
automated
manner
while
it’s
easy
for
you
or
me
to
realize
that
the
above
web
page
is
selling
some
jeans
a
computer
would
have
a
hard
time
making
the
distinction
from
the
above
page
from
either
of
the
following
web
pages:
or
both
of
these
pages
share
many
similarities
to
the
actual
product
page
but
also
have
many
key
differences
the
real
challenge
though
is
that
if
we
look
at
the
entire
set
of
possible
web
pages
those
similarities
and
differences
become
somewhat
blurred
which
means
hard
and
fast
rules
for
classifications
will
fail
often
in
fact
we
can’t
even
rely
on
just
looking
at
the
underlying
html
since
there
are
huge
variations
in
how
product
pages
are
laid
out
in
html
while
we
could
try
and
develop
a
complicated
set
of
rules
to
account
for
all
the
conditions
that
perfectly
identify
a
product
page
doing
so
would
be
extremely
time
consuming
and
frankly
incredibly
boring
work
instead
we
can
try
using
a
classical
technique
out
of
the
artificial
intelligence
handbook:
neural
networks
here’s
a
quick
primer
on
neural
networks
let’s
say
we
want
to
know
whether
any
particular
mushroom
is
poisonous
or
not
we’re
not
entirely
sure
what
determines
this
but
we
do
have
a
record
of
mushrooms
with
their
diameters
and
heights
along
with
which
of
these
mushrooms
were
poisonous
to
eat
for
sure
in
order
to
see
if
we
could
use
diameter
and
heights
to
determine
poisonous-ness
we
could
set
up
the
following
equation:
a
*
diameter
""
b
*
height
=
0
or
1
for
not-poisonous
""
poisonous
we
would
then
try
various
combinations
of
a
and
b
for
all
possible
diameters
and
heights
until
we
found
a
combination
that
correctly
determined
poisonous-ness
for
as
many
mushrooms
as
possible
neural
networks
provide
a
structure
for
using
the
output
of
one
set
of
input
data
to
adjust
a
and
b
to
the
most
likely
best
values
for
the
next
set
of
input
data
by
constantly
adjusting
a
and
b
this
way
we
can
quickly
get
to
the
best
possible
values
for
them
in
order
to
introduce
more
complex
relationships
in
our
data
we
can
introduce
hidden
layers
in
this
model
which
would
end
up
looking
something
like:
for
a
more
detailed
explanation
of
neural
networks
you
can
check
out
the
following
links:
in
our
product
page
classifier
algorithm
we
setup
a
neural
network
with
1
input
layer
with
27
nodes
1
hidden
layer
with
25
nodes
and
1
output
layer
with
3
output
nodes
our
input
layer
modeled
several
features
including:
our
output
layer
had
the
following:
our
algorithm
for
the
neural
network
took
the
following
steps:
the
ultimate
output
is
two
sets
of
input
layers
t1
and
t2
that
we
can
use
in
a
matrix
equation
to
predict
page
type
for
any
given
web
page
this
works
like
so:
so
how
did
we
do?
in
order
to
determine
how
successful
we
were
in
our
predictions
we
need
to
determine
how
to
measure
success
in
general
we
want
to
measure
how
many
true
positive
tp
results
as
compared
to
false
positives
fp
and
false
negatives
fn
conventional
measurements
for
these
are:
our
implementation
had
the
following
results:
these
scores
are
just
over
our
training
set
of
course
the
actual
scores
on
real-life
data
may
be
a
bit
lower
but
not
by
much
this
is
pretty
good!
we
should
have
an
algorithm
on
our
hands
that
can
accurately
classify
product
pages
about
90%
of
the
time
of
course
identifying
product
pages
isn’t
enough
we
also
want
to
pull
out
the
actual
structured
data!
in
particular
we’re
interested
in
product
name
price
and
any
unique
identifiers
eg
upc
ean
""
isbn
this
information
would
help
us
fill
out
our
product
search
we
don’t
actually
use
neural
networks
for
doing
this
neural
networks
are
better-suited
toward
classification
problems
and
extracting
data
from
a
web
page
is
a
different
type
of
problem
instead
we
use
a
variety
of
heuristics
specific
to
each
attribute
we’re
trying
to
extract
for
example
for
product
name
we
look
at
the
<h1>
and
<h2>
tags
and
use
a
few
metrics
to
determine
the
best
choice
we’ve
been
able
to
achieve
around
a
80%
accuracy
here
we
may
go
into
the
actual
metrics
and
methodology
for
developing
them
in
a
separate
post!
we
feel
pretty
good
about
our
ability
to
classify
and
extract
product
data
the
extraction
part
could
be
better
but
it’s
steadily
being
improved
in
the
meantime
we’re
also
working
on
classifying
other
types
of
pages
such
as
business
data
company
team
pages
event
data
and
moreas
we
roll-out
these
classifiers
and
data
extractors
we’re
including
each
one
in
our
crawl
of
the
entire
internet
this
means
that
we
can
scan
the
entire
internet
and
pull
out
any
available
data
that
exists
out
there
exciting
stuff!
you
can
connect
with
us
and
learn
more
about
our
business
people
product
and
property
apis
and
datasets
by
selecting
one
of
the
options
below
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
instant
access
to
web
data
building
the
world’s
largest
database
of
web
data
—
follow
our
journey
""
sune
lehmann
is
an
associate
professor
at
dtu
informatics
technical
university
of
denmark
in
the
past
he
has
worked
as
a
postdoctoral
fellow
at
institute
for
quantitative
social
science
at
harvard
university
and
the
college
of
computer
and
information
science
at
northeasthern
university
before
that
he
was
at
laszlo
barabási’s
center
for
complex
network
research
at
northeastern
university
and
the
center
for
cancer
systems
biology
at
the
dana
farber
cancer
institute
i
wouldn’t
call
him
stupid
he
is
okay
well
he
is
actually
pretty
great
forget
that
he
is
freaking
fantastic!
we
should
get
him
over
for
one
of
our
events!
and
so
we
did
sune
spoke
at
the
2nd
#projectwaalhalla
this
time
let’s
begin
at
the
beginning
before
we
dive
in
deeper
your
main
research
project
has
to
do
with
measuring
real
social
networks
with
high
resolution
i
know
for
a
fact
you
don’t
mean
3d
printed
social
networks
but
what
are
you
aiming
for
and
how
are
you
going
to
get
there?
my
humble
research
goal
is
to
reinvent
social
sciences
in
the
age
of
big
data
my
background
is
in
mathematical
analysis
of
large
networks
but
over
the
past
10
years
i’ve
slowly
grown
more
and
more
interested
in
understanding
social
systems
as
a
scientist
i
was
blown
away
by
the
promise
of
all
of
the
digital
traces
of
human
behavior
collected
as
a
consequence
of
cheap
hard
drives
and
databases
everywhere
but
in
spite
of
the
promise
of
big
data
the
results
so
far
have
been
less
exciting
than
i
had
hoped
for
all
the
hype
deep
new
scientific
insights
from
big
data
are
far
and
few
between
a
central
hypothesis
in
my
work
is
that
in
order
to
advance
our
quantitative
understanding
of
social
interaction
we
cannot
get
by
with
noisy
incomplete
big
data:
we
need
good
data
let
me
explain
why
and
use
my
own
field
as
an
example
let’s
say
you
have
a
massive
cell
phone
data
set
from
a
telco
that
provides
service
to
30%
or
the
population
of
a
large
country
of
66
million
people
that’s
something
like
20
million
people
and
easily
terabytes
of
monthly
data
so
a
massive
dataset
but
when
you
start
thinking
about
the
network
you
run
into
problems
the
standard
approach
is
to
simply
look
at
the
network
between
the
individuals
in
your
sample
assuming
that
people
are
randomly
sampled
and
links
are
randomly
distributed
you
realize
that
30%
of
the
population
corresponds
to
only
9%
of
the
links
is
9%
of
cell
phone
calls
enough
to
understand
how
the
network
works?
with
only
one
in
ten
links
remaining
in
the
dataset
the
social
structure
almost
completely
erased
and
it
gets
worse
telecommunication
is
only
one
small
""
biased
aspect
of
human
communication
human
interactions
may
also
unfold
face-to-face
via
text
message
email
facebook
skype
etc
and
these
streams
are
collected
in
silos
where
we
cannot
generally
identify
individualsentities
across
datasets
so
if
we
think
about
all
these
ways
we
can
communicate
access
to
only
one
in
ten
of
my
cell
phone
contacts
is
very
likely
insufficient
for
making
valid
inferences
and
the
worst
part
is
that
we
can’t
know
without
access
to
the
full
data
set
we
can’t
even
tell
what
we
can
and
can’t
tell
from
a
sample
so
when
i
started
out
as
an
assistant
professor
i
decided
to
change
the
course
of
my
career
and
move
from
sitting
comfortably
in
front
of
my
computer
as
a
computationaltheoretical
scientist
to
becoming
an
experimenter
to
try
and
attack
this
problem
head
onnow
a
few
of
years
later
we
have
put
together
a
dataset
of
human
social
interactions
that
is
unparalleled
in
terms
of
quality
and
size
we
recording
social
interactions
within
more
than
1000
students
at
my
university
using
top-of-the-line
cell
phones
as
censors
we
can
capture
detailed
interaction
patterns
such
as
face-to-face
via
bluetooth
social
network
data
eg
facebook
and
twitter
via
apps
telecommunication
data
from
call
logs
and
geolocation
via
gps
""
wifi
we
like
to
call
this
type
of
data
‘deep
data’:
a
densely
connected
group
of
participants
all
the
links
observations
across
many
communication
channels
high
frequency
observations
minute-by-minute
scale
but
with
long
observation
windows
years
of
collection
and
with
behavioral
data
supplemented
by
classic
questionnaires
as
well
as
the
possibility
of
running
intervention
experiments
but
my
expertise
and
ultimate
interest
is
not
in
building
a
deep
data
collection
platform
although
that
has
been
a
lot
of
fun
i
want
to
get
back
to
the
questions
that
motivated
the
enthusiasm
for
computational
social
science
in
the
first
place
reinventing
social
sciences
is
what
it’s
all
about
what
can
we
learn
from
just
one
channel?
now
that
we
know
about
all
the
communication
channels
we
can
begin
to
understand
what
kind
of
things
one
may
learn
from
a
single
channel
let’s
get
quantitative
about
the
usefulness
of
eg
large
cell
phone
data
sets
or
facebook
when
that’s
the
only
data
available
my
heart
is
still
with
the
network
science
in
some
ways
this
whole
project
is
designed
to
build
a
system
that
will
really
take
us
places
in
terms
of
modeling
human
social
networks
lots
of
network
science
is
still
about
unweighted
undirected
static
networks
we
are
already
using
this
dataset
to
create
better
models
for
dynamic
multiplex
networks
understanding
spreading
processes
influence
behavior
disease
etc
is
a
central
goal
if
we
look
a
bit
forward
in
time
we
have
an
system
where
n
is
big
enough
to
perform
intervention
experiments
with
randomized
controls
etc
we’re
still
far
from
implementing
this
goal
but
we’re
working
on
finding
the
right
questions
—
and
working
closely
with
social
scientists
to
get
our
protocols
for
these
questions
just
right
what
a
coincidence
we
are
all
about
modeling
behavior
and
learning
across
channels
and
with
contagionapi
prominently
on
our
product
roadmap
we
want
to
start
dabbling
with
spreading
processes
as
well
in
the
near
future
what
would
you
say
were
major
challenges
the
last
years
in
modeling
behavior
and
what
do
see
as
biggest
challenges
""
opportunities
for
the
future?
there
are
many
challenges
although
we’ve
made
amazing
progress
in
network
science
for
example
it’s
still
a
fact
that
our
fundamental
understanding
of
dynamicmulti-channel
networks
is
still
in
its
infancy
there
aren’t
a
lot
of
easily
interpretable
models
that
really
explain
the
underlying
networks
so
that’s
an
area
with
lots
of
challenges
and
corresponding
opportunities
and
when
we
want
to
figure
out
questions
about
things
taking
place
on
networks
we
run
into
all
kinds
of
problems
about
how
to
do
statistics
right
brilliant
statisticians
have
shown
that
homophily
and
contagion
are
generically
confounded
in
observational
social
network
studies
on
that
front
guys
like
sinan
aral
are
doing
really
exciting
work
using
interventions
to
get
at
some
of
the
issues
but
there
is
still
lots
to
do
in
that
area
finally
privacy
is
a
big
issue
we’re
working
closely
with
collaborators
at
the
mit
medialab
to
develop
new
responsible
solutions
—
and
we’ve
already
gotten
far
on
that
topic
but
in
terms
of
data
sharing
that
respects
the
privacy
of
study
participants
there
is
still
a
long
way
to
go
but
since
studies
of
digital
traces
of
human
behavior
will
not
be
going
away
anytime
soon
we
have
to
make
progress
in
this
area
and
oh
yeah
why
does
this
all
matter?
and
should
we
be
concerned
by
these
things?
i
think
there
are
many
reasons
to
be
concerned
and
excited
the
more
we
learn
about
how
systems
work
the
more
we
are
able
to
influence
them
to
control
them
that
is
also
true
for
systems
of
humans
if
we
think
about
spreading
of
disease
it’d
be
great
to
know
how
to
slow
down
or
stop
the
spread
of
sars
or
similar
contagious
viruses
or
as
a
society
we
may
be
able
to
increase
spread
of
things
we
support
such
as
tolerance
good
exercise
habits
etc
""
and
similarly
we
can
use
an
understanding
influence
in
social
systems
to
inhibit
negative
behavior
such
as
intolerance
smoking
etc
and
all
this
ties
into
another
good
reason
to
be
concerned
companies
like
google
facebook
apple
or
governmental
agencies
like
nsa
are
committing
serious
resources
to
research
in
this
area
it’s
not
a
coincidence
that
both
google
and
facebook
are
developing
their
own
cell-phones
but
none
of
these
walled-off
players
are
sharing
their
results
they’re
simply
applying
them
to
the
public
in
my
opinion
that’s
one
of
the
key
problems
of
the
current
state
of
affairs
the
imbalance
of
information
we
hand
over
our
personal
data
to
powerful
corporations
but
have
nearly
zero
insight
into
a
what
they
know
about
us
and
b
what
they’re
doing
with
all
the
stuff
they
know
about
us
by
doing
research
that
is
open
collaborative
explicit
about
privacy
and
public
i
hope
we
can
act
as
a
counter-point
and
work
to
diminish
the
information-gap
okay
great
but
should
companies
be
interested
in
the
stuff
you
are
doing?
and
if
so
why?
i
think
so!
one
of
the
exciting
things
about
this
area
is
that
basic
research
is
very
close
to
applied
research
insight
into
the
mechanisms
that
drive
human
nature
is
indeed
valuable
for
companies
i
presume
that’s
why
science
rockstars
exists
for
example
[note
from
the
editor:
not
stupid
at
all]
we
already
know
that
human
behavior
can
be
influenced
significantly
with
nudging
that
certain
kinds
of
collective
behaviors
influence
our
opinions
and
purchasing
behaviors
the
more
we
uncover
about
the
details
of
these
mechanism
the
more
precise
and
effective
we
can
be
about
influencing
others
let’s
discuss
the
ethics
of
this
another
time
but
it’s
not
just
marketing
if
used
for
good
this
is
the
science
of
what
makes
people
happy
so
inside
organizations
work
like
this
could
be
used
to
re-think
organizational
structures
incentives
etc
to
make
employees
happier
""
more
fulfilled
or
if
we
think
about
organizations
as
organisms
having
access
to
realtime
information
about
employees
can
be
thought
of
as
a
nervous
system
for
the
company
allowing
for
faster
reaction
times
when
crises
arise
identification
of
pain
points
etc
finally
for
the
medical
field
we
know
that
genes
only
explain
part
of
what
makes
us
sick
being
able
to
quantify
and
analyze
behavior
means
knowing
more
about
the
environment
the
nurture
part
of
nurture
vs
nature
in
that
sense
detailed
data
on
how
we
behave
could
also
help
us
understand
how
to
be
healthier
originally
published
at
wwwsciencerockstarscom
on
november
2
2013
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
let’s
fix
the
future:
scientific
advisor
@jadatascience
a
blog
series
about
the
discipline
of
business
experimentation
how
to
run
and
learn
from
experiments
in
different
contexts
is
a
complex
matter
but
lays
at
the
heart
of
innovation
""
one
way
that
we
deal
with
this
volume
of
data
is
to
cluster
up
all
the
similar
messages
together
to
find
patterns
in
behavior
of
senders
for
example
if
someone
is
contacting
thousands
of
different
organizers
with
similar
messages
that
behavior
is
suspect
and
will
be
examined
the
big
question
is
how
can
we
compare
every
single
message
we
see
with
every
other
message
efficiently
and
accurately?
in
this
article
we’ll
be
exploring
a
technique
known
as
multi-index
locality
sensitive
hashing
to
perform
the
the
comparison
efficiently
we
pre-process
the
data
with
a
series
of
steps:
let’s
first
define
what
similar
messages
are
here
we
have
and
example
of
two
similar
messages
a
and
b:
to
our
human
eyes
of
course
they’re
similar
but
we
want
determine
this
similarity
quantitatively
the
solution
is
to
break
up
the
message
into
tokens
and
then
treat
each
message
as
a
bag
of
tokens
the
simplest
naive
way
to
do
tokenization
is
to
split
up
a
message
on
spacespunctuation
and
convert
each
character
to
lowercase
so
our
result
from
our
tokenization
of
the
above
messages
would
be:
i’ll
leave
as
an
exercise
to
the
reader
to
come
up
with
more
interesting
ways
to
do
tokenization
for
handling
contractions
plurals
foreign
languages
etc
to
calculate
the
similarity
between
these
two
bags
of
tokens
we’ll
use
an
estimation
known
as
the
jaccard
similarity
coefficient
this
is
defined
as
the
ratio
of
sizes
of
the
intersection
and
union
of
a
and
b
therefore
in
our
example:
we’ll
then
set
a
threshold
above
which
we
will
consider
two
messages
to
be
similar
so
then
when
given
a
set
of
m
messages
we
simply
compute
the
similarity
of
a
message
to
every
other
message
this
works
in
theory
but
in
practice
there
are
cases
where
this
metric
is
unreliable
eg
if
one
message
is
significantly
longer
than
the
other
not
to
mention
horribly
inefficient
on2
m2
where
n
is
the
number
of
tokens
per
message
we
need
do
things
smarter!
one
problem
with
doing
a
simple
jaccard
similarity
is
that
the
scale
of
the
value
changes
with
the
size
number
of
tokens
of
the
message
to
address
this
we
can
transform
our
tokens
with
a
method
known
as
minhash
here’s
a
psuedo-code
snippet:
the
interesting
property
of
the
minhash
transformation
is
that
it
leaves
us
with
a
constant
n
number
of
hashes
and
that
chosen
hashes
will
be
in
the
same
positions
in
the
vector
after
the
minhash
transformation
the
jaccard
similarity
can
be
approximated
by
an
element-wise
comparison
of
two
hash
vectors
implemented
as
pseudo-code
above
so
we
can
stop
here
but
we’re
having
so
much
fun
and
we
can
do
so
much
better
notice
when
we
do
comparison
we
have
to
to
on
integer
comparisons
and
if
we
have
m
messages
then
comparing
every
message
to
each
other
is
on
m2
integer
comparisons
this
is
still
not
acceptable
to
reduce
the
time
complexity
of
comparing
minhashes
to
each
other
we
can
do
better
with
a
technique
known
as
bit
sampling
the
main
idea
is
that
we
don’t
need
to
know
the
exact
value
of
each
hash
but
only
that
the
hashes
are
equal
at
their
respective
positions
in
each
hash
vector
with
this
insight
let’s
only
look
at
the
least
significant
bit
lsb
of
each
hash
value
more
pseudo-code:
when
comparing
two
messages
if
the
hashes
are
equal
in
the
same
position
in
the
minhash
vector
then
the
bits
in
the
equivalent
position
after
bit
sampling
should
be
also
equal
so
we
can
emulate
the
jaccard
similarity
of
two
minhashes
by
counting
the
equal
bits
in
the
two
bit
vectors
aka
the
hamming
distance
and
dividing
by
the
number
of
bits
of
course
two
different
hashes
will
have
the
same
lsb
50%
of
the
time
to
increase
our
efficacy
we
would
pick
a
large
n
initially
here
is
some
naive
and
inefficient
pseudo-code:
in
practice
more
efficient
implementations
of
the
bitsimilarity
function
can
calculate
in
near
o1
time
for
reasonable
sizes
of
n
bit
twiddling
hacks
this
means
that
when
comparing
m
messages
to
each
other
we’ve
reduced
the
time
complexity
to
om2
but
wait
there’s
more!
remember
how
i
said
we
have
a
lot
of
data?
om2
is
still
unreasonable
when
m
is
a
very
large
number
of
messages
so
we
need
to
try
to
reduce
the
number
of
comparisons
to
make
using
a
divide
and
conquer
strategy
lets
start
with
an
example
where
we
set
n=32
and
we
want
to
have
a
bitsimilarity
of
9:
in
the
worst
case
to
do
this
we
need
28
of
the
32
bits
to
be
equal
or
4
bits
unequal
we
will
refer
to
the
number
of
unequal
bits
as
the
radius
of
the
bit
vectors
ie
if
two
bit
vectors
are
within
a
certain
radius
of
bits
then
they
are
similar
the
unequal
bits
can
be
found
by
taking
the
bit-wise
xor
of
the
two
bit
vectors
for
example:
if
we
split
up
xor_mask
into
4
chunks
of
8
bits
then
at
least
one
chunk
will
have
exactly
zero
or
exactly
one
of
the
bit
differences
pigeonhole
principal
more
generally
if
we
split
xor_mask
of
size
n
into
k
chunks
with
an
expected
radius
r
then
at
least
one
chunk
is
guaranteed
to
have
floorr
""
k
or
less
bits
unequal
for
the
purpose
of
explanation
we
will
assume
that
we
have
chosen
all
the
parameters
such
that
floorr
""
k
=
1
now
you’re
wondering
how
this
piece
of
logic
help
us?
we
can
now
design
a
data
structure
lshtable
to
index
the
bit
vectors
to
reduce
the
number
of
bitsimilarity
comparisons
drastically
but
increase
memory
consumption
in
om
[fast
search
in
hamming
space
with
multi-index
hashing]
we
will
define
lshtable
with
some
pseudo-code:
basically
in
lshtable
initialization
we
create
k
hash
tables
for
each
k
chunks
during
add
of
a
bit
vector
we
split
the
bit
vector
into
k
chunks
for
each
of
these
chunks
we
add
the
original
bit
vector
into
the
associated
hash
table
under
the
index
chunk
upon
the
lookup
of
a
bit
vector
we
once
again
split
it
into
chunks
and
for
each
chunk
look
up
the
associated
hash
table
for
a
chunk
that’s
close
zero
or
one
bits
off
the
returned
list
is
a
set
of
candidate
bit
vectors
to
check
bitsimilarity
because
of
the
property
explained
in
the
previous
section
at
least
one
hash
table
will
contain
a
set
of
candidates
that
contains
a
similar
bit
vector
to
compare
every
m
message
to
every
other
message
we
first
insert
its
bit
vector
into
an
lshtable
an
ok
operation
k
is
constant
then
to
find
similar
messages
we
simply
do
a
lookup
from
the
lshtable
another
ok
operation
and
then
check
bitsimilarity
for
each
of
the
candidates
returned
the
number
of
candidates
to
check
is
usually
on
the
order
of
m
""
2^nk
if
at
all
therefore
the
time
complexity
to
compare
all
m
messages
to
each
other
is
om
*
m
""
2^nk
in
practice
n
and
k
are
empirically
chosen
such
that
2^nk
>>
m
so
the
final
time
complexity
is
om
—
remember
we
started
with
on
m2!
phew
what
a
ride
so
we’ve
detailed
how
to
find
similar
messages
in
a
very
large
set
of
messages
efficiently
by
using
multi-index
locality
sensitivity
hashing
we
can
reduce
the
time
complexity
of
from
quadratic
with
a
very
high
constant
to
near
linear
with
a
more
manageable
constant
i
should
also
mention
that
many
of
the
ancillary
pseudo-code
excerpts
used
here
describe
the
most
naive
implementation
of
each
method
and
are
for
instructive
purposes
only
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
we
help
bring
the
world
together
through
live
experiences
""
in
this
picture
pranav
mistry
is
using
color
marker
on
his
fingers
to
track
the
gesture
and
his
wearable
computer
perform
action
based
on
gestures
that
sounds
easy!
but
no
it’s
not
computer
need
to
understand
those
color
marker
first
for
that
it
needs
to
separate
marker
from
any
surroundings
segmentation
can
be
helpful
to
achieve
this
various
methods
are
available
for
segmentation
however
this
article
talks
about
robust
color
based
object
segmentation
create
binary
mask
that
separates
blue
t-shirt
from
rest
to
find
blue
t-shirt
in
given
image
i
used
opencv’s
inrange
method:
which
takes
color
or
greyscale
image
lower
""
higher
range
value
as
its
parameter
and
returns
binary
image
where
pixel
value
set
to
0
when
input
pixel
doesn’t
fall
in
specified
range
otherwise
pixel
value
set
to
1
with
the
help
of
this
function
and
after
determining
range
values
i
ended
up
with
this
mask
but
you
can
see
there
are
problems!
it’s
not
able
to
create
mask
for
complete
t-shirt
also
it
mask
eyes
which
aren’t
blue
this
is
happening
because
light
from
one
side
of
body
whitens
the
right
side
at
the
same
time
creates
shadow
in
left
region
thus
it
creates
different
shades
of
blue
and
results
into
partial
segmentation
normalization
of
color
plane
reduces
variation
in
light
by
averaging
pixel
values
thus
it
removes
highlighted
and
shadowed
region
and
make
image
flatten
following
image
is
free
from
highlights
""
shadows
and
it
is
divided
into
one
large
green
background
blue
t-shirt
and
skin
now
the
inrange
method
able
to
mask
only
t-shirt
following
function
converts
a
pixel
at
x
y
location
into
its
corresponding
normalized
rgb
pixel
let
rgb
are
pixel
values
then
normalized
pixel
gxy
is
calculated
asdivide
the
individual
color
component
by
sum
of
all
color
components
and
multiply
by
255
division
results
into
floating
point
number
in
range
of
00
to
10
and
as
this
is
8
bit
image
result
is
scaled
up
by
255
this
function
accepts
8
bit
rgb
image
matrix
of
size
800x600
and
returns
normalized
rgb
image
originally
published
at
akash0x53githubio
on
april
29
2013
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
python
आणि
बरच
काही
""
""
kaggle
announced
the
traveling
santa
problem
in
the
christmas
season
i
joined
in
excitedly
but
soon
realized
this
is
not
an
easy
problem
solving
this
problem
would
require
expertise
on
data
structures
and
some
good
familiarity
with
tsp
problems
and
its
many
heuristic
algorithms
i
had
neither
i
had
to
find
a
way
to
deal
with
this
problem
i
compenseted
my
lack
of
algorithmic
expertise
with
common
sense
logic
and
intuition
i
finished
65th
out
of
356
total
competitors
i
did
some
research
on
packaged
tsp
solvers
and
top
tsp
algorithms
i
found
concorde
but
i
could
not
get
it
to
work
on
my
ubuntu
machine
so
i
settled
with
lkh
which
uses
lin-kernighan
heuristic
for
solving
tsp
and
related
problems
i
wrote
scripts
for
file
conversions
and
for
running
lkh
lkh
easily
solved
my
tsp
problem
in
around
30
hours
but
it
was
just
one
path
i
still
had
to
figure
out
how
to
make
it
find
the
second
patha
simple
idea
to
get
2
disjoint
paths
is
to
generate
first
path
and
then
make
weight
of
those
edges
infinite
and
run
lkh
on
the
problem
again
but
this
required
the
problem
to
be
in
distance
matrix
formatthen
i
found
a
major
problem
problem:
ram
too
lowcreating
distance
matrix
for
150000
points
was
unimaginableit
would
requirememory
for
one
digit
*
150000
*
150000assuming
memory
for
one
digit
=
8
bytes
memory
required
=
8*1500002which
is
167
gb!
correct
me
if
i
am
wrong
solution:a
simple
solution
was
to
divide
the
map
in
manageable
chunksi
used
scipy’s
distance
matrix
creation
function
scipyspatialdistancepdist
it
creates
distance
matrix
from
coordinatesthe
matrix
created
by
pdist
is
in
compressed
form
a
flattened
matrix
of
upper
diagonal
elements
scipyspatialdistancesquareform
can
create
a
square
distance
matrix
from
compressed
matrix
but
that
would
waste
a
lot
of
ramso
i
created
a
custom
function
which
divided
compressed
matrix
by
rows
so
lkh
can
read
it
input:coordinates1
12
34
1
output
of
pdist:compressed
upper
column1
2
4
output
of
squreform:uncompressed
square
matrix0
1
21
0
42
4
0
output
of
my
function
which
processed
compressed
matrix:upper
diagonal
elements[[12][4]]
lots
of
ram
saved!
i
tried
using
manhattan
distance
instead
of
euclidean
distance
but
after
dividing
the
problem
in
grids
time
taken
by
distance
calculation
was
manageable
so
i
stuck
with
euclidean
distance
through
trial
and
error
i
found
that
on
my
laptop
with
4
gb
ram
a
6
by
6
grid
in
the
above
format
was
manageable
for
both
creating
distance
matrix
and
for
lkh
i
ran
lkh
on
resulting
distance
matrices
and
joined
the
individual
solutions
i
joined
the
resulting
solutions
in
different
combination
for
both
paths
so
as
to
avoid
common
paths
i
got
7415334
with
this
method
i
tried
time
limit
on
lkh
algorithm
from
40000
seconds
i
reduced
it
to
300
20
5
1
seconds
but
it
made
the
results
slightly
worse
mingle
the
solution
above
was
good
but
it
could
have
been
better
the
problem
was
that
the
first
path
was
so
good
that
the
second
path
struggled
to
find
good
path
the
difference
between
the
two
paths
was
big
path1
~=
62mpath2
~=
74mfor
a
long
time
i
thought
this
would
require
either
solving
both
paths
simultaneously
or
using
genetic
algorithm
or
similar
algorithm
to
combine
both
paths
both
were
pretty
difficult
to
implementthen
i
got
a
simple
idea
my
map
was
divided
in
36
squares
if
i
combine
18
squares
of
first
path
and
18
squares
of
second
path
i
will
have
a
path
whose
distance
will
be
approximately
average
of
the
two
pathsi
tried
this
trick
and
used
different
combinations
of
the
two
paths
squares
and
got
the
best
score
of
6807498
for
new
path1
select
blue
squres
from
old
path1
and
grey
square
from
old
path2
use
remaining
squares
for
new
path2remove
cross
lines
my
squares
were
joined
in
a
zigzag
manner
i
removed
the
zig-zag
lines
for
a
further
improvement
i
scored
6744291
which
was
my
best
scoreanother
idea
was
to
make
end
point
of
one
square
and
the
beginning
point
of
next
square
as
near
as
possible
but
i
couldn’t
implement
the
idea
before
deadlinemy
score
was
around
200000
points
away
from
the
first
place
which
was
6526972
not
bad!
public
repo:
https:bitbucketorghrishikeshiotraveling-santa
more
documentation
for
source
code
coming
soon
originally
published
at
wwwblogiciouscom
on
january
19
2013
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
blockchain
cryptocurrencies
and
the
decentralised
future
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
日本語
português
português
alternate
türkçe
français
한국어
""
العَرَبِيَّة‎‎
español
méxico
español
españa
polski
italiano
普通话
русский
한국어
""
tiếng
việt
or
فارسی
bigger
update:
the
content
of
this
article
is
now
available
as
a
full-length
video
course
that
walks
you
through
every
step
of
the
code
you
can
take
the
course
for
free
and
access
everything
else
on
lyndacom
free
for
30
days
if
you
sign
up
with
this
link
have
you
heard
people
talking
about
machine
learning
but
only
have
a
fuzzy
idea
of
what
that
means?
are
you
tired
of
nodding
your
way
through
conversations
with
co-workers?
let’s
change
that!
this
guide
is
for
anyone
who
is
curious
about
machine
learning
but
has
no
idea
where
to
start
i
imagine
there
are
a
lot
of
people
who
tried
reading
the
wikipedia
article
got
frustrated
and
gave
up
wishing
someone
would
just
give
them
a
high-level
explanation
that’s
what
this
is
the
goal
is
be
accessible
to
anyone
—
which
means
that
there’s
a
lot
of
generalizations
but
who
cares?
if
this
gets
anyone
more
interested
in
ml
then
mission
accomplished
machine
learning
is
the
idea
that
there
are
generic
algorithms
that
can
tell
you
something
interesting
about
a
set
of
data
without
you
having
to
write
any
custom
code
specific
to
the
problem
instead
of
writing
code
you
feed
data
to
the
generic
algorithm
and
it
builds
its
own
logic
based
on
the
data
for
example
one
kind
of
algorithm
is
a
classification
algorithm
it
can
put
data
into
different
groups
the
same
classification
algorithm
used
to
recognize
handwritten
numbers
could
also
be
used
to
classify
emails
into
spam
and
not-spam
without
changing
a
line
of
code
it’s
the
same
algorithm
but
it’s
fed
different
training
data
so
it
comes
up
with
different
classification
logic
machine
learning
is
an
umbrella
term
covering
lots
of
these
kinds
of
generic
algorithms
you
can
think
of
machine
learning
algorithms
as
falling
into
one
of
two
main
categories
—
supervised
learning
and
unsupervised
learning
the
difference
is
simple
but
really
important
let’s
say
you
are
a
real
estate
agent
your
business
is
growing
so
you
hire
a
bunch
of
new
trainee
agents
to
help
you
out
but
there’s
a
problem
—
you
can
glance
at
a
house
and
have
a
pretty
good
idea
of
what
a
house
is
worth
but
your
trainees
don’t
have
your
experience
so
they
don’t
know
how
to
price
their
houses
to
help
your
trainees
and
maybe
free
yourself
up
for
a
vacation
you
decide
to
write
a
little
app
that
can
estimate
the
value
of
a
house
in
your
area
based
on
it’s
size
neighborhood
etc
and
what
similar
houses
have
sold
for
so
you
write
down
every
time
someone
sells
a
house
in
your
city
for
3
months
for
each
house
you
write
down
a
bunch
of
details
—
number
of
bedrooms
size
in
square
feet
neighborhood
etc
but
most
importantly
you
write
down
the
final
sale
price:
using
that
training
data
we
want
to
create
a
program
that
can
estimate
how
much
any
other
house
in
your
area
is
worth:
this
is
called
supervised
learning
you
knew
how
much
each
house
sold
for
so
in
other
words
you
knew
the
answer
to
the
problem
and
could
work
backwards
from
there
to
figure
out
the
logic
to
build
your
app
you
feed
your
training
data
about
each
house
into
your
machine
learning
algorithm
the
algorithm
is
trying
to
figure
out
what
kind
of
math
needs
to
be
done
to
make
the
numbers
work
out
this
kind
of
like
having
the
answer
key
to
a
math
test
with
all
the
arithmetic
symbols
erased:
from
this
can
you
figure
out
what
kind
of
math
problems
were
on
the
test?
you
know
you
are
supposed
to
do
something
with
the
numbers
on
the
left
to
get
each
answer
on
the
right
in
supervised
learning
you
are
letting
the
computer
work
out
that
relationship
for
you
and
once
you
know
what
math
was
required
to
solve
this
specific
set
of
problems
you
could
answer
to
any
other
problem
of
the
same
type!
let’s
go
back
to
our
original
example
with
the
real
estate
agent
what
if
you
didn’t
know
the
sale
price
for
each
house?
even
if
all
you
know
is
the
size
location
etc
of
each
house
it
turns
out
you
can
still
do
some
really
cool
stuff
this
is
called
unsupervised
learning
this
is
kind
of
like
someone
giving
you
a
list
of
numbers
on
a
sheet
of
paper
and
saying
i
don’t
really
know
what
these
numbers
mean
but
maybe
you
can
figure
out
if
there
is
a
pattern
or
grouping
or
something
—
good
luck!
so
what
could
do
with
this
data?
for
starters
you
could
have
an
algorithm
that
automatically
identified
different
market
segments
in
your
data
maybe
you’d
find
out
that
home
buyers
in
the
neighborhood
near
the
local
college
really
like
small
houses
with
lots
of
bedrooms
but
home
buyers
in
the
suburbs
prefer
3-bedroom
houses
with
lots
of
square
footage
knowing
about
these
different
kinds
of
customers
could
help
direct
your
marketing
efforts
another
cool
thing
you
could
do
is
automatically
identify
any
outlier
houses
that
were
way
different
than
everything
else
maybe
those
outlier
houses
are
giant
mansions
and
you
can
focus
your
best
sales
people
on
those
areas
because
they
have
bigger
commissions
supervised
learning
is
what
we’ll
focus
on
for
the
rest
of
this
post
but
that’s
not
because
unsupervised
learning
is
any
less
useful
or
interesting
in
fact
unsupervised
learning
is
becoming
increasingly
important
as
the
algorithms
get
better
because
it
can
be
used
without
having
to
label
the
data
with
the
correct
answer
side
note:
there
are
lots
of
other
types
of
machine
learning
algorithms
but
this
is
a
pretty
good
place
to
start
as
a
human
your
brain
can
approach
most
any
situation
and
learn
how
to
deal
with
that
situation
without
any
explicit
instructions
if
you
sell
houses
for
a
long
time
you
will
instinctively
have
a
feel
for
the
right
price
for
a
house
the
best
way
to
market
that
house
the
kind
of
client
who
would
be
interested
etc
the
goal
of
strong
ai
research
is
to
be
able
to
replicate
this
ability
with
computers
but
current
machine
learning
algorithms
aren’t
that
good
yet
—
they
only
work
when
focused
a
very
specific
limited
problem
maybe
a
better
definition
for
learning
in
this
case
is
figuring
out
an
equation
to
solve
a
specific
problem
based
on
some
example
data
unfortunately
machine
figuring
out
an
equation
to
solve
a
specific
problem
based
on
some
example
data
isn’t
really
a
great
name
so
we
ended
up
with
machine
learning
instead
of
course
if
you
are
reading
this
50
years
in
the
future
and
we’ve
figured
out
the
algorithm
for
strong
ai
then
this
whole
post
will
all
seem
a
little
quaint
maybe
stop
reading
and
go
tell
your
robot
servant
to
go
make
you
a
sandwich
future
human
so
how
would
you
write
the
program
to
estimate
the
value
of
a
house
like
in
our
example
above?
think
about
it
for
a
second
before
you
read
further
if
you
didn’t
know
anything
about
machine
learning
you’d
probably
try
to
write
out
some
basic
rules
for
estimating
the
price
of
a
house
like
this:
if
you
fiddle
with
this
for
hours
and
hours
you
might
end
up
with
something
that
sort
of
works
but
your
program
will
never
be
perfect
and
it
will
be
hard
to
maintain
as
prices
change
wouldn’t
it
be
better
if
the
computer
could
just
figure
out
how
to
implement
this
function
for
you?
who
cares
what
exactly
the
function
does
as
long
is
it
returns
the
correct
number:
one
way
to
think
about
this
problem
is
that
the
price
is
a
delicious
stew
and
the
ingredients
are
the
number
of
bedrooms
the
square
footage
and
the
neighborhood
if
you
could
just
figure
out
how
much
each
ingredient
impacts
the
final
price
maybe
there’s
an
exact
ratio
of
ingredients
to
stir
in
to
make
the
final
price
that
would
reduce
your
original
function
with
all
those
crazy
if’s
and
else’s
down
to
something
really
simple
like
this:
notice
the
magic
numbers
in
bold
—
841231951398213
12311231231
23242341421
and
20123432095
these
are
our
weights
if
we
could
just
figure
out
the
perfect
weights
to
use
that
work
for
every
house
our
function
could
predict
house
prices!
a
dumb
way
to
figure
out
the
best
weights
would
be
something
like
this:
start
with
each
weight
set
to
10:
run
every
house
you
know
about
through
your
function
and
see
how
far
off
the
function
is
at
guessing
the
correct
price
for
each
house:
for
example
if
the
first
house
really
sold
for
$250000
but
your
function
guessed
it
sold
for
$178000
you
are
off
by
$72000
for
that
single
house
now
add
up
the
squared
amount
you
are
off
for
each
house
you
have
in
your
data
set
let’s
say
that
you
had
500
home
sales
in
your
data
set
and
the
square
of
how
much
your
function
was
off
for
each
house
was
a
grand
total
of
$86123373
that’s
how
wrong
your
function
currently
is
now
take
that
sum
total
and
divide
it
by
500
to
get
an
average
of
how
far
off
you
are
for
each
house
call
this
average
error
amount
the
cost
of
your
function
if
you
could
get
this
cost
to
be
zero
by
playing
with
the
weights
your
function
would
be
perfect
it
would
mean
that
in
every
case
your
function
perfectly
guessed
the
price
of
the
house
based
on
the
input
data
so
that’s
our
goal
—
get
this
cost
to
be
as
low
as
possible
by
trying
different
weights
repeat
step
2
over
and
over
with
every
single
possible
combination
of
weights
whichever
combination
of
weights
makes
the
cost
closest
to
zero
is
what
you
use
when
you
find
the
weights
that
work
you’ve
solved
the
problem!
that’s
pretty
simple
right?
well
think
about
what
you
just
did
you
took
some
data
you
fed
it
through
three
generic
really
simple
steps
and
you
ended
up
with
a
function
that
can
guess
the
price
of
any
house
in
your
area
watch
out
zillow!
but
here’s
a
few
more
facts
that
will
blow
your
mind:
pretty
crazy
right?
ok
of
course
you
can’t
just
try
every
combination
of
all
possible
weights
to
find
the
combo
that
works
the
best
that
would
literally
take
forever
since
you’d
never
run
out
of
numbers
to
try
to
avoid
that
mathematicians
have
figured
out
lots
of
clever
ways
to
quickly
find
good
values
for
those
weights
without
having
to
try
very
many
here’s
one
way:
first
write
a
simple
equation
that
represents
step
#2
above:
now
let’s
re-write
exactly
the
same
equation
but
using
a
bunch
of
machine
learning
math
jargon
that
you
can
ignore
for
now:
this
equation
represents
how
wrong
our
price
estimating
function
is
for
the
weights
we
currently
have
set
if
we
graph
this
cost
equation
for
all
possible
values
of
our
weights
for
number_of_bedrooms
and
sqft
we’d
get
a
graph
that
might
look
something
like
this:
in
this
graph
the
lowest
point
in
blue
is
where
our
cost
is
the
lowest
—
thus
our
function
is
the
least
wrong
the
highest
points
are
where
we
are
most
wrong
so
if
we
can
find
the
weights
that
get
us
to
the
lowest
point
on
this
graph
we’ll
have
our
answer!
so
we
just
need
to
adjust
our
weights
so
we
are
walking
down
hill
on
this
graph
towards
the
lowest
point
if
we
keep
making
small
adjustments
to
our
weights
that
are
always
moving
towards
the
lowest
point
we’ll
eventually
get
there
without
having
to
try
too
many
different
weights
if
you
remember
anything
from
calculus
you
might
remember
that
if
you
take
the
derivative
of
a
function
it
tells
you
the
slope
of
the
function’s
tangent
at
any
point
in
other
words
it
tells
us
which
way
is
downhill
for
any
given
point
on
our
graph
we
can
use
that
knowledge
to
walk
downhill
so
if
we
calculate
a
partial
derivative
of
our
cost
function
with
respect
to
each
of
our
weights
then
we
can
subtract
that
value
from
each
weight
that
will
walk
us
one
step
closer
to
the
bottom
of
the
hill
keep
doing
that
and
eventually
we’ll
reach
the
bottom
of
the
hill
and
have
the
best
possible
values
for
our
weights
if
that
didn’t
make
sense
don’t
worry
and
keep
reading
that’s
a
high
level
summary
of
one
way
to
find
the
best
weights
for
your
function
called
batch
gradient
descent
don’t
be
afraid
to
dig
deeper
if
you
are
interested
on
learning
the
details
when
you
use
a
machine
learning
library
to
solve
a
real
problem
all
of
this
will
be
done
for
you
but
it’s
still
useful
to
have
a
good
idea
of
what
is
happening
the
three-step
algorithm
i
described
is
called
multivariate
linear
regression
you
are
estimating
the
equation
for
a
line
that
fits
through
all
of
your
house
data
points
then
you
are
using
that
equation
to
guess
the
sales
price
of
houses
you’ve
never
seen
before
based
where
that
house
would
appear
on
your
line
it’s
a
really
powerful
idea
and
you
can
solve
real
problems
with
it
but
while
the
approach
i
showed
you
might
work
in
simple
cases
it
won’t
work
in
all
cases
one
reason
is
because
house
prices
aren’t
always
simple
enough
to
follow
a
continuous
line
but
luckily
there
are
lots
of
ways
to
handle
that
there
are
plenty
of
other
machine
learning
algorithms
that
can
handle
non-linear
data
like
neural
networks
or
svms
with
kernels
there
are
also
ways
to
use
linear
regression
more
cleverly
that
allow
for
more
complicated
lines
to
be
fit
in
all
cases
the
same
basic
idea
of
needing
to
find
the
best
weights
still
applies
also
i
ignored
the
idea
of
overfitting
it’s
easy
to
come
up
with
a
set
of
weights
that
always
works
perfectly
for
predicting
the
prices
of
the
houses
in
your
original
data
set
but
never
actually
works
for
any
new
houses
that
weren’t
in
your
original
data
set
but
there
are
ways
to
deal
with
this
like
regularization
and
using
a
cross-validation
data
set
learning
how
to
deal
with
this
issue
is
a
key
part
of
learning
how
to
apply
machine
learning
successfully
in
other
words
while
the
basic
concept
is
pretty
simple
it
takes
some
skill
and
experience
to
apply
machine
learning
and
get
useful
results
but
it’s
a
skill
that
any
developer
can
learn!
once
you
start
seeing
how
easily
machine
learning
techniques
can
be
applied
to
problems
that
seem
really
hard
like
handwriting
recognition
you
start
to
get
the
feeling
that
you
could
use
machine
learning
to
solve
any
problem
and
get
an
answer
as
long
as
you
have
enough
data
just
feed
in
the
data
and
watch
the
computer
magically
figure
out
the
equation
that
fits
the
data!
but
it’s
important
to
remember
that
machine
learning
only
works
if
the
problem
is
actually
solvable
with
the
data
that
you
have
for
example
if
you
build
a
model
that
predicts
home
prices
based
on
the
type
of
potted
plants
in
each
house
it’s
never
going
to
work
there
just
isn’t
any
kind
of
relationship
between
the
potted
plants
in
each
house
and
the
home’s
sale
price
so
no
matter
how
hard
it
tries
the
computer
can
never
deduce
a
relationship
between
the
two
so
remember
if
a
human
expert
couldn’t
use
the
data
to
solve
the
problem
manually
a
computer
probably
won’t
be
able
to
either
instead
focus
on
problems
where
a
human
could
solve
the
problem
but
where
it
would
be
great
if
a
computer
could
solve
it
much
more
quickly
in
my
mind
the
biggest
problem
with
machine
learning
right
now
is
that
it
mostly
lives
in
the
world
of
academia
and
commercial
research
groups
there
isn’t
a
lot
of
easy
to
understand
material
out
there
for
people
who
would
like
to
get
a
broad
understanding
without
actually
becoming
experts
but
it’s
getting
a
little
better
every
day
if
you
want
to
try
out
what
you’ve
learned
in
this
article
i
made
a
course
that
walks
you
through
every
step
of
this
article
including
writing
all
the
code
give
it
a
try!
if
you
want
to
go
deeper
andrew
ng’s
free
machine
learning
class
on
coursera
is
pretty
amazing
as
a
next
step
i
highly
recommend
it
it
should
be
accessible
to
anyone
who
has
a
comp
sci
degree
and
who
remembers
a
very
minimal
amount
of
math
also
you
can
play
around
with
tons
of
machine
learning
algorithms
by
downloading
and
installing
scikit-learn
it’s
a
python
framework
that
has
black
box
versions
of
all
the
standard
algorithms
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
newsletter:
also
please
check
out
the
full-length
course
version
of
this
article
it
covers
everything
in
this
article
in
more
detail
including
writing
the
actual
code
in
python
you
can
get
a
free
30-day
trial
to
watch
the
course
if
you
sign
up
with
this
link
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun
part
2!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
the
2016
machine
intelligence
landscape
and
post
can
be
found
here
i
spent
the
last
three
months
learning
about
every
artificial
intelligence
machine
learning
or
data
related
startup
i
could
find
—
my
current
list
has
2529
of
them
to
be
exact
yes
i
should
find
better
things
to
do
with
my
evenings
and
weekends
but
until
then
why
do
this?
a
few
years
ago
investors
and
startups
were
chasing
big
data
i
helped
put
together
a
landscape
on
that
industry
now
we’re
seeing
a
similar
explosion
of
companies
calling
themselves
artificial
intelligence
machine
learning
or
somesuch
—
collectively
i
call
these
machine
intelligence
i’ll
get
into
the
definitions
in
a
second
our
fund
bloomberg
beta
which
is
focused
on
the
future
of
work
has
been
investing
in
these
approaches
i
created
this
landscape
to
start
to
put
startups
into
context
i’m
a
thesis-oriented
investor
and
it’s
much
easier
to
identify
crowded
areas
and
see
white
space
once
the
landscape
has
some
sort
of
taxonomy
what
is
machine
intelligence
anyway?
i
mean
machine
intelligence
as
a
unifying
term
for
what
others
call
machine
learning
and
artificial
intelligence
some
others
have
used
the
term
before
without
quite
describing
it
or
understanding
how
laden
this
field
has
been
with
debates
over
descriptions
i
would
have
preferred
to
avoid
a
different
label
but
when
i
tried
either
artificial
intelligence
or
machine
learning
both
proved
to
too
narrow:
when
i
called
it
artificial
intelligence
too
many
people
were
distracted
by
whether
certain
companies
were
true
ai
and
when
i
called
it
machine
learning
many
thought
i
wasn’t
doing
justice
to
the
more
ai-esque
like
the
various
flavors
of
deep
learning
people
have
immediately
grasped
machine
intelligence
so
here
we
are
☺
computers
are
learning
to
think
read
and
write
they’re
also
picking
up
human
sensory
function
with
the
ability
to
see
and
hear
arguably
to
touch
taste
and
smell
though
those
have
been
of
a
lesser
focus
machine
intelligence
technologies
cut
across
a
vast
array
of
problem
types
from
classification
and
clustering
to
natural
language
processing
and
computer
vision
and
methods
from
support
vector
machines
to
deep
belief
networks
all
of
these
technologies
are
reflected
on
this
landscape
what
this
landscape
doesn’t
include
however
important
is
big
data
technologies
some
have
used
this
term
interchangeably
with
machine
learning
and
artificial
intelligence
but
i
want
to
focus
on
the
intelligence
methods
rather
than
data
storage
and
computation
pieces
of
the
puzzle
for
this
landscape
though
of
course
data
technologies
enable
machine
intelligence
which
companies
are
on
the
landscape?
i
considered
thousands
of
companies
so
while
the
chart
is
crowded
it’s
still
a
small
subset
of
the
overall
ecosystem
admissions
rates
to
the
chart
were
fairly
in
line
with
those
of
yale
or
harvard
and
perhaps
equally
arbitrary
☺
i
tried
to
pick
companies
that
used
machine
intelligence
methods
as
a
defining
part
of
their
technology
many
of
these
companies
clearly
belong
in
multiple
areas
but
for
the
sake
of
simplicity
i
tried
to
keep
companies
in
their
primary
area
and
categorized
them
by
the
language
they
use
to
describe
themselves
instead
of
quibbling
over
whether
a
company
used
nlp
accurately
in
its
self-description
if
you
want
to
get
a
sense
for
innovations
at
the
heart
of
machine
intelligence
focus
on
the
core
technologies
layer
some
of
these
companies
have
apis
that
power
other
applications
some
sell
their
platforms
directly
into
enterprise
some
are
at
the
stage
of
cryptic
demos
and
some
are
so
stealthy
that
all
we
have
is
a
few
sentences
to
describe
them
the
most
exciting
part
for
me
was
seeing
how
much
is
happening
in
the
application
space
these
companies
separated
nicely
into
those
that
reinvent
the
enterprise
industries
and
ourselves
if
i
were
looking
to
build
a
company
right
now
i’d
use
this
landscape
to
help
figure
out
what
core
and
supporting
technologies
i
could
package
into
a
novel
industry
application
everyone
likes
solving
the
sexy
problems
but
there
are
an
incredible
amount
of
‘unsexy’
industry
use
cases
that
have
massive
market
opportunities
and
powerful
enabling
technologies
that
are
begging
to
be
used
for
creative
applications
eg
watson
developer
cloud
alchemyapi
reflections
on
the
landscape:
we’ve
seen
a
few
great
articles
recently
outlining
why
machine
intelligence
is
experiencing
a
resurgence
documenting
the
enabling
factors
of
this
resurgence
kevin
kelly
for
example
chalks
it
up
to
cheap
parallel
computing
large
datasets
and
better
algorithms
i
focused
on
understanding
the
ecosystem
on
a
company-by-company
level
and
drawing
implications
from
that
yes
it’s
true
machine
intelligence
is
transforming
the
enterprise
industries
and
humans
alike
on
a
high
level
it’s
easy
to
understand
why
machine
intelligence
is
important
but
it
wasn’t
until
i
laid
out
what
many
of
these
companies
are
actually
doing
that
i
started
to
grok
how
much
it
is
already
transforming
everything
around
us
as
kevin
kelly
more
provocatively
put
it
the
business
plans
of
the
next
10000
startups
are
easy
to
forecast:
take
x
and
add
ai
in
many
cases
you
don’t
even
need
the
x
—
machine
intelligence
will
certainly
transform
existing
industries
but
will
also
likely
create
entirely
new
ones
machine
intelligence
is
enabling
applications
we
already
expect
like
automated
assistants
siri
adorable
robots
jibo
and
identifying
people
in
images
like
the
highly
effective
but
unfortunately
named
deepface
however
it’s
also
doing
the
unexpected:
protecting
children
from
sex
trafficking
reducing
the
chemical
content
in
the
lettuce
we
eat
helping
us
buy
shoes
online
that
fit
our
feet
precisely
and
destroying
80's
classic
video
games
many
companies
will
be
acquired
i
was
surprised
to
find
that
over
10%
of
the
eligible
non-public
companies
on
the
slide
have
been
acquired
it
was
in
stark
contrast
to
big
data
landscape
we
created
which
had
very
few
acquisitions
at
the
time
no
jaw
will
drop
when
i
reveal
that
google
is
the
number
one
acquirer
though
there
were
more
than
15
different
acquirers
just
for
the
companies
on
this
chart
my
guess
is
that
by
the
end
of
2015
almost
another
10%
will
be
acquired
for
thoughts
on
which
specific
ones
will
get
snapped
up
in
the
next
year
you’ll
have
to
twist
my
arm
big
companies
have
a
disproportionate
advantage
especially
those
that
build
consumer
products
the
giants
in
search
google
baidu
social
networks
facebook
linkedin
pinterest
content
netflix
yahoo!
mobile
apple
and
e-commerce
amazon
are
in
an
incredible
position
they
have
massive
datasets
and
constant
consumer
interactions
that
enable
tight
feedback
loops
for
their
algorithms
and
these
factors
combine
to
create
powerful
network
effects
—
and
they
have
the
most
to
gain
from
the
low
hanging
fruit
that
machine
intelligence
bears
best-in-class
personalization
and
recommendation
algorithms
have
enabled
these
companies’
success
it’s
both
impressive
and
disconcerting
that
facebook
recommends
you
add
the
person
you
had
a
crush
on
in
college
and
netflix
tees
up
that
perfect
guilty
pleasure
sitcom
now
they
are
all
competing
in
a
new
battlefield:
the
move
to
mobile
winning
mobile
will
require
lots
of
machine
intelligence:
state
of
the
art
natural
language
interfaces
like
apple’s
siri
visual
search
like
amazon’s
firefly
and
dynamic
question
answering
technology
that
tells
you
the
answer
instead
of
providing
a
menu
of
links
all
of
the
search
companies
are
wrestling
with
thislarge
enterprise
companies
ibm
and
microsoft
have
also
made
incredible
strides
in
the
field
though
they
don’t
have
the
same
human-facing
requirements
so
are
focusing
their
attention
more
on
knowledge
representation
tasks
on
large
industry
datasets
like
ibm
watson’s
application
to
assist
doctors
with
diagnoses
the
talent’s
in
the
new
aivy
league
in
the
last
20
years
most
of
the
best
minds
in
machine
intelligence
especially
the
‘hardcore
ai’
types
worked
in
academia
they
developed
new
machine
intelligence
methods
but
there
were
few
real
world
applications
that
could
drive
business
value
now
that
real
world
applications
of
more
complex
machine
intelligence
methods
like
deep
belief
nets
and
hierarchical
neural
networks
are
starting
to
solve
real
world
problems
we’re
seeing
academic
talent
move
to
corporate
settings
facebook
recruited
nyu
professors
yann
lecun
and
rob
fergus
to
their
ai
lab
google
hired
university
of
toronto’s
geoffrey
hinton
baidu
wooed
andrew
ng
it’s
important
to
note
that
they
all
still
give
back
significantly
to
the
academic
community
one
of
lecun’s
lab
mandates
is
to
work
on
core
research
to
give
back
to
the
community
hinton
spends
half
of
his
time
teaching
ng
has
made
machine
intelligence
more
accessible
through
coursera
but
it
is
clear
that
a
lot
of
the
intellectual
horsepower
is
moving
away
from
academia
for
aspiring
minds
in
the
space
these
corporate
labs
not
only
offer
lucrative
salaries
and
access
to
the
godfathers
of
the
industry
but
the
most
important
ingredient:
data
these
labs
offer
talent
access
to
datasets
they
could
never
get
otherwise
the
imagenet
dataset
is
fantastic
but
can’t
compare
to
what
facebook
google
and
baidu
have
in
house
as
a
result
we’ll
likely
see
corporations
become
the
home
of
many
of
the
most
important
innovations
in
machine
intelligence
and
recruit
many
of
the
graduate
students
and
postdocs
that
would
have
otherwise
stayed
in
academia
there
will
be
a
peace
dividend
big
companies
have
an
inherent
advantage
and
it’s
likely
that
the
ones
who
will
win
the
machine
intelligence
race
will
be
even
more
powerful
than
they
are
today
however
the
good
news
for
the
rest
of
the
world
is
that
the
core
technology
they
develop
will
rapidly
spill
into
other
areas
both
via
departing
talent
and
published
research
similar
to
the
big
data
revolution
which
was
sparked
by
the
release
of
google’s
bigtable
and
bigquery
papers
we
will
see
corporations
release
equally
groundbreaking
new
technologies
into
the
community
those
innovations
will
be
adapted
to
new
industries
and
use
cases
that
the
googles
of
the
world
don’t
have
the
dna
or
desire
to
tackle
opportunities
for
entrepreneurs:
my
company
does
deep
learning
for
x
few
words
will
make
you
more
popular
in
2015
that
is
if
you
can
credibly
say
them
deep
learning
is
a
particularly
popular
method
in
the
machine
intelligence
field
that
has
been
getting
a
lot
of
attention
google
facebook
and
baidu
have
achieved
excellent
results
with
the
method
for
vision
and
language
based
tasks
and
startups
like
enlitic
have
shown
promising
results
as
well
yes
it
will
be
an
overused
buzzword
with
excitement
ahead
of
results
and
business
models
but
unlike
the
hundreds
of
companies
that
say
they
do
big
data
it’s
much
easier
to
cut
to
the
chase
in
terms
of
verifying
credibility
here
if
you’re
paying
attention
the
most
exciting
part
about
the
deep
learning
method
is
that
when
applied
with
the
appropriate
levels
of
care
and
feeding
it
can
replace
some
of
the
intuition
that
comes
from
domain
expertise
with
automatically-learned
features
the
hope
is
that
in
many
cases
it
will
allow
us
to
fundamentally
rethink
what
a
best-in-class
solution
is
as
an
investor
who
is
curious
about
the
quirkier
applications
of
data
and
machine
intelligence
i
can’t
wait
to
see
what
creative
problems
deep
learning
practitioners
try
to
solve
i
completely
agree
with
jeff
hawkins
when
he
says
a
lot
of
the
killer
applications
of
these
types
of
technologies
will
sneak
up
on
us
i
fully
intend
to
keep
an
open
mind
acquihire
as
a
business
model
people
say
that
data
scientists
are
unicorns
in
short
supply
the
talent
crunch
in
machine
intelligence
will
make
it
look
like
we
had
a
glut
of
data
scientists
in
the
data
field
many
people
had
industry
experience
over
the
past
decade
most
hardcore
machine
intelligence
work
has
only
been
in
academia
we
won’t
be
able
to
grow
this
talent
overnight
this
shortage
of
talent
is
a
boon
for
founders
who
actually
understand
machine
intelligence
a
lot
of
companies
in
the
space
will
get
seed
funding
because
there
are
early
signs
that
the
acquihire
price
for
a
machine
intelligence
expert
is
north
of
5x
that
of
a
normal
technical
acquihire
take
for
example
deep
mind
where
price
per
technical
head
was
somewhere
between
$5–10m
if
we
choose
to
consider
it
in
the
acquihire
category
i’ve
had
multiple
friends
ask
me
only
semi-jokingly
shivon
should
i
just
round
up
all
of
my
smartest
friends
in
the
ai
world
and
call
it
a
company?
to
be
honest
i’m
not
sure
what
to
tell
them
at
bloomberg
beta
we’d
rather
back
companies
building
for
the
long
term
but
that
doesn’t
mean
this
won’t
be
a
lucrative
strategy
for
many
enterprising
founders
a
good
demo
is
disproportionately
valuable
in
machine
intelligence
i
remember
watching
watson
play
jeopardy
when
it
struggled
at
the
beginning
i
felt
really
sad
for
it
when
it
started
trouncing
its
competitors
i
remember
cheering
it
on
as
if
it
were
the
toronto
maple
leafs
in
the
stanley
cup
finals
disclaimers:
1
i
was
an
ibmer
at
the
time
so
was
biased
towards
my
team
2
the
maple
leafs
have
not
made
the
finals
during
my
lifetime
—
yet
—
so
that
was
purely
a
hypothetical
why
do
these
awe-inspiring
demos
matter?
the
last
wave
of
technology
companies
to
ipo
didn’t
have
demos
that
most
of
us
would
watch
so
why
should
machine
intelligence
companies?
the
last
wave
of
companies
were
very
computer-like:
database
companies
enterprise
applications
and
the
like
sure
i’d
like
to
see
a
10x
more
performant
database
but
most
people
wouldn’t
care
machine
intelligence
wins
and
loses
on
demos
because
1
the
technology
is
very
human
enough
to
inspire
shock
and
awe
2
business
models
tend
to
take
a
while
to
form
so
they
need
more
funding
for
longer
period
of
time
to
get
them
there
3
they
are
fantastic
acquisition
bait
watson
beat
the
world’s
best
humans
at
trivia
even
if
it
thought
toronto
was
a
us
city
deepmind
blew
people
away
by
beating
video
games
vicarious
took
on
captcha
there
are
a
few
companies
still
in
stealth
that
promise
to
impress
beyond
that
and
i
can’t
wait
to
see
if
they
get
there
demo
or
not
i’d
love
to
talk
to
anyone
using
machine
intelligence
to
change
the
world
there’s
no
industry
too
unsexy
no
problem
too
geeky
i’d
love
to
be
there
to
help
so
don’t
be
shy
i
hope
this
landscape
chart
sparks
a
conversation
the
goal
to
is
make
this
a
living
document
and
i
want
to
know
if
there
are
companies
or
categories
missing
i
welcome
feedback
and
would
like
to
put
together
a
dynamic
visualization
where
i
can
add
more
companies
and
dimensions
to
the
data
methods
used
data
types
end
users
investment
to
date
location
etc
so
that
folks
can
interact
with
it
to
better
explore
the
space
questions
and
comments:
please
email
me
thank
you
to
andrew
paprocki
aria
haghighi
beau
cronin
ben
lorica
doug
fulop
david
andrzejewski
eric
berlow
eric
jonas
gary
kazantsev
gideon
mann
greg
smithies
heidi
skinner
jack
clark
jon
lehr
kurt
keutzer
lauren
barless
pete
skomoroch
pete
warden
roger
magoulas
sean
gourley
stephen
purpura
wes
mckinney
zach
bogue
the
quid
team
and
the
bloomberg
beta
team
for
your
ever-helpful
perspectives!
disclaimer:
bloomberg
beta
is
an
investor
in
adatao
alation
aviso
brightfunnel
context
relevant
mavrx
newsle
orbital
insights
pop
up
archive
and
two
others
on
the
chart
that
are
still
undisclosed
we’re
also
investors
in
a
few
other
machine
intelligence
companies
that
aren’t
focusing
on
areas
that
were
a
fit
for
this
landscape
so
we
left
them
off
for
the
full
resolution
version
of
the
landscape
please
click
here
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
partner
at
bloomberg
beta
all
about
machine
intelligence
for
good
equal
parts
nerd
and
athlete
straight
up
canadian
stereotype
and
proud
of
it
""
by
naseem
hakim
""
aaron
keys
at
airbnb
we
want
to
build
the
world’s
most
trusted
community
guests
trust
airbnb
to
connect
them
with
world-class
hosts
for
unique
and
memorable
travel
experiences
airbnb
hosts
trust
that
guests
will
treat
their
home
with
the
same
care
and
respect
that
they
would
their
own
the
airbnb
review
system
helps
users
find
community
members
who
earn
this
trust
through
positive
interactions
with
others
and
the
ecosystem
as
a
whole
prospers
the
overwhelming
majority
of
web
users
act
in
good
faith
but
unfortunately
there
exists
a
small
number
of
bad
actors
who
attempt
to
profit
by
defrauding
websites
and
their
communities
the
trust
and
safety
team
at
airbnb
works
across
many
disciplines
to
help
protect
our
users
from
these
bad
actors
ideally
before
they
have
the
opportunity
to
impart
negativity
on
the
community
there
are
many
different
kinds
of
risk
that
online
businesses
may
have
to
protect
against
with
varying
exposure
depending
on
the
particular
business
for
example
email
providers
devote
significant
resources
to
protecting
users
from
spam
whereas
payments
companies
deal
more
with
credit
card
chargebacks
we
can
mitigate
the
potential
for
bad
actors
to
carry
out
different
types
of
attacks
in
different
ways
many
risks
can
be
mitigated
through
user-facing
changes
to
the
product
that
require
additional
verification
from
the
user
for
example
requiring
email
confirmation
or
implementing
2fa
to
combat
account
takeovers
as
many
banks
have
done
scripted
attacks
are
often
associated
with
a
noticeable
increase
in
some
measurable
metric
over
a
short
period
of
time
for
example
a
sudden
1000%
increase
in
reservations
in
a
particular
city
could
be
a
result
of
excellent
marketing
or
fraud
fraudulent
actors
often
exhibit
repetitive
patterns
as
we
recognize
these
patterns
we
can
apply
heuristics
to
predict
when
they
are
about
to
occur
again
and
help
stop
them
for
complex
evolving
fraud
vectors
heuristics
eventually
become
too
complicated
and
therefore
unwieldy
in
such
cases
we
turn
to
machine
learning
which
will
be
the
focus
of
this
blog
post
for
a
more
detailed
look
at
other
aspects
of
online
risk
management
check
out
ohad
samet’s
great
ebook
different
risk
vectors
can
require
different
architectures
for
example
some
risk
vectors
are
not
time
critical
but
require
computationally
intensive
techniques
to
detect
an
offline
architecture
is
best
suited
for
this
kind
of
detection
for
the
purposes
of
this
post
we
are
focusing
on
risks
requiring
realtime
or
near-realtime
action
from
a
broad
perspective
a
machine-learning
pipeline
for
these
kinds
of
risk
must
balance
two
important
goals:
these
may
seem
like
competing
goals
since
optimizing
for
realtime
calculations
during
a
web
transaction
creates
a
focus
on
speed
and
reliability
whereas
optimizing
for
model
building
and
iteration
creates
more
of
a
focus
on
flexibility
at
airbnb
engineering
and
data
teams
have
worked
closely
together
to
develop
a
framework
that
accommodates
both
goals:
a
fast
robust
scoring
framework
with
an
agile
model-building
pipeline
in
keeping
with
our
service-oriented
architecture
we
built
a
separate
fraud
prediction
service
to
handle
deriving
all
the
features
for
a
particular
model
when
a
critical
event
occurs
in
our
system
eg
a
reservation
is
created
we
query
the
fraud
prediction
service
for
this
event
this
service
can
then
calculate
all
the
features
for
the
reservation
creation
model
and
send
these
features
to
our
openscoring
service
which
is
described
in
more
detail
below
the
openscoring
service
returns
a
score
and
a
decision
based
on
a
threshold
we’ve
set
and
the
fraud
prediction
service
can
then
use
this
information
to
take
action
ie
put
the
reservation
on
hold
the
fraud
prediction
service
has
to
be
fast
to
ensure
that
we
are
taking
action
on
suspicious
events
in
near
realtime
like
many
of
our
backend
services
for
which
performance
is
critical
it
is
built
in
java
and
we
parallelize
the
database
queries
necessary
for
feature
generation
however
we
also
want
the
freedom
to
occasionally
do
some
heavy
computation
in
deriving
features
so
we
run
it
asynchronously
so
that
we
are
never
blocking
for
reservations
etc
this
asynchronous
model
works
for
many
situations
where
a
few
seconds
of
delay
in
fraud
detection
has
no
negative
effect
it’s
worth
noting
however
that
there
are
cases
where
you
may
want
to
react
in
realtime
to
block
transactions
in
which
case
a
synchronous
query
and
precomputed
features
may
be
necessary
this
service
is
built
in
a
very
modular
way
and
exposes
an
internal
restful
api
making
adding
new
events
and
models
easy
openscoring
is
a
java
service
that
provides
a
json
rest
interface
to
the
java
predictive
model
markup
language
pmml
evaluator
jpmml
both
jpmml
and
openscoring
are
open
source
projects
released
under
the
apache
20
license
and
authored
by
villu
ruusmann
edit
—
the
most
recent
version
is
licensed
the
under
agpl
30
""
the
jpmml
backend
of
openscoring
consumes
pmml
an
xml
markup
language
that
encodes
several
common
types
of
machine
learning
models
including
tree
models
logit
models
svms
and
neural
networks
we
have
streamlined
openscoring
for
a
production
environment
by
adding
several
features
including
kafka
logging
and
statsd
monitoring
andy
kramolisch
has
modified
openscoring
to
permit
using
several
models
simultaneously
as
described
below
there
are
several
considerations
that
we
weighed
carefully
before
moving
forward
with
openscoring:
after
considering
all
of
these
factors
we
decided
that
openscoring
best
satisfied
our
two-pronged
goal
of
having
a
fast
and
robust
yet
flexible
machine
learning
framework
a
schematic
of
our
model-building
pipeline
using
pmml
is
illustrated
above
the
first
step
involves
deriving
features
from
the
data
stored
on
the
site
since
the
combination
of
features
that
gives
the
optimal
signal
is
constantly
changing
we
store
the
features
in
a
json
format
which
allows
us
to
generalize
the
process
of
loading
and
transforming
features
based
on
their
names
and
types
we
then
transform
the
raw
features
through
bucketing
or
binning
values
and
replacing
missing
values
with
reasonable
estimates
to
improve
signal
we
also
remove
features
that
are
shown
to
be
statistically
unimportant
from
our
dataset
while
we
omit
most
of
the
details
regarding
how
we
perform
these
transformations
for
brevity
here
it
is
important
to
recognize
that
these
steps
take
a
significant
amount
of
time
and
care
we
then
use
our
transformed
features
to
train
and
cross-validate
the
model
using
our
favorite
pmml-compatible
machine
learning
library
and
upload
the
pmml
model
to
openscoring
the
final
model
is
tested
and
then
used
for
decision-making
if
it
becomes
the
best
performer
the
model-training
step
can
be
performed
in
any
language
with
a
library
that
outputs
pmml
one
commonly
used
and
well-supported
library
is
the
r
pmml
package
as
illustrated
below
generating
a
pmml
with
r
requires
very
little
code
this
r
script
has
the
advantage
of
simplicity
and
a
script
similar
to
this
is
a
great
way
to
start
building
pmmls
and
to
get
a
first
model
into
production
in
the
long
run
however
a
setup
like
this
has
some
disadvantages
first
our
script
requires
that
we
perform
feature
transformation
as
a
pre-processing
step
and
therefore
we
have
add
these
transformation
instructions
to
the
pmml
by
editing
it
afterwards
the
r
pmml
package
supports
many
pmml
transformations
and
data
manipulations
but
it
is
far
from
universal
we
deploy
the
model
as
a
separate
step
—
post
model-training
—
and
so
we
have
to
manually
test
it
for
validity
which
can
be
a
time-consuming
process
yet
another
disadvantage
of
r
is
that
the
implementation
of
the
pmml
exporter
is
somewhat
slow
for
a
random
forest
model
with
many
features
and
many
trees
however
we’ve
found
that
simply
re-writing
the
export
function
in
c
decreases
run
time
by
a
factor
of
10000
from
a
few
days
to
a
few
seconds
we
can
get
around
the
drawbacks
of
r
while
maintaining
its
advantages
by
building
a
pipeline
based
on
python
and
scikit-learn
scikit-learn
is
a
python
package
that
supports
many
standard
machine
learning
models
and
includes
helpful
utilities
for
validating
models
and
performing
feature
transformations
we
find
that
python
is
a
more
natural
language
than
r
for
ad-hoc
data
manipulation
and
feature
extraction
we
automate
the
process
of
feature
extraction
based
on
a
set
of
rules
encoded
in
the
names
and
types
of
variables
in
the
features
json
thus
new
features
can
be
incorporated
into
the
model
pipeline
with
no
changes
to
the
existing
code
deployment
and
testing
can
also
be
performed
automatically
in
python
by
using
its
standard
network
libraries
to
interface
with
openscoring
standard
model
performance
tests
precision
recall
roc
curves
etc
are
carried
out
using
sklearn’s
built-in
capabilities
sklearn
does
not
support
pmml
export
out
of
the
box
so
have
written
an
in-house
exporter
for
particular
sklearn
classifiers
when
the
pmml
file
is
uploaded
to
openscoring
it
is
automatically
tested
for
correspondence
with
the
scikit-learn
model
it
represents
because
feature-transformation
model
building
model
validation
deployment
and
testing
are
all
carried
out
in
a
single
script
a
data
scientist
or
engineer
is
able
to
quickly
iterate
on
a
model
based
on
new
features
or
more
recent
data
and
then
rapidly
deploy
the
new
model
into
production
although
this
blog
post
has
focused
mostly
on
our
architecture
and
model
building
pipeline
the
truth
is
that
much
of
our
time
has
been
spent
elsewhere
our
process
was
very
successful
for
some
models
but
for
others
we
encountered
poor
precision-recall
initially
we
considered
whether
we
were
experiencing
a
bias
or
a
variance
problem
and
tried
using
more
data
and
more
features
however
after
finding
no
improvement
we
started
digging
deeper
into
the
data
and
found
that
the
problem
was
that
our
ground
truth
was
not
accurate
consider
chargebacks
as
an
example
a
chargeback
can
be
not
as
described
nad
or
fraud
this
is
a
simplification
and
grouping
both
types
of
chargebacks
together
for
a
single
model
would
be
a
bad
idea
because
legitimate
users
can
file
nad
chargebacks
this
is
an
easy
problem
to
resolve
and
not
one
we
actually
had
agents
categorize
chargebacks
as
part
of
our
workflow
however
there
are
other
types
of
attacks
where
distinguishing
legitimate
activity
from
illegitimate
is
more
subtle
and
necessitated
the
creation
of
new
data
stores
and
logging
pipelines
most
people
who’ve
worked
in
machine
learning
will
find
this
obvious
but
it’s
worth
re-stressing:
towards
this
end
sometimes
you
don’t
know
what
data
you’re
going
to
need
until
you’ve
seen
a
new
attack
especially
if
you
haven’t
worked
in
the
risk
space
before
or
have
worked
in
the
risk
space
but
only
in
a
different
sector
so
the
best
advice
we
can
offer
in
this
case
is
to
log
everything
throw
it
all
in
hdfs
whether
you
need
it
now
or
not
in
the
future
you
can
always
use
this
data
to
backfill
new
data
stores
if
you
find
it
useful
this
can
be
invaluable
in
responding
to
a
new
attack
vector
although
our
current
ml
pipeline
uses
scikit-learn
and
openscoring
our
system
is
constantly
evolving
our
current
setup
is
a
function
of
the
stage
of
the
company
and
the
amount
of
resources
both
in
terms
of
personnel
and
data
that
are
currently
available
smaller
companies
may
only
have
a
few
ml
models
in
production
and
a
small
number
of
analysts
and
can
take
time
to
manually
curate
data
and
train
the
model
in
many
non-standardized
steps
larger
companies
might
have
many
many
models
and
require
a
high
degree
of
automation
and
get
a
sizable
boost
from
online
training
a
unique
challenge
of
working
at
a
hyper-growth
company
is
that
landscape
fundamentally
changes
year-over-year
and
pipelines
need
to
adjust
to
account
for
this
as
our
data
and
logging
pipelines
improve
investing
in
improved
learning
algorithms
will
become
more
worthwhile
and
we
will
likely
shift
to
testing
new
algorithms
incorporating
online
learning
and
expanding
on
our
model
building
framework
to
support
larger
data
sets
additionally
some
of
the
most
important
opportunities
to
improve
our
models
are
based
on
insights
into
our
unique
data
feature
selection
and
other
aspects
our
risk
systems
that
we
are
not
able
to
share
publicly
we
would
like
to
acknowledge
the
other
engineers
and
analysts
who
have
contributed
to
these
critical
aspects
of
this
project
we
work
in
a
dynamic
highly-collaborative
environment
and
this
project
is
an
example
of
how
engineers
and
data
scientists
at
airbnb
work
together
to
arrive
at
a
solution
that
meets
a
diverse
set
of
needs
if
you’re
interested
in
learning
more
contact
us
about
our
data
science
and
engineering
teams!
originally
published
at
nerdsairbnbcom
on
june
16
2014
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
creative
engineers
and
data
scientists
building
a
world
where
you
can
belong
anywhere
http:airbnbio
creative
engineers
and
data
scientists
building
a
world
where
you
can
belong
anywhere
http:airbnbio
""
google’s
word2vec
project
has
created
lots
of
interests
in
the
text
mining
community
it’s
a
neural
network
language
model
that
is
both
supervised
and
unsupervised
unsupervised
in
the
sense
that
you
only
have
to
provide
a
big
corpus
say
english
wiki
supervised
in
the
sense
that
the
model
cleverly
generates
supervised
learning
tasks
from
the
corpus
how?
two
approaches
known
as
continuous
bag
of
words
cbow
and
skip-gram
see
figure
1
in
this
paper
cbow
forces
the
neural
net
to
predict
current
word
by
surrounding
words
and
skip-gram
forces
the
neural
net
to
predict
surrounding
words
of
the
current
word
training
is
essentially
a
classic
back-propagation
method
with
a
few
optimization
and
approximation
tricks
eg
hierarchical
softmax
word
vectors
generated
by
the
neural
net
have
nice
semantic
and
syntactic
behaviors
semantically
ios
is
close
to
android
syntactically
boys
minus
boy
is
close
to
girls
minus
girl
one
can
checkout
more
examples
here
although
this
provides
high
quality
word
vectors
there
is
still
no
clear
way
to
combine
them
into
a
high
quality
document
vector
in
this
article
we
discuss
one
possible
heuristic
inspired
by
a
stochastic
process
called
chinese
restaurant
process
crp
basic
idea
is
to
use
crp
to
drive
a
clustering
process
and
summing
word
vectors
in
the
right
cluster
imagine
we
have
an
document
about
chicken
recipe
it
contains
words
like
chicken
pepper
salt
cheese
it
also
contains
words
like
use
buy
definitely
my
the
the
word2vec
model
gives
us
a
vector
for
each
word
one
could
naively
sum
up
every
word
vector
as
the
doc
vector
this
clearly
introduces
lots
of
noise
a
better
heuristic
is
to
use
a
weighted
sum
based
on
other
information
like
idf
or
part
of
speech
pos
tag
the
question
is:
could
we
be
more
selective
when
adding
terms?
if
this
is
a
chicken
recipe
document
i
shouldn’t
even
consider
words
like
definitely
use
my
in
the
summation
one
can
argue
that
idf
based
weights
can
significantly
reduce
noise
of
boring
words
like
the
and
is
however
for
words
like
definitely
overwhelming
the
idfs
are
not
necessarily
small
as
you
would
hope
it’s
natural
to
think
that
if
we
can
first
group
words
into
clusters
words
like
chicken
pepper
may
stay
in
one
cluster
along
with
other
clusters
of
junk
words
if
we
can
identify
the
relevant
clusters
and
only
summing
up
word
vectors
from
relevant
clusters
we
should
have
a
good
doc
vector
this
boils
down
to
clustering
the
words
in
the
document
one
can
of
course
use
off-the-shelf
algorithms
like
k-means
but
most
these
algorithms
require
a
distance
metric
word2vec
behaves
nicely
by
cosine
similarity
this
doesn’t
necessarily
mean
it
behaves
as
well
under
eucledian
distance
even
after
projection
to
unit
sphere
it’s
perhaps
best
to
use
geodesic
distance
it
would
be
nice
if
we
can
directly
work
with
cosine
similarity
we
have
done
a
quick
experiment
on
clustering
words
driven
by
crp-like
stochastic
process
it
worked
surprisingly
well
—
so
far
now
let’s
explain
crp
imagine
you
go
to
a
chinese
restaurant
there
are
already
n
tables
with
different
number
of
peoples
there
is
also
an
empty
table
crp
has
a
hyperparamter
r
>
0
which
can
be
regarded
as
the
imagined
number
of
people
on
the
empty
table
you
go
to
one
of
the
n1
tables
with
probability
proportional
to
existing
number
of
people
on
the
table
for
the
empty
table
the
number
is
r
if
you
go
to
one
of
the
n
existing
tables
you
are
done
if
you
decide
to
sit
down
at
the
empty
table
the
chinese
restaurant
will
automatically
create
a
new
empty
table
in
that
case
the
next
customer
comes
in
will
choose
from
n2
tables
including
the
new
empty
table
inspired
by
crp
we
tried
the
following
variations
of
crp
to
include
the
similarity
factor
common
setup
is
the
following:
we
are
given
m
vectors
to
be
clustered
we
maintain
two
things:
cluster
sum
not
centroid!
and
vectors
in
clusters
we
iterate
through
vectors
for
current
vector
v
suppose
we
have
n
clusters
already
now
we
find
the
cluster
c
whose
cluster
sum
is
most
similar
to
current
vector
call
this
score
simv
c
variant
1:
v
creates
a
new
cluster
with
probability
11
""
n
otherwise
v
goes
to
cluster
c
variant
2:
if
simv
c
>
11
""
n
goes
to
cluster
c
otherwise
with
probability
11n
it
creates
a
new
cluster
and
with
probability
n1n
it
goes
to
c
in
any
of
the
two
variants
if
v
goes
to
a
cluster
we
update
cluster
sum
and
cluster
membership
there
is
one
distinct
difference
to
traditional
crp:
if
we
don’t
go
to
empty
table
we
deterministically
go
to
the
most
similar
table
in
practice
we
find
these
variants
create
similar
results
one
difference
is
that
variant
1
tend
to
have
more
clusters
and
smaller
clusters
variant
2
tend
to
have
fewer
but
larger
clusters
the
examples
below
are
from
variant
2
for
example
for
a
chicken
recipe
document
the
clusters
look
like
this:
apparently
the
first
cluster
is
most
relevant
now
let’s
take
the
cluster
sum
vector
which
is
the
sum
of
all
vectors
from
this
cluster
and
test
if
it
really
preserves
semantic
below
is
a
snippet
of
python
console
we
trained
word
vector
using
the
c
implementation
on
a
fraction
of
english
wiki
and
read
the
model
file
using
python
library
gensimmodelword2vec
c[0]
below
denotes
the
cluster
0
looks
like
the
semantic
is
preserved
well
it’s
convincing
that
we
can
use
this
as
the
doc
vector
the
recipe
document
seems
easy
now
let’s
try
something
more
challenging
like
a
news
article
news
articles
tend
to
tell
stories
and
thus
has
less
concentrated
topic
words
we
tried
the
clustering
on
this
article
titled
signals
on
radar
puzzle
officials
in
hunt
for
malaysian
jet
we
got
4
clusters:
again
looks
decent
note
that
this
is
a
simple
1-pass
clustering
process
and
we
don’t
have
to
specify
number
of
clusters!
could
be
very
helpful
for
latency
sensitive
services
there
is
still
a
missing
step:
how
to
find
out
the
relevant
clusters?
we
haven’t
yet
done
extensive
experiments
on
this
part
a
few
heuristics
to
consider:
there
are
other
problems
to
think
about:
1
how
do
we
merge
clusters?
based
on
similarity
among
cluster
sum
vectors?
or
averaging
similarity
between
cluster
members?
2
what
is
the
minimal
set
of
words
that
can
reconstruct
cluster
sum
vector
in
the
sense
of
cosine
similarity?
this
could
be
used
as
a
semantic
keyword
extraction
method
conclusion:
google’s
word2vec
provides
powerful
word
vectors
we
are
interested
in
using
these
vectors
to
generate
high
quality
document
vectors
in
an
efficient
way
we
tried
a
strategy
based
on
a
variant
of
chinese
restaurant
process
and
obtained
interesting
results
there
are
some
open
problems
to
explore
and
we
would
like
to
hear
what
you
think
appendix:
python
style
pseudo-code
for
similarity
driven
crp
we
wrote
this
post
while
working
on
kifi
—
connecting
people
with
knowledge
learn
more
originally
published
at
engkificom
on
march
17
2014
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
the
kifi
engineering
blog
""
chris
pinchak
|
pinterest
engineer
discovery
the
home
feed
should
be
a
reflection
of
what
each
user
cares
about
content
is
sourced
from
inputs
such
as
people
and
boards
the
user
follows
interests
and
recommendations
to
ensure
we
maintain
fast
reliable
and
personalized
home
feeds
we
built
the
smart
feed
with
the
following
design
values
in
mind:
1
different
sources
of
pins
should
be
mixed
together
at
different
rates
2
some
pins
should
be
selectively
dropped
or
deferred
until
a
later
time
some
sources
may
produce
pins
of
poor
quality
for
a
user
so
instead
of
showing
everything
available
immediately
we
can
be
selective
about
what
to
show
and
what
to
hold
back
for
a
future
session
3
pins
should
be
arranged
in
the
order
of
best-first
rather
than
newest-first
for
some
sources
newer
pins
are
intuitively
better
while
for
others
newness
is
less
important
we
shifted
away
from
our
previously
time-ordered
home
feed
system
and
onto
a
more
flexible
one
the
core
feature
of
the
smart
feed
architecture
is
its
separation
of
available
but
unseen
content
and
content
that’s
already
been
presented
to
the
user
we
leverage
knowledge
of
what
the
user
hasn’t
yet
seen
to
our
advantage
when
deciding
how
the
feed
evolves
over
time
smart
feed
is
a
composition
of
three
independent
services
each
of
which
has
a
specific
role
in
the
construction
of
a
home
feed
the
smart
feed
worker
is
the
first
to
process
pins
and
has
two
primary
responsibilities
—
to
accept
incoming
pins
and
assign
some
score
proportional
to
their
quality
or
value
to
the
receiving
user
and
to
remember
these
scored
pins
in
some
storage
for
later
consumption
essentially
the
worker
manages
pins
as
they
become
newly
available
such
as
those
from
the
repins
of
the
people
the
user
follows
pins
have
varying
value
to
the
receiving
user
so
the
worker
is
tasked
with
deciding
the
magnitude
of
their
subjective
quality
incoming
pins
are
currently
obtained
from
three
separate
sources:
repins
made
by
followed
users
related
pins
and
pins
from
followed
interests
each
is
scored
by
the
worker
and
then
inserted
into
a
pool
for
that
particular
type
of
pin
each
pool
is
a
priority
queue
sorted
on
score
and
belongs
to
a
single
user
newly
added
pins
mix
with
those
added
before
allowing
the
highest
quality
pins
to
be
accessible
over
time
at
the
front
of
the
queue
pools
can
be
implemented
in
a
variety
of
ways
so
long
as
the
priority
queue
requirement
is
met
we
choose
to
do
this
by
exploiting
the
key-based
sorting
of
hbase
each
key
is
a
combination
of
user
score
and
pin
such
that
for
any
user
we
may
scan
a
list
of
available
pins
according
to
their
score
newly
added
triples
will
be
inserted
at
their
appropriate
location
to
maintain
the
score
order
this
combination
of
user
score
and
pin
into
a
key
value
can
be
used
to
create
a
priority
queue
in
other
storage
systems
aside
from
hbase
a
property
we
may
use
in
the
future
depending
on
evolving
storage
requirements
distinct
from
the
smart
feed
worker
the
smart
feed
content
generator
is
concerned
primarily
with
defining
what
new
means
in
the
context
of
a
home
feed
when
a
user
accesses
the
home
feed
we
ask
the
content
generator
for
new
pins
since
their
last
visit
the
generator
decides
the
quantity
composition
and
arrangement
of
new
pins
to
return
in
response
to
this
request
the
content
generator
assembles
available
pins
into
chunks
for
consumption
by
the
user
as
part
of
their
home
feed
the
generator
is
free
to
choose
any
arrangement
based
on
a
variety
of
input
signals
and
may
elect
to
use
some
or
all
of
the
pins
available
in
the
pools
pins
that
are
selected
for
inclusion
in
a
chunk
are
thereafter
removed
from
from
the
pools
so
they
cannot
be
returned
as
part
of
subsequent
chunks
the
content
generator
is
generally
free
to
perform
any
rearrangements
it
likes
but
is
bound
to
the
priority
queue
nature
of
the
pools
when
the
generator
asks
for
n
pins
from
a
pool
it’ll
get
the
n
highest
scoring
ie
best
pins
available
therefore
the
generator
doesn’t
need
to
concern
itself
with
finding
the
best
available
content
but
instead
with
how
the
best
available
content
should
be
presented
in
addition
to
providing
high
availability
of
the
home
feed
the
smart
feed
service
is
responsible
for
combining
new
pins
returned
by
the
content
generator
with
those
that
previously
appeared
in
the
home
feed
we
can
separate
these
into
the
chunk
returned
by
the
content
generator
and
the
materialized
feed
managed
by
the
smart
feed
service
the
materialized
feed
represents
a
frozen
view
of
the
feed
as
it
was
the
last
time
the
user
viewed
it
to
the
materialized
pins
we
add
the
pins
from
the
content
generator
in
the
chunk
the
service
makes
no
decisions
about
order
instead
it
adds
the
pins
in
exactly
the
order
given
by
the
chunk
because
it
has
a
fairly
low
rate
of
reading
and
writing
the
materialized
feed
is
likely
to
suffer
from
fewer
availability
events
in
addition
feeds
can
be
trimmed
to
restrict
them
to
a
maximum
size
the
need
for
less
storage
means
we
can
easily
increase
the
availability
and
reliability
of
the
materialized
feed
through
replication
and
the
use
of
faster
storage
hardware
the
smart
feed
service
relies
on
the
content
generator
to
provide
new
pins
if
the
generator
experiences
a
degradation
in
performance
the
service
can
gracefully
handle
the
loss
of
its
availability
in
the
event
the
content
generator
encounters
an
exception
while
generating
a
chunk
or
if
it
simply
takes
too
long
to
produce
one
the
smart
feed
service
will
return
the
content
contained
in
the
materialized
feed
in
this
instance
the
feed
will
appear
to
the
end
user
as
unchanged
from
last
time
future
feed
views
will
produce
chunks
as
large
as
or
larger
than
the
last
so
that
eventually
the
user
will
see
new
pins
by
moving
to
smart
feed
we
achieved
the
goals
of
a
highly
flexible
architecture
and
better
control
over
the
composition
of
home
feeds
the
home
feed
is
now
powered
by
three
separate
services
each
with
a
well-defined
role
in
its
production
and
distribution
the
individual
services
can
be
altered
or
replaced
with
components
that
serve
the
same
general
purpose
the
use
of
pools
to
buffer
pins
according
to
their
quality
allows
us
a
greater
amount
of
control
over
the
composition
of
home
feeds
continuing
with
this
project
we
intend
to
better
model
users’
preferences
with
respect
to
pins
in
their
home
feeds
our
accuracy
of
recommendation
quality
varies
considerably
over
our
user
base
and
we
would
benefit
from
using
preference
information
gathered
from
recent
interactions
with
the
home
feed
knowledge
of
personal
preference
will
also
help
us
order
home
feeds
so
the
pins
of
most
value
can
be
discovered
with
the
least
amount
of
effort
if
you’re
interested
in
tackling
challenges
and
making
improvements
like
this
join
our
team!
chris
pinchak
is
a
software
engineer
at
pinterest
acknowledgements:
this
technology
was
built
in
collaboration
with
dan
feng
dmitry
chechik
raghavendra
prabhu
jeremy
carroll
xun
liu
varun
sharma
joe
lau
yuchen
liu
tian-ying
chang
and
yun
park
this
team
as
well
as
people
from
across
the
company
helped
make
this
project
a
reality
with
their
technical
insights
and
invaluable
feedback
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
inventive
engineers
building
the
first
visual
discovery
engine
100
billion
ideas
and
counting
https:careerspinterestcomcareersengineering
""
the
term
data
scientist
has
been
used
lately
to
describe
a
wide
variety
of
skills
""
roles
in
this
post
i
will
focus
on
a
particular
flavor
of
data
scientist
i
will
talk
about
the
qualities
needed
to
be
a
good
data
scientist-engineer
who
ships
relevance
products
to
users
some
examples
of
relevance
products
are:
these
folks
need
to
be
strong
at
data
science
and
engineering
to
be
successful
some
places
call
these
folks
as
machine
learning
engineers
since
most
of
the
work
they
do
involves
machine
learning
more
generally
i
feel
relevance
engineer
is
a
good
term
to
describe
them
relevance
engineers
have
a
common
set
of
skills
that
they
draw
upon
to
get
their
jobs
done
the
list
below
doesn’t
include
some
of
the
known
obvious
skills
you
obviously
need
to
be
smart
you
obviously
need
to
have
or
be
able
to
learn
quickly
the
required
book
knowledge
but
beyond
that
there
are
a
bunch
of
not-so-obvious
skills
that
you
can’t
learn
from
a
book
here
are
some
of
those
in
no
particular
order:
this
list
is
by
no
means
exhaustive
but
does
capture
some
of
the
qualities
of
the
smartest
folks
i
have
worked
with
happy
to
hear
what
you
think
thanks
to
peter
bailey
and
andrew
hogue
for
feedback
on
the
initial
revisions
*in
this
post
feature
means
a
software
feature
not
a
machine
learning
feature
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
engineering
manager
doing
machine
learning
@
google
previously
worked
on
ml
and
search
at
quora
foursquare
and
bing
sharing
concepts
ideas
and
codes
""
i
recently
wrapped
up
my
second
hackathon
at
intent
media
you
can
see
my
summary
of
one
of
our
previous
hackathons
here
these
past
two
hackathons
i’ve
taken
on
some
slightly
different
challenges
than
people
usually
go
after
in
a
hackathon:
developing
new
machine
learning
models
while
i‘ve
been
working
on
data
science
and
machine
learning
systems
for
a
while
i’ve
found
that
trying
to
do
so
under
extreme
constraints
can
be
a
distinctly
different
experience
a
very
good
data
hacker
can
easily
find
themselves
with
a
great
idea
at
a
hackathon
but
with
little
to
nothing
to
demo
at
the
end
accepting
that
my
personal
experience
is
just
my
own
let
me
offer
three
tips
for
building
new
models
at
a
hackathon
when
you’re
doing
a
more
traditional
web
app
hack
at
a
hackathon
you
can
almost
run
out
of
time
and
still
come
up
with
something
pretty
good
as
long
as
you
get
that
last
bug
fixed
before
the
demo
this
is
a
great
characteristic
to
build
into
the
plan
of
a
hack
but
one
that
simply
does
not
apply
to
a
machine
learning
hack
think
about
what
happens
when
you
do
find
that
last
bug
in
a
machine
learning
project
you
still
need
to
potentially
do
all
of
the
below:
that’s
no
just
hit
refresh
workflow
even
with
a
well-oiled
workflow
some
of
those
tasks
can
take
all
of
the
time
your
average
one-day
hackathon
is
scheduled
for
take
#3
for
example
training
a
production
grade
model
using
say
hadoop
can
take
a
lot
of
time
even
if
you
have
the
cash
to
spin
up
a
fair-sized
cluster
of
ec2
instances
what
that
means
for
your
hack
can
vary
but
you’re
just
asking
for
trouble
if
you
don’t
start
with
that
fact
taken
into
account
in
the
scope
and
goals
of
your
project
a
solid
project
design
is
absolutely
crucial
if
you’re
going
to
hope
to
take
all
of
the
little
steps
involved
in
getting
your
model
ready
to
demo
which
leads
me
to
my
next
point
one
of
the
best
things
about
working
in
data
science
is
all
of
the
really
smart
people
but
of
course
the
corollary
is
that
one
of
the
worst
things
about
working
in
data
science
is
all
of
the
really
smart
people
sharp
engineers
and
data
scientists
can
take
the
nugget
of
an
idea
and
envision
a
useful
powerful
suite
of
products
that
would
take
years
to
build
which
is
not
so
useful
when
you
have
a
day
or
two
mature
dataists
know
just
how
much
ambition
is
too
much
and
plan
accordingly
i
happen
to
be
lucky
enough
to
work
with
some
very
smart
and
very
mature
data
scientists
and
engineers
so
this
has
not
been
a
problem
for
either
of
my
last
few
hacks
but
i’m
just
lucky
that
way
you
might
not
be
so
lucky
unrealistic
ambitions
are
a
constant
danger
in
a
machine
learning
hack
running
along
the
edge
of
all
activities
like
a
precipice
beckoning
you
to
dive
off
and
see
where
you
land
if
you
take
one
thing
away
from
this
post
let
it
be
this:
don’t
dive
off
the
cliff
just
don’t
do
it
you
won’t
like
where
you
land
you’ll
wind
up
with
more
questions
than
answers
and
you’ll
have
nothing
to
show
come
demo
time
moreover
your
fellow
devs
who
worked
on
apps
and
not
models
will
simply
not
understand
what
you
spent
your
time
on
what
does
a
precipice
look
like?
it
could
be
a
novel
distance
metric
it
could
be
a
fundamental
improvement
to
a
widely
used
technique
like
svrs
or
it
could
just
be
something
really
benign
sounding
like
a
longer
training
set
i
would
say
that
even
choosing
to
pose
the
problem
as
a
regression
one
instead
of
a
classification
one
could
qualify
the
danger
originates
in
the
intrinsic
tension
between
the
rigorous
and
exploratory
mode
of
academic
data
sciencemachine
learning
education
and
the
pedal-to-the-metal
pace
mandated
by
a
hackathon
they
are
very
different
modes
of
working
and
you’re
just
going
to
have
suspend
some
of
your
good
habits
for
a
day
or
so
if
you
want
to
have
something
to
demo
this
last
point
can
be
the
trickiest
to
put
in
practice
but
i
think
it
can
totally
be
the
difference
between
a
project
that
feels
like
a
hack
and
one
that
feels
like
just
getting
warmed
up
on
a
weeklong
story
if
you’ve
figured
out
how
to
scope
your
project
appropriately
and
designed
something
that
can
really
be
built
in
a
day
or
two
you
can
still
actually
fail
to
do
so
i
think
it
can
the
difference
can
easily
come
down
to
technology
choices
for
example
i
currently
make
my
living
writing
cascalog
clojure
and
java
on
top
of
hadoop
to
process
files
stored
in
s3
i
know
these
tools
well
enough
to
pay
my
rent
but
i
would
absolutely
hesitate
to
use
any
of
them
in
a
tight-paced
context
i
have
spent
weeks
trying
to
understand
a
single
cascalog
bug
seriously
if
you
know
the
language
python
offers
an
unbeatable
value
proposition
for
this
use
case
scikit-learn
has
nearly
everything
you
could
imagine
needing
pandas
numpy
and
scipy
are
all
sitting
there
to
be
brought
in
when
appropriate
and
don’t
forget
how
awesome
it
can
be
to
prototype
in
a
purpose-built
exploratory
development
environment
like
ipython
but
this
is
machine
learning
and
sometimes
our
data
is
just
big
maybe
even
web
scale
some
people
hate
these
phrases
but
they
serve
a
purpose
we
don’t
all
use
hadoop
out
of
love
for
horrendously
complex
java
applications
big
data
is
not
just
statistics
on
a
mac
pro
although
it
can
often
look
like
that
scale
can
be
a
real
necessity
even
in
a
hackathon
when
it
is
there
are
no
easy
answers
if
you’re
lucky
maybe
you
can
actually
work
with
multiple
hour
model
learning
times
if
you’re
really
lucky
you
might
be
using
spark
and
not
hadoop
in
which
case
it
might
not
take
hours
to
learn
your
model
my
point
is
that
insofar
as
you
have
a
choice
choose
the
leaner
meaner
tool
the
one
that
will
let
you
do
more
with
less
input
required
from
you
don’t
use
that
c
library
that
promises
awesome
runtime
but
with
python
bindings
that
you’ve
never
tried
you’ll
never
figure
out
its
quirks
in
time
write
as
little
data
cleanup
code
as
you
can
manage
commands
like
dropna
can
save
you
precious
minutes
to
hours
and
if
you
can
get
your
data
from
database
or
an
api
instead
of
files
then
for
the
love
of
cthulhu
do
it
hell
even
if
you
have
to
load
your
data
from
files
to
a
database
first
it
might
be
worth
your
time
sql
is
one
of
the
highest
productivity
rapid
prototyping
tools
i
know
and
though
i
love
to
bash
on
the
clunkiness
of
hadoop
there
are
even
ways
of
taking
some
serious
pain
out
of
using
it
under
pressure
depending
on
what
you’re
doing
elastic
map
reduce
or
predictionio
can
get
you
to
the
point
of
being
productive
much
faster
i
love
hackathons
and
their
variations
they
remind
me
of
the
fun
old
days
in
grad
school
furiously
hacking
away
to
come
up
with
something
interesting
to
say
about
definitionally
uncertain
stuff
the
furious
pace
and
the
pragmatic
compromises
are
part
of
the
fun
compared
to
things
like
pitch
events
hackathons
have
way
less
problems
even
if
they
have
their
issues
as
well
at
their
best
they’re
about
the
love
of
unconstrained
creation
i’ve
tried
to
do
machine
learning
hacks
because
it’s
just
so
damn
cool
to
go
from
zero
to
having
a
program
that
makes
decisions
it
amazes
me
every
time
it
works
and
doubly
so
when
i
can
manage
to
get
something
working
on
a
deadline
taking
on
a
challenge
like
building
a
new
model
in
a
hackathon
is
also
a
great
learning
experience
especially
if
you
get
to
work
as
part
of
a
strong
team
machine
learning
in
the
real
world
is
an
even
larger
topic
than
its
academic
cousin
and
there’s
always
interesting
things
to
learn
hackathons
can
be
great
places
to
rapidly
iterate
through
approaches
and
learn
from
your
teammates
how
to
build
things
better
and
faster
that’s
pretty
likely
to
come
in
handy
sometime
the
main
part
of
the
post
is
over
but
i
wanted
to
make
sure
to
leave
a
note
for
anyone
who
was
interested
in
what
we
hack
at
intent
media
or
what
we
build
for
our
customers
we’re
hiring
all
sorts
of
smart
people
to
build
systems
for
machine
learning
and
more
please
reach
out
if
you
want
to
hear
more
about
how
and
why
we
do
what
we
do
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
author
of
reactive
machine
learning
systems
@manningbooks
building
ais
for
fun
and
profit
friend
of
animals
laying
the
foundation
of
tomorrow’s
big
data
""
many
people
are
already
familiar
with
apple’s
voice
search
called
siri
or
the
search
engine
behind
it
called
wolfram
alpha
this
search
engine
can
use
natural
language
to
search
vast
sets
of
data
and
even
compute
math
however
this
is
just
a
tiny
fraction
of
what
the
language
can
do
and
i
don’t
even
think
it’s
a
good
introduction
to
what’s
possible
to
understand
the
raw
power
of
the
underlying
technology
you
really
have
to
understand
what
it
is
and
a
little
about
how
it
works
the
wolfram
website
has
wonderful
documentation
and
explanations
but
for
the
uninitiated
it
can
seem
bewildering
they
have
repackaged
the
language
so
many
different
ways
that
it
can
be
hard
for
the
beginner
to
understand
exactly
what
it
is
that’s
why
i
want
to
venture
my
own
introduction
let’s
start
with
it’s
origins
mathematica
was
designed
as
a
desktop
tool
for
computational
research
and
exploration
it
continued
evolving
and
the
breakthrough
was
realizing
those
symbols
could
be
anything:
images
sounds
algorithms
geometry
data-sets
""
anything
so
it
became
more
than
just
a
language
stephen
wolfram
calls
this
a
knowledge-based
language
because
it
has
smart-objects
built
in
that
can
be
computed
the
language
doesn’t
simply
find
results
it
computes
results
into
actual
models
analysis
and
other
symbolic
objects
the
real
power
is
that
the
results
remain
symbolic
objects
that
can
be
further
manipulated
symbolically
ie
embed
in
another
symbolic
object
operate
on
it
in
short
anything
can
be
computed
pretty
abstract
i
know
don’t
worry
we’ll
get
to
examples
soon
the
actual
syntax
is
a
combination
of
objects
and
operators
which
are
grouped
and
ordered
by
square
brackets
[
]
the
stuff
at
the
center
of
the
formula
gets
read
first
and
then
it
expands
out
like
a
russian
doll
out
of
many
potential
examples
i
have
carefully
selected
one
from
their
site
to
illustrate
it’s
simplicity
and
power
let’s
say
we
want
our
system
to
determine
the
difference
between
poetry
and
prose
this
would
be
difficult
to
program
directly
because
there
are
so
many
variables
and
the
differences
are
so
subtle
with
wolfram
language
that
hard
stuff
becomes
easy
you
can
train
it
recognize
the
difference
very
quickly
here’s
how
it
works
let’s
use
shakespeare
for
an
example:
first
scan
all
of
hamlet
and
call
that
type
of
stuff
prose
then
scan
all
of
shakespeare’s
sonnets
and
call
that
stuff
poetry
easy
next
train
the
system
with
machine
learning:
classify
and
predict
are
the
two
big
functions
we
want
to
classify
which
is
poetry
and
which
is
prose
wolfram
looks
at
our
situation
and
instantly
determines
that
the
markov
method
is
the
best
for
differentiating
among
all
the
subtle
differences
between
prose
and
poetry
that’s
it
any
system
using
this
bit
of
training
will
automatically
be
able
to
detect
the
difference
between
poetry
and
prose
with
a
high
degree
of
accuracy
the
key
to
this
accuracy
is
the
size
of
the
data
set
you
really
need
at
millions
of
data
points
to
train
it
reliably
but
with
wolfram
many
of
those
data
sets
are
already
built
in
easy
this
is
just
one
tiny
example
to
illustrate
what
the
language
looks
like
and
how
it
goes
beyond
symbols
to
work
with
computable
objects
we
could
continue
translating
poems
into
interactive
maps
and
interactions
into
music
and
so
on
how
does
wolfram
compare
with
other
products
like
apache
hadoop
and
others?
well
it’s
a
totally
different
thing
in
those
products
everything
is
manual
the
various
axis
and
all
the
variables
are
manually
defined
instead
wolfram
intelligently
applies
formulas
and
makes
choices
to
optimize
results
based
on
specific
conditions
it
makes
the
hard
stuff
automatic
plus
it’s
capable
of
much
more
than
machine
learning
that’s
just
one
example
of
hundreds:
sound
3d-geometry
language
images
etc
—
and
a
mixture
of
them
all
mathematica
is
still
the
most
powerful
and
polished
way
to
access
the
wolfram
language
their
new
programming
cloud
and
other
cloud
offerings
signal
serious
intent
to
move
to
the
web
but
it
is
still
early
days
the
language
is
very
mature
for
desktop
exploration
and
some
companies
have
even
made
mathematica
applications
for
small
scale
internal
use
which
can
be
quite
useful
even
though
the
wolfram
website
has
signaled
intent
to
make
it
more
broadly
deployable
within
commercial
services
i
don’t
think
this
is
the
proper
way
to
use
the
language
within
my
own
company
we
find
wolfram
extremely
handy
for
research
but
not
deployment
within
a
web-based
product
in
short
it
isn’t
performant:
commercial
products
require
more
than
a
powerful
language
they
are
made
within
an
ecosystem
of
services
and
vendors
that
all
have
to
work
together
without
machine
learning
built
into
the
native
cloud
where
data
is
stored
it
can’t
be
deployed
in
a
saas
product
in
a
way
that
lives
up
to
expectations
while
stephen
wolfram
would
love
for
his
language
to
be
used
within
commercial
products
i
think
he
resents
having
to
play
nice
with
lower
level
languages
his
alternative
of
making
api
requests
across
the
web
isn’t
a
good
way
to
embed
intelligence
within
products
and
i
don’t
think
we
will
ever
see
entire
saas
products
built
entirely
with
a
functional
language
programming
is
the
art
of
automation
the
wolfram
community
is
full
of
very
smart
people
using
the
language
for
research
and
exploration
they
represent
the
cutting
edge
of
computation
personally
i’m
looking
forward
to
when
we
can
see
intelligence
woven
into
commercial
and
consumer
products
that
solve
real
problems
for
people
on
a
daily
basis
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ceo
of
learning
machine
wwwlearningmachinecom
""
this
content
originally
appeared
on
curious
insight
this
post
is
part
of
a
series
covering
the
exercises
from
andrew
ng’s
machine
learning
class
on
coursera
the
original
code
exercise
text
and
data
files
for
this
post
are
available
here
part
1
—
simple
linear
regressionpart
2
—
multivariate
linear
regressionpart
3
—
logistic
regressionpart
4
—
multivariate
logistic
regressionpart
5
—
neural
networkspart
6
—
support
vector
machinespart
7
—
k-means
clustering
""
pcapart
8
—
anomaly
detection
""
recommendation
one
of
the
pivotal
moments
in
my
professional
development
this
year
came
when
i
discovered
coursera
i’d
heard
of
the
mooc
phenomenon
but
had
not
had
the
time
to
dive
in
and
take
a
class
earlier
this
year
i
finally
pulled
the
trigger
and
signed
up
for
andrew
ng’s
machine
learning
class
i
completed
the
whole
thing
from
start
to
finish
including
all
of
the
programming
exercises
the
experience
opened
my
eyes
to
the
power
of
this
type
of
education
platform
and
i’ve
been
hooked
ever
since
this
blog
post
will
be
the
first
in
a
series
covering
the
programming
exercises
from
andrew’s
class
one
aspect
of
the
course
that
i
didn’t
particularly
care
for
was
the
use
of
octave
for
assignments
although
octavematlab
is
a
fine
platform
most
real-world
data
science
is
done
in
either
r
or
python
certainly
there
are
other
languages
and
tools
being
used
but
these
two
are
unquestionably
at
the
top
of
the
list
since
i’m
trying
to
develop
my
python
skills
i
decided
to
start
working
through
the
exercises
from
scratch
in
python
the
full
source
code
is
available
at
my
ipython
repo
on
github
you’ll
also
find
the
data
used
in
these
exercises
and
the
original
exercise
pdfs
in
sub-folders
off
the
root
directory
if
you’re
interested
while
i
can
explain
some
of
the
concepts
involved
in
this
exercise
along
the
way
it’s
impossible
for
me
to
convey
all
the
information
you
might
need
to
fully
comprehend
it
if
you’re
really
interested
in
machine
learning
but
haven’t
been
exposed
to
it
yet
i
encourage
you
to
check
out
the
class
it’s
completely
free
and
there’s
no
commitment
whatsoever
with
that
let’s
get
started!
in
the
first
part
of
exercise
1
we’re
tasked
with
implementing
simple
linear
regression
to
predict
profits
for
a
food
truck
suppose
you
are
the
ceo
of
a
restaurant
franchise
and
are
considering
different
cities
for
opening
a
new
outlet
the
chain
already
has
trucks
in
various
cities
and
you
have
data
for
profits
and
populations
from
the
cities
you’d
like
to
figure
out
what
the
expected
profit
of
a
new
food
truck
might
be
given
only
the
population
of
the
city
that
it
would
be
placed
in
let’s
start
by
examining
the
data
which
is
in
a
file
called
ex1data1txt
in
the
data
directory
of
my
repository
above
first
we
need
to
import
a
few
libraries
now
let’s
get
things
rolling
we
can
use
pandas
to
load
the
data
into
a
data
frame
and
display
the
first
few
rows
using
the
head
function
note:
medium
can’t
render
tables
—
the
full
example
is
here
another
useful
function
that
pandas
provides
out-of-the-box
is
the
describe
function
which
calculates
some
basic
statistics
on
a
data
set
this
is
helpful
to
get
a
feel
for
the
data
during
the
exploratory
analysis
stage
of
a
project
note:
medium
can’t
render
tables
—
the
full
example
is
here
examining
stats
about
your
data
can
be
helpful
but
sometimes
you
need
to
find
ways
to
visualize
it
too
fortunately
this
data
set
only
has
one
dependent
variable
so
we
can
toss
it
in
a
scatter
plot
to
get
a
better
idea
of
what
it
looks
like
we
can
use
the
plot
function
provided
by
pandas
for
this
which
is
really
just
a
wrapper
for
matplotlib
it
really
helps
to
actually
look
at
what’s
going
on
doesn’t
it?
we
can
clearly
see
that
there’s
a
cluster
of
values
around
cities
with
smaller
populations
and
a
somewhat
linear
trend
of
increasing
profit
as
the
size
of
the
city
increases
now
let’s
get
to
the
fun
part
—
implementing
a
linear
regression
algorithm
in
python
from
scratch!
if
you’re
not
familiar
with
linear
regression
it’s
an
approach
to
modeling
the
relationship
between
a
dependent
variable
and
one
or
more
independent
variables
if
there’s
one
independent
variable
then
it’s
called
simple
linear
regression
and
if
there’s
more
than
one
independent
variable
then
it’s
called
multiple
linear
regression
there
are
lots
of
different
types
and
variances
of
linear
regression
that
are
outside
the
scope
of
this
discussion
so
i
won’t
go
into
that
here
but
to
put
it
simply
—
we’re
trying
to
create
a
*linear
model*
of
the
data
x
using
some
number
of
parameters
theta
that
describes
the
variance
of
the
data
such
that
given
a
new
data
point
that’s
not
in
x
we
could
accurately
predict
what
the
outcome
y
would
be
without
actually
knowing
what
y
is
in
this
implementation
we’re
going
to
use
an
optimization
technique
called
gradient
descent
to
find
the
parameters
theta
if
you’re
familiar
with
linear
algebra
you
may
be
aware
that
there’s
another
way
to
find
the
optimal
parameters
for
a
linear
model
called
the
normal
equation
which
basically
solves
the
problem
at
once
using
a
series
of
matrix
calculations
however
the
issue
with
this
approach
is
that
it
doesn’t
scale
very
well
for
large
data
sets
in
contrast
we
can
use
variants
of
gradient
descent
and
other
optimization
methods
to
scale
to
data
sets
of
unlimited
size
so
for
machine
learning
problems
this
approach
is
more
practical
okay
that’s
enough
theory
let’s
write
some
code
the
first
thing
we
need
is
a
cost
function
the
cost
function
evaluates
the
quality
of
our
model
by
calculating
the
error
between
our
model’s
prediction
for
a
data
point
using
the
model
parameters
and
the
actual
data
point
for
example
if
the
population
for
a
given
city
is
4
and
we
predicted
that
it
was
7
our
error
is
7–4^2
=
3^2
=
9
assuming
an
l2
or
least
squares
loss
function
we
do
this
for
each
data
point
in
x
and
sum
the
result
to
get
the
cost
here’s
the
function:
notice
that
there
are
no
loops
we’re
taking
advantage
of
numpy’s
linear
algrebra
capabilities
to
compute
the
result
as
a
series
of
matrix
operations
this
is
far
more
computationally
efficient
than
an
unoptimizted
for
loop
in
order
to
make
this
cost
function
work
seamlessly
with
the
pandas
data
frame
we
created
above
we
need
to
do
some
manipulating
first
we
need
to
insert
a
column
of
1s
at
the
beginning
of
the
data
frame
in
order
to
make
the
matrix
operations
work
correctly
i
won’t
go
into
detail
on
why
this
is
needed
but
it’s
in
the
exercise
text
if
you’re
interested
—
basically
it
accounts
for
the
intercept
term
in
the
linear
equation
second
we
need
to
separate
our
data
into
independent
variables
x
and
our
dependent
variable
y
finally
we’re
going
to
convert
our
data
frames
to
numpy
matrices
and
instantiate
a
parameter
matirx
one
useful
trick
to
remember
when
debugging
matrix
operations
is
to
look
at
the
shape
of
the
matrices
you’re
dealing
with
it’s
also
helpful
to
remember
when
walking
through
the
steps
in
your
head
that
matrix
multiplications
look
like
i
x
j
*
j
x
k
=
i
x
k
where
i
j
and
k
are
the
shapes
of
the
relative
dimensions
of
the
matrix
97l
2l
1l
2l
97l
1l
okay
so
now
we
can
try
out
our
cost
function
remember
the
parameters
were
initialized
to
0
so
the
solution
isn’t
optimal
yet
but
we
can
see
if
it
works
32072733877455676
so
far
so
good
now
we
need
to
define
a
function
to
perform
gradient
descent
on
the
parameters
*theta*
using
the
update
rules
defined
in
the
exercise
text
here’s
the
function
for
gradient
descent:
the
idea
with
gradient
descent
is
that
for
each
iteration
we
compute
the
gradient
of
the
error
term
in
order
to
figure
out
the
appropriate
direction
to
move
our
parameter
vector
in
other
words
we’re
calculating
the
changes
to
make
to
our
parameters
in
order
to
reduce
the
error
thus
bringing
our
solution
closer
to
the
optimal
solution
ie
best
fit
this
is
a
fairly
complex
topic
and
i
could
easily
devote
a
whole
blog
post
just
to
discussing
gradient
descent
if
you’re
interested
in
learning
more
i
would
recommend
starting
with
this
article
and
branching
out
from
there
once
again
we’re
relying
on
numpy
and
linear
algebra
for
our
solution
you
may
notice
that
my
implementation
is
not
100%
optimal
in
particular
there’s
a
way
to
get
rid
of
that
inner
loop
and
update
all
of
the
parameters
at
once
i’ll
leave
it
up
to
the
reader
to
figure
it
out
for
now
i’ll
cover
it
in
a
later
post
now
that
we’ve
got
a
way
to
evaluate
solutions
and
a
way
to
find
a
good
solution
it’s
time
to
apply
this
to
our
data
set
matrix[[-324140214
11272942
]]
note
that
we’ve
initialized
a
few
new
variables
here
if
you
look
closely
at
the
gradient
descent
function
it
has
parameters
called
alpha
and
iters
alpha
is
the
learning
rate
—
it’s
a
factor
in
the
update
rule
for
the
parameters
that
helps
determine
how
quickly
the
algorithm
will
converge
to
the
optimal
solution
iters
is
just
the
number
of
iterations
there
is
no
hard
and
fast
rule
for
how
to
initialize
these
parameters
and
typically
some
trial-and-error
is
involved
we
now
have
a
parameter
vector
descibing
what
we
believe
is
the
optimal
linear
model
for
our
data
set
one
quick
way
to
evaluate
just
how
good
our
regression
model
is
might
be
to
look
at
the
total
error
of
our
new
solution
on
the
data
set:
45159555030789118
that’s
certainly
a
lot
better
than
32
but
it’s
not
a
very
intuitive
way
to
look
at
it
fortunately
we
have
some
other
techniques
at
our
disposal
we’re
now
going
to
use
matplotlib
to
visualize
our
solution
remember
the
scatter
plot
from
before?
let’s
overlay
a
line
representing
our
model
on
top
of
a
scatter
plot
of
the
data
to
see
how
well
it
fits
we
can
use
numpy’s
linspace
function
to
create
an
evenly-spaced
series
of
points
within
the
range
of
our
data
and
then
evaluate
those
points
using
our
model
to
see
what
the
expected
profit
would
be
we
can
then
turn
it
into
a
line
graph
and
plot
it
not
bad!
our
solution
looks
like
and
optimal
linear
model
of
the
data
set
since
the
gradient
decent
function
also
outputs
a
vector
with
the
cost
at
each
training
iteration
we
can
plot
that
as
well
notice
that
the
cost
always
decreases
—
this
is
an
example
of
what’s
called
a
convex
optimization
problem
if
you
were
to
plot
the
entire
solution
space
for
the
problem
ie
plot
the
cost
as
a
function
of
the
model
parameters
for
every
possible
value
of
the
parameters
you
would
see
that
it
looks
like
a
bowl
shape
with
a
basin
representing
the
optimal
solution
that’s
all
for
now!
in
part
2
we’ll
finish
off
the
first
exercise
by
extending
this
example
to
more
than
1
variable
i’ll
also
show
how
the
above
solution
can
be
reached
by
using
a
popular
machine
learning
library
called
scikit-learn
to
comment
on
this
article
check
out
the
original
post
at
curious
insight
follow
me
on
twitter
to
get
new
post
updates
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
data
scientist
engineer
author
investor
entrepreneur
""
ningning
hu
|
pinterest
engineer
discovery
the
core
value
of
pinterest
is
to
help
people
find
the
things
they
care
about
by
connecting
them
to
pins
and
people
that
relate
to
their
interests
we’re
building
a
service
that’s
powered
by
people
and
supercharged
with
technology
the
interest
graph
—
the
connections
that
make
up
the
pinterest
index
—
creates
bridges
between
pins
boards
and
pinners
it’s
our
job
to
build
a
system
that
helps
people
to
collect
the
things
they
love
and
connect
them
to
communities
of
engaged
people
who
share
similar
interests
and
can
help
them
discover
more
from
categories
like
travel
fitness
and
humor
to
more
niche
areas
like
vintage
motorcycles
craft
beer
or
japanese
architecture
we’re
building
a
visual
discovery
tool
for
all
interests
the
interests
platform
is
built
to
support
this
vision
specifically
it’s
responsible
for
producing
high
quality
data
on
interests
interest
relationships
and
their
association
with
pins
boards
and
pinners
figure
1:
feedback
loop
between
machine
intelligence
and
human
curation
in
contrast
with
conventional
methods
of
generating
such
data
which
rely
primarily
on
machine
learning
and
data
mining
techniques
our
system
relies
heavily
on
human
curation
the
ultimate
goal
is
to
build
a
system
that’s
both
machine
and
human
powered
creating
a
feedback
mechanism
by
which
human
curated
data
helps
drive
improvements
in
our
machine
algorithms
and
vice
versa
figure
2:
system
components
raw
input
to
the
system
includes
existing
data
about
pins
boards
pinners
and
search
queries
as
well
as
explicit
human
curation
signals
about
interests
with
this
data
we’re
able
to
construct
a
continuously
evolving
interest
dictionary
which
provides
the
foundation
to
support
other
key
components
such
as
interest
feeds
interest
recommendations
and
related
interests
from
a
technology
standpoint
interests
are
text
strings
that
represent
entities
for
which
a
group
of
pinners
might
have
a
shared
passion
we
generated
an
initial
collection
of
interests
by
extracting
frequently
occurring
n-grams
from
pin
and
board
descriptions
as
well
as
board
titles
and
filtering
these
n-grams
using
custom
built
grammars
while
this
approach
provided
a
high
coverage
set
of
interests
we
found
many
terms
to
be
malformed
phrases
for
instance
we
would
extract
phrases
such
as
lamborghini
yellow
instead
of
yellow
lamborghini
this
proved
problematic
because
we
wanted
interest
terms
to
represent
how
pinners
would
describe
them
and
so
we
employed
a
variety
of
methods
to
eliminate
malformed
interests
terms
we
first
compared
terms
with
repeated
search
queries
performed
by
a
group
of
pinners
over
a
few
months
intuitively
this
criterion
matches
well
with
the
notion
that
an
interest
should
be
an
entity
for
which
a
group
of
pinners
are
passionate
later
we
filtered
the
candidate
set
through
public
domain
ontologies
like
wikipedia
titles
these
ontologies
were
primarily
used
to
validate
proper
nouns
as
opposed
to
common
phrases
as
all
available
ontologies
represented
only
a
subset
of
possible
interests
this
is
especially
true
for
pinterest
where
pinners
themselves
curate
special
interests
like
mid
century
modern
style
finally
we
also
maintain
an
internal
blacklist
to
filter
abusive
words
and
x-rated
terms
as
well
as
pinterest
specific
stop
words
like
love
this
filtering
is
especially
important
to
interest
terms
which
might
be
recommended
to
millions
of
users
we
arrived
at
a
fair
quality
collection
of
interests
following
the
above
algorithmic
approaches
in
order
to
understand
the
quality
of
our
efforts
we
gave
a
50000
term
subset
of
our
collection
to
a
third
party
vendor
which
used
crowdsourcing
to
rate
our
data
to
be
rigorous
we
composed
a
set
of
four
criteria
by
which
users
would
evaluate
candidate
interests
terms:
-
is
it
english?
-
is
it
a
valid
phrase
in
grammar?
-
is
it
a
standalone
concept?
-
is
it
a
proper
name?
the
crowdsourced
ratings
were
both
interesting
if
not
somewhat
expected
there
was
a
low
rate
of
agreement
amongst
raters
with
especially
high
discrepancy
in
determining
whether
an
interest’s
term
represented
a
standalone
concept
despite
the
ambiguity
we
were
able
to
confirm
that
80%
of
the
collection
generated
using
the
above
algorithms
satisfied
our
interests
criteria
this
type
of
effort
however
is
not
easy
to
scale
the
real
solution
is
to
allow
pinners
to
provide
both
implicit
and
explicit
signals
to
help
us
determine
the
validity
of
an
interest
implicit
signals
behaviors
like
clicking
and
viewing
while
explicit
signals
include
asking
pinners
to
specifically
provide
information
which
can
be
actions
like
a
thumbs
upthumbs
down
starring
or
skipping
recommendations
to
capture
all
the
signals
used
for
defining
the
collections
of
terms
we
built
a
dictionary
that
stores
all
the
data
associated
with
each
interest
including
invalid
interests
and
the
reason
why
it’s
invalid
this
service
plays
a
key
role
in
human
curation
by
aggregating
signals
from
different
people
on
top
of
this
dictionary
service
we
can
build
different
levels
of
reviewing
system
with
the
interests
dictionary
we
can
associate
pins
boards
and
pinners
with
representative
interests
one
of
the
initial
ways
we
experimented
with
this
was
launching
a
preview
of
a
page
where
pinners
can
explore
their
interests
figure
3:
exploring
interests
in
order
to
match
interests
to
pinners
we
need
to
aggregate
all
the
information
related
with
a
person’s
interests
at
its
core
our
system
recommends
interests
based
upon
pins
with
which
a
pinner
interacts
every
pin
on
pinterest
has
been
collected
and
given
context
by
someone
who
thinks
it’s
important
and
in
doing
so
is
helping
other
people
discover
great
content
each
individual
pin
is
an
incredibly
rich
source
of
data
as
discussed
in
a
previous
blog
post
on
discovery
data
model
one
pin
often
has
multiple
copies
—
different
people
may
pin
it
from
different
sources
and
the
same
pin
can
be
repinned
multiple
times
during
this
process
each
pin
accumulates
numerous
unique
textual
descriptions
which
allows
us
to
connect
pins
with
interests
terms
with
high
precision
however
this
conceptually
simple
process
requires
non-trivial
engineering
effort
to
scale
to
the
amount
of
pins
and
pinners
that
the
service
has
today
the
data
process
pipeline
managed
by
pinball
composes
over
35
hadoop
jobs
and
runs
periodically
to
update
the
user-interest
mapping
to
capture
users’
latest
interest
information
the
initial
feedback
on
the
explore
interests
page
has
been
positive
proving
the
capabilities
of
our
system
we’ll
continue
testing
different
ways
of
exposing
a
person’s
interests
and
related
content
based
on
implicit
signals
as
well
as
explicit
signals
such
as
the
ability
to
create
custom
categories
of
interests
related
interests
are
an
important
way
of
enabling
the
ability
to
browse
interests
and
discover
new
ones
to
compute
related
interests
we
simply
combine
the
co-occurrence
relationship
for
interests
computed
at
pin
and
board
levels
figure
4:
computing
related
interests
the
quality
of
the
related
interests
is
surprisingly
high
given
the
simplicity
of
the
algorithm
we
attribute
this
effect
to
the
cleanness
of
pinterest
data
text
data
on
pins
tend
to
be
very
concise
and
contain
less
noise
than
other
types
of
data
like
web
pages
also
related
interests
calculation
already
makes
use
of
boards
which
are
heavily
curated
by
people
vs
machines
in
regards
to
organizing
related
content
we
find
that
utilizing
the
co-occurrence
of
interest
terms
at
the
level
of
both
pins
and
boards
provides
the
best
tradeoff
between
achieving
high
precision
as
well
as
recall
when
computing
the
related
interests
one
of
the
initial
ways
we
began
showing
people
related
content
was
through
related
pins
when
you
pin
an
object
you’ll
see
a
recommendation
for
a
related
board
with
that
same
pin
so
you
can
explore
similar
objects
additionally
if
you
scroll
beneath
a
pin
you’ll
see
pins
from
other
people
who’ve
also
pinned
that
original
object
at
this
point
90%
of
all
pins
have
related
pins
and
we’ve
seen
20%
growth
in
engagement
with
related
pins
in
the
last
six
months
interests
feeds
provide
pinners
with
a
continuous
feed
of
pins
that
are
highly
related
our
feeds
are
populated
using
a
variety
of
sources
including
search
and
through
our
annotation
pipeline
a
key
property
of
the
feed
is
flow
only
feeds
with
decent
flow
can
attract
pinners
to
come
back
repeatedly
thereby
maintaining
high
engagement
in
order
to
optimize
for
our
feeds
we’ve
utilized
a
number
of
real-time
indexing
and
retrieval
systems
including
real-time
search
real-time
annotating
and
also
human
curation
for
some
of
the
interests
to
ensure
quality
we
need
to
guarantee
quality
from
all
sources
for
that
purpose
we
measure
the
engagement
of
pins
from
each
source
and
address
quality
issue
accordingly
figure
5:
how
interest
feeds
are
generated
accurately
capturing
pinner
interests
and
interest
relationships
and
making
this
data
understandable
and
actionable
for
tens
of
millions
of
people
collecting
tens
of
billions
of
pins
is
not
only
an
engineering
challenge
but
also
a
product
design
one
we’re
just
at
the
beginning
as
we
continue
to
improve
the
data
and
design
ways
to
empower
people
to
provide
feedback
that
allows
us
to
build
a
hybrid
system
combining
machine
and
human
curation
to
power
discovery
results
of
these
effort
will
be
reflected
in
future
product
releases
if
you’re
interested
in
building
new
ways
of
helping
people
discover
the
things
they
care
about
join
our
team!
acknowledgements:
the
core
team
members
for
the
interests
backend
platform
are
ningning
hu
leon
lin
ryan
shih
and
yuan
wei
many
other
folks
from
other
parts
of
the
company
especially
the
discovery
team
and
the
infrastructure
teams
have
provided
very
useful
feedback
and
help
along
the
way
to
make
the
ongoing
project
successful
ningning
hu
is
an
engineer
at
pinterest
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
inventive
engineers
building
the
first
visual
discovery
engine
100
billion
ideas
and
counting
https:careerspinterestcomcareersengineering
""
what
machine
learning
teaches
us
about
ourselves
originally
published
at
blogarimocomfollow
me
on
twitter
to
keep
informed
of
interesting
developments
on
these
topics
science
often
follows
technology
because
inventions
give
us
new
ways
to
think
about
the
world
and
new
phenomena
in
need
of
explanation
or
so
aram
harrow
an
mit
physics
professor
counter-intuitively
argues
in
why
now
is
the
right
time
to
study
quantum
computing
he
suggests
that
the
scientific
idea
of
entropy
could
not
really
be
conceived
until
steam
engine
technology
necessitated
understanding
of
thermodynamics
quantum
computing
similarly
arose
from
attempts
to
simulate
quantum
mechanics
on
ordinary
computers
so
what
does
all
this
have
to
do
with
machine
learning?
much
like
steam
engines
machine
learning
is
a
technology
intended
to
solve
specific
classes
of
problems
yet
results
from
the
field
are
indicating
intriguing—possibly
profound—scientific
clues
about
how
our
own
brains
might
operate
perceive
and
learn
the
technology
of
machine
learning
is
giving
us
new
ways
to
think
about
the
science
of
human
thought
""
and
imagination
five
years
ago
deep
learning
pioneer
geoff
hinton
who
currently
splits
his
time
between
the
university
of
toronto
and
google
published
the
following
demo
hinton
had
trained
a
five-layer
neural
network
to
recognize
handwritten
digits
when
given
their
bitmapped
images
it
was
a
form
of
computer
vision
one
that
made
handwriting
machine-readable
but
unlike
previous
works
on
the
same
topic
where
the
main
objective
is
simply
to
recognize
digits
hinton’s
network
could
also
run
in
reverse
that
is
given
the
concept
of
a
digit
it
can
regenerate
images
corresponding
to
that
very
concept
we
are
seeing
quite
literally
a
machine
imagining
an
image
of
the
concept
of
8
the
magic
is
encoded
in
the
layers
between
inputs
and
outputs
these
layers
act
as
a
kind
of
associative
memory
mapping
back-and-forth
from
image
and
concept
from
concept
to
image
all
in
one
neural
network
but
beyond
the
simplistic
brain-inspired
machine
vision
technology
here
the
broader
scientific
question
is
whether
this
is
how
human
imagination
—
visualization
—
works
if
so
there’s
a
huge
a-ha
moment
here
after
all
isn’t
this
something
our
brains
do
quite
naturally?
when
we
see
the
digit
4
we
think
of
the
concept
4
conversely
when
someone
says
8
we
can
conjure
up
in
our
minds’
eye
an
image
of
the
digit
8
is
it
all
a
kind
of
running
backwards
by
the
brain
from
concept
to
images
or
sound
smell
feel
etc
through
the
information
encoded
in
the
layers?
aren’t
we
watching
this
network
create
new
pictures
—
and
perhaps
in
a
more
advanced
version
even
new
internal
connections
—
as
it
does
so?
if
visual
recognition
and
imagination
are
indeed
just
back-and-forth
mapping
between
images
and
concepts
what’s
happening
between
those
layers?
do
deep
neural
networks
have
some
insight
or
analogies
to
offer
us
here?
let’s
first
go
back
234
years
to
immanuel
kant’s
critique
of
pure
reason
in
which
he
argues
that
intuition
is
nothing
but
the
representation
of
phenomena
kant
railed
against
the
idea
that
human
knowledge
could
be
explained
purely
as
empirical
and
rational
thought
it
is
necessary
he
argued
to
consider
intuitions
in
his
definitions
intuitions
are
representations
left
in
a
person’s
mind
by
sensory
perceptions
where
as
concepts
are
descriptions
of
empirical
objects
or
sensory
data
together
these
make
up
human
knowledge
fast
forwarding
two
centuries
later
berkeley
cs
professor
alyosha
efros
who
specializes
in
visual
understanding
pointed
out
that
there
are
many
more
things
in
our
visual
world
than
we
have
words
to
describe
them
with
using
word
labels
to
train
models
efros
argues
exposes
our
techniques
to
a
language
bottleneck
there
are
many
more
un-namable
intuitions
than
we
have
words
for
in
training
deep
networks
such
as
the
seminal
cat-recognition
work
led
by
quoc
le
at
googlestanford
we’re
discovering
that
the
activations
in
successive
layers
appear
to
go
from
lower
to
higher
conceptual
levels
an
image
recognition
network
encodes
bitmaps
at
the
lowest
layer
then
apparent
corners
and
edges
at
the
next
layer
common
shapes
at
the
next
and
so
on
these
intermediate
layers
don’t
necessarily
have
any
activations
corresponding
to
explicit
high-level
concepts
like
cat
or
dog
yet
they
do
encode
a
distributed
representation
of
the
sensory
inputs
only
the
final
output
layer
has
such
a
mapping
to
human-defined
labels
because
they
are
constrained
to
match
those
labels
therefore
the
above
encodings
and
labels
seem
to
correspond
to
exactly
what
kant
referred
to
as
intuitions
and
concepts
in
yet
another
example
of
machine
learning
technology
revealing
insights
about
human
thought
the
network
diagram
above
makes
you
wonder
whether
this
is
how
the
architecture
of
intuition
—
albeit
vastly
simplified
—
is
being
expressed
if
—
as
efros
has
pointed
out
—
there
are
a
lot
more
conceptual
patterns
than
words
can
describe
then
do
words
constrain
our
thoughts?
this
question
is
at
the
heart
of
the
sapir-whorf
or
linguistic
relativity
hypothesis
and
the
debate
about
whether
language
completely
determines
the
boundaries
of
our
cognition
or
whether
we
are
unconstrained
to
conceptualize
anything
—
regardless
of
the
languages
we
speak
in
its
strongest
form
the
hypothesis
posits
that
the
structure
and
lexicon
of
languages
constrain
how
one
perceives
and
conceptualizes
the
world
one
of
the
most
striking
effects
of
this
is
demonstrated
in
the
color
test
shown
here
when
asked
to
pick
out
the
one
square
with
a
shade
of
green
that’s
distinct
from
all
the
others
the
himba
people
of
northern
namibia
—
who
have
distinct
words
for
the
two
shades
of
green
—
can
find
it
almost
instantly
the
rest
of
us
however
have
a
much
harder
time
doing
so
the
theory
is
that
—
once
we
have
words
to
distinguish
one
shade
from
another
our
brains
will
train
itself
to
discriminate
between
the
shades
so
the
difference
would
become
more
and
more
obvious
over
time
in
seeing
with
our
brain
not
with
our
eyes
language
drives
perception
with
machine
learning
we
also
observe
something
similar
in
supervised
learning
we
train
our
models
to
best
match
images
or
text
audio
etc
against
provided
labels
or
categories
by
definition
these
models
are
trained
to
discriminate
much
more
effectively
between
categories
that
have
provided
labels
than
between
other
possible
categories
for
which
we
have
not
provided
labels
when
viewed
from
the
perspective
of
supervised
machine
learning
this
outcome
is
not
at
all
surprising
so
perhaps
we
shouldn’t
be
too
surprised
by
the
results
of
the
color
experiment
above
either
language
does
indeed
influence
our
perception
of
the
world
in
the
same
way
that
labels
in
supervised
machine
learning
influence
the
model’s
ability
to
discriminate
among
categories
and
yet
we
also
know
that
labels
are
not
strictly
required
to
discriminate
between
cues
in
google’s
cat-recognizing
brain
the
network
eventually
discovers
the
concept
of
cat
dog
etc
all
by
itself
—
even
without
training
the
algorithm
against
explicit
labels
after
this
unsupervised
training
whenever
the
network
is
fed
an
image
belonging
to
a
certain
category
like
cats
the
same
corresponding
set
of
cat
neurons
always
gets
fired
up
simply
by
looking
at
the
vast
set
of
training
images
this
network
has
discovered
the
essential
patterns
of
each
category
as
well
as
the
differences
of
one
category
vs
another
in
the
same
way
an
infant
who
is
repeatedly
shown
a
paper
cup
would
soon
recognize
the
visual
pattern
of
such
a
thing
even
before
it
ever
learns
the
words
paper
cup
to
attach
that
pattern
to
a
name
in
this
sense
the
strong
form
of
the
sapir-whorf
hypothesis
cannot
be
entirely
correct
—
we
can
and
do
discover
concepts
even
without
the
words
to
describe
them
supervised
and
unsupervised
machine
learning
turn
out
to
represent
the
two
sides
of
the
controversy’s
coin
and
if
we
recognized
them
as
such
perhaps
sapir-whorf
would
not
be
such
a
controversy
and
more
of
a
reflection
of
supervised
and
unsupervised
human
learning
i
find
these
correspondences
deeply
fascinating
—
and
we’ve
only
scratched
the
surface
philosophers
psychologists
linguists
and
neuroscientists
have
studied
these
topics
for
a
long
time
the
connection
to
machine
learning
and
computer
science
is
more
recent
especially
with
the
advances
in
big
data
and
deep
learning
when
fed
with
huge
amounts
of
text
images
or
audio
data
the
latest
deep
learning
architectures
are
demonstrating
near
or
even
better-than-human
performance
in
language
translation
image
classification
and
speech
recognition
every
new
discovery
in
machine
learning
demystifies
a
bit
more
of
what
may
be
going
on
in
our
brains
we’re
increasingly
able
to
borrow
from
the
vocabulary
of
machine
learning
to
talk
about
our
minds
thanks
to
sonal
chokshi
and
vu
pham
for
extensive
review
""
edits
also
chrisjagers
chickamade
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
@arimoinc
ceo
""
co-founder
leader
entrepreneur
hacker
xoogler
executive
professor
#dataviz
#parallelcomputing
#deeplearning
""
former
#googleapps
fundamentals
and
latest
developments
in
#deeplearning
""
getting
into
machine
learning
ml
can
seem
like
an
unachievable
task
from
the
outside
however
after
dedicating
one
week
to
learning
the
basics
of
the
subject
i
found
it
to
be
much
more
accessible
than
i
anticipated
this
article
is
intended
to
give
others
who’re
interested
in
getting
into
ml
a
roadmap
of
how
to
get
started
drawing
from
the
experiences
i
made
in
my
intro
week
before
my
machine
learning
week
i
had
been
reading
about
the
subject
for
a
while
and
had
gone
through
half
of
andrew
ng’s
course
on
coursera
and
a
few
other
theoretical
courses
so
i
had
a
tiny
bit
of
conceptual
understanding
of
ml
though
i
was
completely
unable
to
transfer
any
of
my
knowledge
into
code
this
is
what
i
wanted
to
change
i
wanted
to
be
able
to
solve
problems
with
ml
by
the
end
of
the
week
even
through
this
meant
skipping
a
lot
of
fundamentals
and
going
for
a
top-down
approach
instead
of
bottoms
up
after
asking
for
advice
on
hacker
news
i
came
to
the
conclusion
that
python’s
scikit
learn-module
was
the
best
starting
point
this
module
gives
you
a
wealth
of
algorithms
to
choose
from
reducing
the
actual
machine
learning
to
a
few
lines
of
code
i
started
off
the
week
by
looking
for
video
tutorials
which
involved
scikit
learn
i
finally
landed
on
sentdex’s
tutorial
on
how
to
use
ml
for
investing
in
stocks
which
gave
me
the
necessary
knowledge
to
move
on
to
the
next
step
the
good
thing
about
the
sentdex
tutorial
is
that
the
instructor
takes
you
through
all
the
steps
of
gathering
the
data
as
you
go
along
you
realize
that
fetching
and
cleaning
up
the
data
can
be
much
more
time
consuming
than
doing
the
actually
machine
learning
so
the
ability
to
write
scripts
to
scrape
data
from
files
or
crawl
the
web
are
essential
skills
for
aspiring
machine
learning
geeks
i
have
re-watched
several
of
the
videos
later
on
to
help
me
when
i’ve
been
stuck
with
problems
so
i’d
recommend
you
to
do
the
same
however
if
you
already
know
how
to
scrape
data
from
websites
this
tutorial
might
not
be
the
perfect
fit
as
a
lot
of
the
videos
evolve
around
data
fetching
in
that
case
the
udacity’s
intro
to
machine
learning
might
be
a
better
place
to
start
tuesday
i
wanted
to
see
if
i
could
use
what
i
had
learned
to
solve
an
actual
problem
as
another
developer
in
my
coding
cooperative
was
working
on
bank
of
england’s
data
visualization
competition
i
teamed
up
with
him
to
check
out
the
datasets
the
bank
has
released
the
most
interesting
data
was
their
household
surveys
this
is
an
annual
survey
the
bank
perform
on
a
few
thousand
households
regarding
money
related
subjects
the
problem
we
decided
to
solve
was
the
following:
i
played
around
with
the
dataset
spent
a
few
hours
cleaning
up
the
data
and
used
the
scikit
learn
map
to
find
a
suitable
algorithm
for
the
problem
we
ended
up
with
a
success
ratio
at
around
63%
which
isn’t
impressive
at
all
but
the
machine
did
at
least
manage
to
guess
a
little
better
than
flipping
a
coin
which
would
have
given
a
success
rate
at
50%
seeing
results
is
like
fuel
to
your
motivation
so
i’d
recommend
you
doing
this
for
yourself
once
you
have
a
basic
grasp
of
how
to
use
scikit
learn
after
playing
around
with
various
scikit
learn
modules
i
decided
to
try
and
write
a
linear
regression
algorithm
from
the
ground
up
i
wanted
to
do
this
because
i
felt
and
still
feel
that
i
really
don’t
understand
what’s
happening
on
under
the
hood
luckily
the
coursera
course
goes
into
detail
on
how
a
few
of
the
algorithms
work
which
came
to
great
use
at
this
point
more
specifically
it
describes
the
underlying
concepts
of
using
linear
regression
with
gradient
descent
this
has
definitely
been
the
most
effective
of
learning
technique
as
it
forces
you
to
understand
the
steps
that
are
going
on
‘under
the
hood’
i
strongly
recommend
you
to
do
this
at
some
point
i
plan
to
rewrite
my
own
implementations
of
more
complex
algorithms
as
i
go
along
but
i
prefer
doing
this
after
i’ve
played
around
with
the
respective
algorithms
in
scikit
learn
on
thursday
i
started
doing
kaggle’s
introductory
tutorials
kaggle
is
a
platform
for
machine
learning
competitions
where
you
can
submit
solutions
to
problems
released
by
companies
or
organizations
""
i
recommend
you
trying
out
kaggle
after
having
a
little
bit
of
a
theoretical
and
practical
understanding
of
machine
learning
you’ll
need
this
in
order
to
start
using
kaggle
otherwise
it
will
be
more
frustrating
than
rewarding
the
bag
of
words
tutorial
guides
you
through
every
steps
you
need
to
take
in
order
to
enter
a
submission
to
a
competition
plus
gives
you
a
brief
and
exciting
introduction
into
natural
language
processing
nlp
i
ended
the
tutorial
with
much
higher
interest
in
nlp
than
i
had
when
entering
it
friday
i
continued
working
on
the
kaggle
tutorials
and
also
started
udacity’s
intro
to
machine
learning
i’m
currently
half
ways
through
and
find
it
quite
enjoyable
it’s
a
lot
easier
the
coursera
course
as
it
doesn’t
go
in
depth
in
the
algorithms
but
it’s
also
more
practical
as
it
teaches
you
scikit
learn
which
is
a
whole
lot
easier
to
apply
to
the
real
world
than
writing
algorithms
from
the
ground
up
in
octave
as
you
do
in
the
coursera
course
doing
it
for
a
week
hasn’t
just
been
great
fun
it
has
also
helped
my
awareness
of
its
usefulness
of
machine
learning
in
society
the
more
i
learn
about
it
the
more
i
see
which
areas
it
can
be
used
to
solve
problems
choose
a
top
down
approach
if
you’re
not
ready
for
the
heavy
stuff
and
get
into
problem
solving
as
quickly
as
possible
good
luck!
thanks
for
reading!
my
name
is
per
i’m
a
co-founder
of
scrimba
—
a
better
way
to
teach
and
learn
code
if
you’ve
read
this
far
i’d
recommend
you
to
check
out
this
demo!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
co-founder
of
scrimba
the
next-generation
platform
for
teaching
and
learning
code
https:scrimbacom
a
publication
about
improving
your
technical
skills
""
by
ahmed
el
deeb
many
technology
companies
now
have
teams
of
smart
data-scientists
versed
in
big-data
infrastructure
tools
and
machine
learning
algorithms
but
every
now
and
then
a
data
set
with
very
few
data
points
turns
up
and
none
of
these
algorithms
seem
to
be
working
properly
anymore
what
the
hell
is
happening?
what
can
you
do
about
it?
most
data
science
relevance
and
machine
learning
activities
in
technology
companies
have
been
focused
around
big
data
and
scenarios
with
huge
data
sets
sets
where
the
rows
represent
documents
users
files
queries
songs
images
etc
things
that
are
in
the
thousands
hundreds
of
thousands
millions
or
even
billions
the
infrastructure
tools
and
algorithms
to
deal
with
these
kinds
of
data
sets
have
been
evolving
very
quickly
and
improving
continuously
during
the
last
decade
or
so
and
most
data
scientists
and
machine
learning
practitioners
have
gained
experience
is
such
situations
have
grown
accustomed
to
the
appropriate
algorithms
and
gained
good
intuitions
about
the
usual
trade-offs
bias-variance
flexibility-stability
hand-crafted
features
vs
feature
learning
etc
but
small
data
sets
still
arise
in
the
wild
every
now
and
then
and
often
they
are
trickier
to
handle
require
a
different
set
of
algorithms
and
a
different
set
of
skills
small
data
sets
arise
is
several
situations:
problems
of
small-data
are
numerous
but
mainly
revolve
around
high
variance:
1-
hire
a
statistician
i’m
not
kidding!
statisticians
are
the
original
data
scientists
the
field
of
statistics
was
developed
when
data
was
much
harder
to
come
by
and
as
such
was
very
aware
of
small-sample
problems
statistical
tests
parametric
models
bootstrapping
and
other
useful
mathematical
tools
are
the
domain
of
classical
statistics
not
modern
machine
learning
lacking
a
good
general-purpose
statistician
get
a
marine-biologist
a
zoologist
a
psychologist
or
anyone
who
was
trained
in
a
domain
that
deals
with
small
sample
experiments
the
closer
to
your
domain
the
better
if
you
don’t
want
to
hire
a
statistician
full
time
on
your
team
make
it
a
temporary
consultation
but
hiring
a
classically
trained
statistician
could
be
a
very
good
investment
2-
stick
to
simple
models
more
precisely:
stick
to
a
limited
set
of
hypotheses
one
way
to
look
at
predictive
modeling
is
as
a
search
problem
from
an
initial
set
of
possible
models
which
is
the
most
appropriate
model
to
fit
our
data?
in
a
way
each
data
point
we
use
for
fitting
down-votes
all
models
that
make
it
unlikely
or
up-vote
models
that
agree
with
it
when
you
have
heaps
of
data
you
can
afford
to
explore
huge
sets
of
modelshypotheses
effectively
and
end
up
with
one
that
is
suitable
when
you
don’t
have
so
many
data
points
to
begin
with
you
need
to
start
from
a
fairly
small
set
of
possible
hypotheses
eg
the
set
of
all
linear
models
with
3
non-zero
weights
the
set
of
decision
trees
with
depth
<=
4
the
set
of
histograms
with
10
equally-spaced
bins
this
means
that
you
rule
out
complex
hypotheses
like
those
that
deal
with
non-linearity
or
feature
interactions
this
also
means
that
you
can’t
afford
to
fit
models
with
too
many
degrees
of
freedom
too
many
weights
or
parameters
whenever
appropriate
use
strong
assumptions
eg
no
negative
weights
no
interaction
between
features
specific
distributions
etc
to
restrict
the
space
of
possible
hypotheses
3-
pool
data
when
possible
are
you
building
a
personalized
spam
filter?
try
building
it
on
top
of
a
universal
model
trained
for
all
users
are
you
modeling
gdp
for
a
specific
country?
try
fitting
your
models
on
gdp
for
all
countries
for
which
you
can
get
data
maybe
using
importance
sampling
to
emphasize
the
country
you’re
interested
in
are
you
trying
to
predict
the
eruptions
of
a
specific
volcano?
""
you
get
the
idea
4-
limit
experimentation
don’t
over-use
your
validation
set
if
you
try
too
many
different
techniques
and
use
a
hold-out
set
to
compare
between
them
be
aware
of
the
statistical
power
of
the
results
you
are
getting
and
be
aware
that
the
performance
you
are
getting
on
this
set
is
not
a
good
estimator
for
out
of
sample
performance
5-
do
clean
up
your
data
with
small
data
sets
noise
and
outliers
are
especially
troublesome
cleaning
up
your
data
could
be
crucial
here
to
get
sensible
models
alternatively
you
can
restrict
your
modeling
to
techniques
especially
designed
to
be
robust
to
outliers
eg
quantile
regression
6-
do
perform
feature
selection
i
am
not
a
big
fan
of
explicit
feature
selection
i
typically
go
for
regularization
and
model
averaging
next
two
points
to
avoid
over-fitting
but
if
the
data
is
truly
limiting
sometimes
explicit
feature
selection
is
essential
wherever
possible
use
domain
expertise
to
do
feature
selection
or
elimination
as
brute
force
approaches
eg
all
subsets
or
greedy
forward
selection
are
as
likely
to
cause
over-fitting
as
including
all
features
7-
do
use
regularization
regularization
is
an
almost-magical
solution
that
constraints
model
fitting
and
reduces
the
effective
degrees
of
freedom
without
reducing
the
actual
number
of
parameters
in
the
model
l1
regularization
produces
models
with
fewer
non-zero
parameters
effectively
performing
implicit
feature
selection
which
could
be
desirable
for
explainability
of
performance
in
production
while
l2
regularization
produces
models
with
more
conservative
closer
to
zero
parameters
and
is
effectively
similar
to
having
strong
zero-centered
priors
for
the
parameters
in
the
bayesian
world
l2
is
usually
better
for
prediction
accuracy
than
l1
8-
do
use
model
averaging
model
averaging
has
similar
effects
to
regularization
is
that
it
reduces
variance
and
enhances
generalization
but
it
is
a
generic
technique
that
can
be
used
with
any
type
of
models
or
even
with
heterogeneous
sets
of
models
the
downside
here
is
that
you
end
up
with
huge
collections
of
models
which
could
be
slow
to
evaluate
or
awkward
to
deploy
to
a
production
system
two
very
reasonable
forms
of
model
averaging
are
bagging
and
bayesian
model
averaging
9-
try
bayesian
modeling
and
model
averaging
again
not
a
favorite
technique
of
mine
but
bayesian
inference
may
be
well
suited
for
dealing
with
smaller
data
sets
especially
if
you
can
use
domain
expertise
to
construct
sensible
priors
10-
prefer
confidence
intervals
to
point
estimates
it
is
usually
a
good
idea
to
get
an
estimate
of
confidence
in
your
prediction
in
addition
to
producing
the
prediction
itself
for
regression
analysis
this
usually
takes
the
form
of
predicting
a
range
of
values
that
is
calibrated
to
cover
the
true
value
95%
of
the
time
or
in
the
case
of
classification
it
could
be
just
a
matter
of
producing
class
probabilities
this
becomes
more
crucial
with
small
data
sets
as
it
becomes
more
likely
that
certain
regions
in
your
feature
space
are
less
represented
than
others
model
averaging
as
referred
to
in
the
previous
two
points
allows
us
to
do
that
pretty
easily
in
a
generic
way
for
regression
classification
and
density
estimation
it
is
also
useful
to
do
that
when
evaluating
your
models
producing
confidence
intervals
on
the
metrics
you
are
using
to
compare
model
performance
is
likely
to
save
you
from
jumping
to
many
wrong
conclusions
this
could
be
a
somewhat
long
list
of
things
to
do
or
try
but
they
all
revolve
around
three
main
themes:
constrained
modeling
smoothing
and
quantification
of
uncertainty
most
figures
used
in
this
post
were
taken
from
the
book
pattern
recognition
and
machine
learning
by
christopher
bishop
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
relevance
engineer
machine
learning
practitioner
and
hobbyist
former
entrepreneur
rants
about
machine
learning
and
its
future
""
data
science
and
machine
learning
have
long
been
interests
of
mine
but
now
that
i’m
working
on
fuzzyai
and
trying
to
make
ai
and
machine
learning
accessible
to
all
developers
i
need
to
keep
on
top
of
all
the
news
in
both
fields
my
preferred
way
to
do
this
is
through
listening
to
podcasts
i’ve
listened
to
a
bunch
of
machine
learning
and
data
science
podcasts
in
the
last
few
months
so
i
thought
i’d
share
my
favorites:
a
great
starting
point
on
some
of
the
basics
of
data
science
and
machine
learning
every
other
week
they
release
a
10–15
minute
episode
where
hosts
kyle
and
linda
polich
give
a
short
primer
on
topics
like
k-means
clustering
natural
language
processing
and
decision
tree
learning
often
using
analogies
related
to
their
pet
parrot
yoshi
this
is
the
only
place
where
you’ll
learn
about
k-means
clustering
via
placement
of
parrot
droppings
website
|
itunes
hosted
by
katie
malone
and
ben
jaffe
of
online
education
startup
udacity
this
weekly
podcast
covers
diverse
topics
in
data
science
and
machine
learning:
teaching
specific
concepts
like
hidden
markov
models
and
how
they
apply
to
real-world
problems
and
datasets
they
make
complex
topics
extremely
accessible
website
|
itunes
each
week
hosts
chris
albon
and
jonathon
morgan
both
experienced
technologists
and
data
scientists
talk
about
the
latest
news
in
data
science
over
drinks
listening
to
partially
derivative
is
a
great
way
to
keep
up
on
the
latest
data
news
website
|
itunes
this
podcast
features
ben
lorica
o’reilly
media’s
chief
data
scientist
speaking
with
other
experts
about
timely
big
data
and
data
science
topics
it
can
often
get
quite
technical
but
the
topics
of
discussion
are
always
really
interesting
website
|
itunes
data
stories
is
a
little
more
focused
on
data
visualization
than
data
science
but
there
is
often
some
interesting
overlap
between
the
topics
every
other
week
enrico
bertini
and
moritz
stefaner
cover
diverse
topics
in
data
with
their
guests
recent
episodes
about
smart
cities
and
nicholas
felton’s
annual
reports
are
particularly
interesting
website
|
itunes
billing
itself
as
a
gentle
introduction
to
artificial
intelligence
and
machine
learning
this
podcast
can
still
get
quite
technical
and
complex
covering
topics
like:
how
to
reason
about
uncertain
events
using
fuzzy
set
theory
and
fuzzy
measure
theory
and
how
to
represent
knowledge
using
logical
rules
website
|
itunes
the
newest
podcasts
on
this
list
with
8
episodes
released
as
of
this
writing
every
other
week
hosts
katherine
gorman
and
ryan
adams
speak
with
a
guest
about
their
work
and
news
stories
related
to
machine
learning
website
|
itunes
feel
i’ve
unfairly
left
a
podcast
off
this
list?
leave
me
a
note
to
let
me
know
published
in
startups
wanderlust
and
life
hacking
-
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
cofounder
of
@fuzzyai
helping
developers
make
their
software
smarter
faster
medium's
largest
publication
for
makers
subscribe
to
receive
our
top
stories
here
→
https:googlzhclji
""
upd
april
20
2016:
scikit
flow
has
been
merged
into
tensorflow
since
version
08
and
now
called
tensorflow
learn
or
tflearn
google
released
a
machine
learning
framework
called
tensorflow
and
it’s
taking
the
world
by
storm
10k
stars
on
github
a
lot
of
publicity
and
general
excitement
in
between
ai
researchers
now
but
how
you
to
use
it
for
something
regular
problem
data
scientist
may
have?
and
if
you
are
ai
researcher
—
we
will
build
up
to
interesting
problems
over
time
a
reasonable
question
why
as
a
data
scientist
who
already
has
a
number
of
tools
in
your
toolbox
r
scikit
learn
etc
you
care
about
yet
another
framework?
the
answer
is
two
part:
let’s
start
with
simple
example
—
take
titanic
dataset
from
kaggle
first
make
sure
you
have
installed
tensorflow
and
scikit
learn
with
few
helpful
libs
including
scikit
flow
that
is
simplifying
a
lot
of
work
with
tensorflow:
you
can
get
dataset
and
the
code
from
http:githubcomilblackdragontf_examples
quick
look
at
the
data
use
ipython
or
ipython
notebook
for
ease
of
interactive
exploration:
let’s
test
how
we
can
predict
survived
class
based
on
float
variables
in
scikit
learn:
we
separate
dataset
into
features
and
target
fill
in
na
in
the
data
with
zeros
and
build
a
logistic
regression
predicting
on
the
training
data
gives
us
some
measure
of
accuracy
of
cause
it
doesn’t
properly
evaluate
the
model
quality
and
test
dataset
should
be
used
but
for
simplicity
we
will
look
at
train
only
for
now
now
using
tflearn
previously
scikit
flow:
congratulations
you
just
built
your
first
tensorflow
model!
tflearn
is
a
library
that
wraps
a
lot
of
new
apis
by
tensorflow
with
nice
and
familiar
scikit
learn
api
tensorflow
is
all
about
a
building
and
executing
graph
this
is
a
very
powerful
concept
but
it
is
also
cumbersome
to
start
with
looking
under
the
hood
of
tflearn
we
just
used
three
parts:
even
as
you
get
more
familiar
with
tensorflow
pieces
of
scikit
flow
will
be
useful
like
graph_actions
and
layers
and
host
of
other
ops
and
tools
see
future
posts
for
examples
of
handling
categorical
variables
text
and
images
part
2
—
deep
neural
networks
custom
tensorflow
models
with
scikit
flow
and
digit
recognition
with
convolutional
networks
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
co-founder
@
nearai
—
teaching
machines
to
code
i’m
tweeting
as
@ilblackdragon
""
it’s
very
common
for
machine
learning
practitioners
to
have
favorite
algorithms
it’s
a
bit
irrational
since
no
algorithm
strictly
dominates
in
all
applications
the
performance
of
ml
algorithms
varies
wildly
depending
on
the
application
and
the
dimensionality
of
the
dataset
and
even
for
a
given
problem
and
a
given
dataset
any
single
model
will
likely
be
beaten
by
an
ensemble
of
diverse
models
trained
by
diverse
algorithms
anyway
but
people
have
favorites
nevertheless
some
like
svms
for
the
elegance
of
their
formulation
or
the
quality
of
the
available
implementations
some
like
decision
rules
for
their
simplicity
and
interpretability
and
some
are
crazy
about
neural
networks
for
their
flexibility
my
favorite
out-of-the-box
algorithm
is
as
you
might
have
guessed
the
random
forest
and
it’s
the
second
modeling
technique
i
typically
try
on
any
given
data
set
after
a
linear
model
this
beautiful
visualization
from
scikit-learn
illustrates
the
modelling
capacity
of
a
decision
forest:
here’s
a
paper
by
leo
breiman
the
inventor
of
the
algorithms
describing
random
forests
here’s
another
amazing
paper
by
rich
caruana
et
al
evaluating
several
supervised
learning
algorithms
on
many
different
datasets
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
relevance
engineer
machine
learning
practitioner
and
hobbyist
former
entrepreneur
rants
about
machine
learning
and
its
future
""
here
are
a
few
things
i
learned
from
the
otto
group
kaggle
competition
i
had
the
chance
to
team
up
with
great
kaggle
master
xavier
conort
and
the
french
community
as
a
whole
has
been
very
active
teaming
with
xavier
has
been
the
opportunity
to
practice
some
ensembling
technics
we
heavily
used
stacking
we
added
to
an
initial
set
of
93
features
new
features
being
the
predictions
of
n
different
classifiers
random
forest
gbm
neural
networks
""
and
then
retrained
p
classifiers
over
the
93
""
n
features
and
finally
made
a
weighted
average
of
the
p
outputs
we
tested
two
tricks
:
this
is
one
of
the
great
functionalities
of
the
last
scikit-learn
version
016
it
allows
to
rescale
the
classifier
predictions
by
taking
observations
predicted
within
a
segments
eg
03–04
and
comparing
to
the
actual
truth
ratio
of
these
observation
eg
023
with
means
that
a
rescaling
is
needed
here
is
a
mini
notebook
explaining
how
to
use
calibration
and
demonstrating
how
well
it
worked
on
the
otto
challenge
data
at
the
beginning
of
the
competition
it
appeared
quickly
that
—
once
again
—
gradient
boosting
trees
was
one
of
the
best
performing
algorithm
provided
that
you
find
the
right
hyper
parameters
on
the
scikit-learn
implementation
most
important
hyper
parameters
are
learning_rate
the
shrinkage
parameter
n_estimators
the
number
of
boosting
stages
and
max_depth
limits
the
number
of
nodes
in
the
tree
the
best
value
depends
on
the
interaction
of
the
input
variables
min_samples_split
and
min_samples_leaf
can
also
be
a
way
to
control
depth
of
the
trees
for
optimal
performance
i
also
discovered
that
two
other
parameters
were
crucial
for
this
competition
i
must
admit
i
never
paid
attention
on
it
before
this
challenge
:
namely
subsample
the
fraction
of
samples
to
be
used
for
fitting
the
individual
base
learners
and
max_features
the
number
of
features
to
consider
when
looking
for
the
best
split
the
problem
was
to
find
a
way
to
quickly
find
the
best
hyperparameters
combination
i
first
discovered
gridsearchcv
that
makes
an
exhaustive
search
over
specified
parameter
ranges
as
always
with
scikit-learn
it
has
a
convenient
programming
interface
handling
for
example
smoothly
cross-validation
and
parallel
distributing
of
search
however
the
number
of
parameters
to
tune
and
their
range
was
too
large
to
discover
the
best
ones
in
the
acceptable
time
frame
i
had
in
mind
typically
while
sleeping
ie
7
to
10
hours
i
had
to
fall
back
to
an
other
option
:
i
then
used
randomizedsearchcv
that
appeared
in
014
version
with
this
method
search
is
done
randomly
on
a
subspace
of
parameters
it
gives
generally
very
good
results
as
described
in
this
paper
and
i
was
able
to
find
a
suitable
parameter
set
within
a
few
hours
note
that
some
competitors
like
french
kaggler
amine
used
hyperopt
for
hyperparameters
optimization
xgboost
is
a
gradient
boosting
implementation
heavily
used
by
kagglers
and
i
now
understand
why
i
never
used
it
before
but
it
was
a
hot
topic
discussed
in
the
forum
i
decided
to
have
a
look
at
it
even
if
its
main
interface
is
in
r
but
there
is
a
python
api
that
i
didn’t
use
yet
xgboost
is
much
faster
than
scikit-learn
and
gave
better
prediction
it
will
remain
for
sure
part
of
my
toolblox
someone
posted
on
the
forum
:
he
was
right
it
has
been
for
me
the
opportunity
to
play
with
neural
networks
for
the
first
time
several
implementations
have
been
used
by
the
competitors
:
h2o
keras
cxxnet
""
i
personally
used
lasagne
main
challenges
was
to
fine
tune
the
number
of
layers
number
of
neurons
dropout
and
learning
rate
here
is
a
notebook
on
what
i
learned
one
of
the
secret
of
the
competition
was
to
run
several
times
the
same
algorithm
with
random
selection
of
observations
and
features
and
take
the
average
of
the
output
to
do
that
easily
i
discovered
the
scikit-learn
baggingclassifier
meta-estimator
it
hides
the
tedious
complexity
of
looping
over
model
fits
random
subsets
selection
and
averaging
—
and
exposes
easy
fit
""
predict_proba
entry
points
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
data
enthusiast
#bigdata
#datascience
#machinelearning
#frenchdata
#kaggle
""
machine
learning
จริงๆแล้วมันคืออะไรกันแน่?
ครั้งแรกที่ผมได้ยินคําคํานี้
ผมก็พูดกับตัวเองในใจ
เครื่องจักรที่เรียนรู้ได้ด้วยตัวเองงั้นเหรอ?
ใครมาถามว่ารู้จัก
machine
learning
หรือเปล่า?
มันคืออะไรอะ?
ผมก็ได้แต่บอกเขาไปว่า
เคยได้ยินแต่ชื่อว่ะ
ผมหวังเป็นอย่างยิ่งว่า
คุณที่หลงเข้ามาอ่านบทความนี้
ที่อาจจะเป็นเหมือนผมที่เคยได้ยินมาแต่ชื่อ
เมื่ออ่านจบ
หากมีใครมาถามว่า
รู้จัก
machine
learning
หรือเปล่า?
มันคืออะไรอะ?
จะมีความมั่นใจมากพอที่จะตอบเขาไปว่า
รู้ดิ
นั่งๆ
เดี๋ยวเล่าให้ฟัง
ก่อนที่จะอธิบายว่า
machine
learning
คืออะไร
ขอโม้สักเล็กน้อย
ให้เห็นความสําคัญของมันเสียก่อน
แล้วจะพบว่า
machine
learning
นี่แทบจะเป็นส่วนหนึ่งของชีวิตประจําวันไปแล้ว
machine
learning
เป็นเรื่องที่ใกล้ตัวเรามากๆ
ยิ่งสําหรับคนที่ใช้
internet
เป็นประจํานั้น
แทบทุกวันจะได้ใช้ประโยชน์จาก
machine
learning
ไม่ว่าจะรู้ตัวหรือไม่ก็ตาม
ยกตัวอย่างเช่น
เมื่อเราต้องการค้นหาอะไรบางอย่างด้วย
google
search
แต่ไม่ค่อยแน่ใจ
คับคล้ายคับคลาว่ามันน่าจะสะกดแบบนี้
machn
leaning
ตัวอย่างโง่นิดนึง
๕๕๕๕๕
ปรากฏว่า
โอ้
พระสงฆ์!
มันเดาใจเราได้
เป็นหมอดูหรืออย่างไร!?
แน่นอนว่าโค้ดก็คงจะไม่ใช่แบบด้านล่างนี้แน่ๆ
แล้วทําไม
google
ถึงได้รู้ใจเรากันนะ?
ยังมีตัวอย่างอีกมากมายที่นํา
machine
learning
ไปใช้
เช่น
spam
filtering
face
recognition
handwriting
recognition
แม้แต่การทํา
marketing
ในปัจจุบันก็เริ่มใช้ประโยชน์
machine
learning
เช่นกัน
อาทิ
การแบ่งกลุ่มลูกค้าcustomer
segmentation
การทํานายการสูญเสียลูกค้าcustomer
churn
prediction
เป็นต้น
และที่จะไม่พูดถึงไม่ได้เลยคือ
facebook’s
friend
suggestions
ที่ผมเองก็ไม่รู้ว่าทําไมมันถึงแนะนําสาวสวยให้ผมอย่างสม่ําเสมอ
แต่สิ่งที่ผมมั่นใจคือ
facebook
ไม่ได้ใช้คนมานั่งเลือกให้แน่ๆ
แล้วมันทําได้ยังไงกัน?
แน่นอนว่า
machine
learning
คือคําตอบ
สมองของมนุษย์นั้นมีความสามารถที่น่าทึ่งมากมาย
เช่น
การตระหนักรู้
อารมณ์ความรู้สึก
ความทรงจํา
ความสามารถในการควบคุมร่างกาย
รวมถึงประสาทสัมผัสทั้งห้าที่ทําให้เรามีความสามารถในการรับรู้
แต่ก็มีปัญหาบางอย่างที่ซับซ้อน
และไม่เหมาะที่จะแก้ปัญหาโดยการใช้สมองของมนุษย์เพียงอย่างเดียว
เมื่อต้องเขียนโปรแกรมที่จัดการกับข้อมูลจํานวนมาก
และมีรูปแบบที่แตกต่างกันออกไป
เป็นเรื่องยากที่เราจะทําความเข้าใจข้อมูลและเขียนโปรแกรมที่จะตอบสนองต่อมัน
เมื่อมีข้อมูลเข้ามาเพิ่มและมีลักษณะที่ต่างไปอีกก็เหมือนกับ
requirement
เปลี่ยนตลอดเวลา
เราก็ต้องวิเคราะห์ข้อมูลใหม่และแก้โปรแกรมของเราเรื่อยๆซึ่งลําบากมาก
arthur
samuel
หนึ่งในผู้บุกเบิก
computer
gaming
artificial
intelligence
และ
machine
learning
ชาวอเมริกัน
ได้นิยาม
machine
learning
เอาไว้ว่า
เป็น
การศึกษาเกี่ยวกับการทําให้คอมพิวเตอร์มีความสามารถที่จะเรียนรู้โดยที่ไม่ต้องเขียนโปรแกรมลงไปตรงๆ
กล่าวคือ
machine
learning
นั้น
ไม่ได้กําหนดลงไปในโปรแกรมว่า
สําหรับลักษณะ
a
b
ใดๆ
หากข้อมูลมีลักษณะแบบ
a
ต้องทําอย่างไร
แบบ
b
ต้องทําอย่างไร
แต่เป็นโปรแกรมที่ทําความเข้าใจความสัมพันธ์ของข้อมูล
แล้วสร้างวิธีการตอบสนองต่อข้อมูลขึ้นมาเอง
ในเมื่อโปรแกรมสามารถเปลี่ยนแปลงวิธีการตอบสนองต่อข้อมูลได้ด้วยตัวเอง
เราจึงไม่จําเป็นต้องคอยวิเคราะห์ข้อมูลและแก้โปรแกรมทุกครั้งที่มีข้อมูลใหม่เข้ามาอีกต่อไป
ตัวผมเองเคยสงสัยว่า
machine
learning
artificial
intelligence
และdata
mining
มันต่างกันยังไง
รู้สึกว่ามันก็เป็นเรื่องที่คล้ายกันมากๆ
แต่แล้วผมก็พบความจริงว่า
จริงๆแล้ว
ทั้ง
ai
artificial
intelligence
และ
data
mining
นั้นนํา
machine
learning
ไปใช้
สิ่งที่ต่างกันก็คือเป้าหมาย
ai
นั้นโฟกัสที่การสร้าง
intelligent
agent
หรือตัวตนที่มีความคิดขึ้นมา
ซึ่งไม่จําเป็นที่จะต้องใช้
machine
learning
ก็ได้
ถึงแม้จะใช้เพียงแค่การ
search
หากสามารถตอบสนองได้อย่างชาญฉลาด
ก็สามารถเรียกว่าเป็น
ai
ได้
แต่ไม่จําเป็น
ไม่ได้แปลว่าไม่ได้นําไปใช้
ในทางกลับกัน
machine
learning
ถูกนําไปใช้ประโยชน์ใน
ai
เยอะมากๆ
โดยถูกใช้เพื่อที่จะสร้างความรู้ใหม่ๆ
และนําไปสู่การตอบสนองต่อเหตุการณ์ที่ต่างออกไปจากที่กําหนดไว้แต่แรก
ส่วน
data
mining
นั้น
เป็นขั้นตอนการวิเคราะห์ใน
knowledge
discovery
หรือการค้นหาความรู้
โดยจะเปลี่ยนจากข้อมูลดิบdataให้เป็นข้อมูลที่ทําความเข้าใจได้information
เพื่อที่จะนําไปใช้ต่อในอนาคต
data
mining
ใช้วิธีการของทั้ง
ai
machine
learning
สถิติ
และ
database
system
ในการได้มาซึ่งข้อมูลเชิงลึก
หรือ
insight
โดยสรุปแล้ว
ทั้ง
3
ศาสตร์นั้นมีความเกี่ยวข้องกันอย่างมาก
และต่างก็นําวิธีการของกันและกันไปใช้
จึงไม่แปลกที่จะรู้สึกว่ามันดูคล้ายๆกัน
เพียงแต่เป้าหมายของมันต่างกัน
ทําให้วิธีการนั้นไม่เหมือนกันซะทีเดียว
machine
learning
algorithm
นั้น
โดยพื้นฐานแล้วจะแบ่งออกได้เป็น
2
ประเภทคือ
supervised
learning
กับ
unsupervised
learning
supervised
learning
คือ
การเรียนรู้ที่ได้รับคําแนะนํา
สมมติว่าเราเกิดเสียความทรงจํา
ไม่สามารถแยกแยะ
แอปเปิ้ล
มะม่วง
และส้มออกจากกันได้
คุณหมอผู้น่ารักจึงเอา
แอปเปิ้ล
มะม่วง
และส้ม
มาให้ดู
ผลไม้ทั้งหมดที่คุณหมอเอามาให้ดูนี้เรียกว่า
training
data
คือข้อมูลที่นํามาใช้ในการฝึกสอน
คุณหมอเริ่มจากการนําแอปเปิ้ลหลากหลายแบบมาให้ดู
และบอกว่า
นี่คือแอปเปิ้ล
นี่เป็นการให้
label
หรือป้ายที่แปะบอกว่าข้อมูลที่ได้มานี้คืออะไร
เมื่อเราได้เห็นแอปเปิ้ลก็จะพบว่า
แอปเปิ้ลนั้นมีสีแดง
หรือสีเขียว
รูปทรงของแอปเปิ้ลนั้นหากผ่าครึ่งจะมีลักษณะคล้ายผีเสื้อ
สิ่งเหล่านี้เรียกว่า
feature
หรือคุณสมบัติของข้อมูล
หลังจากนั้นคุณหมอก็นํามะม่วงและส้มมาให้ดู
และพบว่า
มะม่วงมีสีเขียวหรือเหลือง
รูปทรงค่อนข้างยาว
ส่วนส้มมีสีส้ม
และมีรูปทรงเป็นทรงรี
เมื่อได้ข้อมูลมากเพียงพอ
ก็จะเริ่มแยกแยะ
แอปเปิ้ล
มะม่วง
และส้ม
ออกจากกันได้
ต่อมา
หากเจอส้มเปลือกสีเขียวก็อาจจะเดาได้ว่าเป็นส้ม
เพราะรูปทรงของมัน
นี่เป็นตัวอย่างหนึ่งของ
classification
หรือการจัดหมวดหมู่
เป็น
supervised
learning
แบบหนึ่งที่ใช้กับข้อมูลที่ไม่ต่อเนื่อง
discrete
regression
เป็นการวิเคราะห์สําหรับข้อมูลที่ต่อเนื่อง
continuous
ภาพด้านบนแสดงถึง
linear
regression
หรือ
line
of
best
fit
ซึ่งเป็น
regression
แบบหนึ่ง
จะเห็นได้ว่าเรามี
training
data
อยู่
ซึ่งก็ไม่ได้เรียงตัวกันเป็นเส้นตรง
แต่ก็พอจะเห็นรูปแบบและแนวโน้มของข้อมูล
หากเราลากเส้นโดยพยายามให้ผลรวมของระยะห่างจาก
training
data
ซึ่งเป็นความคลาดเคลื่อนน้อยที่สุด
เราก็จะได้
model
ที่พอจะทํานายค่า
y
ต่อๆไปได้
george
e
p
box
นักสถิติศาสตร์กล่าวไว้ว่า
อย่างที่เห็นในภาพ
เส้นสีน้ําเงินนั่นคือ
model
หรือแบบจําลองเพื่อทํานายค่า
y
ที่มี
x
สูงกว่านี้
แต่จะเห็นได้ว่าแทบไม่มีจุดไหนที่ตรงเป๊ะๆเลย
เราใช้ได้แค่พอทํานายได้คร่าวๆเท่านั้น
หาก
supervised
learning
คือการเรียนรู้ที่มีคําแนะนํา
unsupervised
learning
ก็คือการไปตายเอาดาบหน้า
ไม่มีใครมาแนะนําเรา
แต่คงต้องขอไปลุยซักตั้ง
เมื่อ
supervised
learning
มี
classification
ทางด้าน
unsupervised
learning
ก็จะมี
clustering
ซึ่งหลายคนรวมถึงผมเอง
เมื่อได้รู้จักครั้งแรก
ก็สงสัยว่า
เอ๊ะ
มันต่างกันยังไง
classification
เป็นการจัดหมวดหมู่
ส่วน
clustering
เป็นการจัดกลุ่ม
ฟังๆดูก็คล้ายๆกันอยู่ดี
เช่นนั้นแล้ว
ลองกลับไปสวมบทผู้ป่วยสูญเสียความทรงจํา
กับคุณหมอน่ารักอีกสักครั้งนะครับ
คราวนี้
คุณหมอ
เอาผลไม้มาอีกสามชนิดที่หน้าตาไม่เหมือนทั้งแอปเปิ้ล
มะม่วงและส้ม
แต่คุณหมอไม่ยอมบอกอะไรเกี่ยวกับเจ้าพวกนี้เลยสักนิด
หรือก็คือตอนนี้
ผลไม้เหล่านี้ไม่มี
label
แต่คุณหมอก็สั่งให้เราแยกมันออกมาเป็นสามกลุ่ม
เมื่อเราสังเกต
feature
ของผลไม้พวกนี้ก็จะพอแยกแยะได้ว่าผลไม้ลูกไหนควรจะอยู่กลุ่มเดียวกัน
เมื่อแยกได้สามกลุ่มแล้ว
คุณหมอก็เดินจากไปเสียเฉยๆ
ไม่บอกไม่กล่าวอะไร
สุดท้ายเราก็รู้แค่ว่า
ผลไม้แต่ละลูกอยู่กลุ่มเดียวกับใคร
แต่บอกไม่ได้ว่ามันคืออะไรกันแน่
นี่คือ
clustering
ต่างจาก
classification
ที่บอกเราตั้งแต่แรกว่าผลไม้แต่ละชนิดมีชื่อว่าอะไรบ้าง
หลายครั้งที่ข้อมูลนั้นมี
feature
หลายชนิด
หรือก็คือเป็นข้อมูลที่มีมิติมาก
เมื่อเป็นเช่นนั้นแล้วก็จะเป็นเรื่องยากที่จะแสดงภาพ
หรือ
visualize
ข้อมูล
เราจึงควรลดมิติของข้อมูลลง
โดยพยายามคงความหมายเดิมอยู่
dimensionality
reduction
หรือ
dimension
reduction
เป็นการลดมิติของข้อมูล
ซึ่งนอกจากจะทําให้ง่ายที่จะ
visualize
แล้ว
เมื่อมีมิติที่น้อยลง
นั่นหมายถึงมี
feature
ที่น้อยลง
ซึ่งทําให้
performance
ดีขึ้น
และลด
space
complexity
อีกด้วย
นอกจากประเภทของ
machine
learning
algorithm
แบบ
basic
ที่เขียนไว้ด้านบนแล้วยังมี
semi-supervised
learning
และ
reinforcement
learning
ที่พี่ต้า
@konpat
เขียนเอาไว้ครับ
สุดท้ายนี้
ผมเชื่อว่า
machine
learning
เป็นศาสตร์หนึ่งที่สําคัญมากๆสําหรับวงการคอมพิวเตอร์ในปัจจุบัน
และอนาคต
และส่วนตัวผมคิดว่าศาสตร์นี้มันเท่มากๆเลยนะ
ใครสนใจใน
machine
learning
ก็เข้ามาคุยกันได้นะครับ
:d
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
software
choreographer
at
thoughtworks
functional
programming
devops
and
machine
learning
enthusiast
a
half-score-day-a-story
blog
by
league
of
machine
learning
from
chulalongkorn
university
""
stories
are
a
fundamental
human
tool
that
we
use
to
communicate
thought
creating
a
stories
about
a
image
is
a
difficult
task
that
many
struggle
with
new
machine-learning
experiments
are
enabling
us
to
generate
stories
based
on
the
content
of
images
this
experiment
explores
how
to
generate
little
romantic
stories
about
images
incl
guest
star
taylor
swift
neural-storyteller
is
a
recently
published
experiment
by
ryan
kiros
university
of
toronto
it
combines
recurrent
neural
networks
rnn
skip-thoughts
vectors
and
other
techniques
to
generate
little
story
about
images
neural-storyteller’s
outputs
are
creative
and
often
comedic
it
is
open-source
this
experiment
started
by
running
5000
randomly
selected
web-images
through
neural-storyteller
and
experimenting
with
hyper-parameters
neural-storyteller
comes
with
2
pre-trained
models:
one
trained
on
14
million
passages
of
romance
novels
the
other
trained
on
taylor
swift
lyrics
inputs
and
outputs
were
manually
filtered
and
recombined
into
two
videos
using
romantic
novel
model
voices
generated
with
a
text-to-speech
using
taylor
swift
model
combined
with
a
well
known
swift
instrumental
neural-storyteller
gives
us
a
fascinating
glimpse
into
the
future
of
storytelling
even
though
these
technologies
are
not
fully
mature
yet
the
art
of
storytelling
is
bound
to
change
in
the
near
future
authors
will
be
training
custom
models
combining
styles
across
genres
and
generating
text
with
images
""
sounds
exploring
this
exiting
new
medium
is
rewarding!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
designer
""
code
magician
working
at
the
intersection
of
hci
machine
learning
""
creativity
building
tools
for
enlightenment
narrative
engineering
""
by
bar
ifrach
at
airbnb
we
seek
to
match
people
who
are
looking
for
accommodation
—
guests
—
with
those
looking
to
rent
out
their
place
—
hosts
guests
reach
out
to
hosts
whose
listings
they
wish
to
stay
in
however
a
match
succeeds
only
if
the
host
also
wants
to
accommodate
the
guest
i
first
heard
about
airbnb
in
2012
from
a
friend
he
offered
his
nice
apartment
on
the
site
when
he
traveled
to
see
his
family
during
our
vacations
from
grad
school
his
main
goal
was
to
fit
as
many
booked
nights
as
possible
into
the
1–2
weeks
when
he
was
away
my
friend
would
accept
or
reject
requests
depending
on
whether
or
not
the
request
would
help
him
to
maximize
his
occupancy
about
two
years
later
i
joined
airbnb
as
a
data
scientist
i
remembered
my
friend’s
behavior
and
was
curious
to
discover
what
affects
hosts’
decisions
to
accept
accommodation
requests
and
how
airbnb
could
increase
acceptances
and
matches
on
the
platform
what
started
as
a
small
research
project
resulted
in
the
development
of
a
machine
learning
model
that
learns
our
hosts’
preferences
for
accommodation
requests
based
on
their
past
behavior
for
each
search
query
that
a
guest
enters
on
airbnb’s
search
engine
our
model
computes
the
likelihood
that
relevant
hosts
will
want
to
accommodate
the
guest’s
request
then
we
surface
likely
matches
more
prominently
in
the
search
results
in
our
ab
testing
the
model
showed
about
a
375%
increase
in
booking
conversion
resulting
in
many
more
matches
on
airbnb
in
this
blog
post
i
outline
the
process
that
brought
us
to
this
model
i
kicked
off
my
research
into
hosts’
acceptances
by
checking
if
other
hosts
maximized
their
occupancy
like
my
friend
every
accommodation
request
falls
in
a
sequence
or
in
a
window
of
available
days
in
the
calendar
such
as
on
april
5–10
in
the
calendar
shown
below
the
gray
days
surrounding
the
window
are
either
blocked
by
the
host
or
already
booked
if
accepted
and
booked
a
request
may
leave
the
host
with
a
sub-window
before
the
check-in
date
check-in
gap
—
april
5–7
andor
a
sub-window
after
the
check-out
check-out
gap
—
april
10
a
host
looking
to
have
a
high
occupancy
will
try
to
avoid
such
gaps
indeed
when
i
plotted
hosts’
tendency
to
accept
over
the
sum
of
the
check-in
gap
and
the
check-out
gap
31=
4
in
the
example
above
as
in
the
next
plot
i
found
the
effect
that
i
expected
to
see:
hosts
were
more
likely
to
accept
requests
that
fit
well
in
their
calendar
and
minimize
gap
days
but
do
all
hosts
try
to
maximize
occupancy
and
prefer
stays
with
short
gaps?
perhaps
some
hosts
are
not
interested
in
maximizing
their
occupancy
and
would
rather
host
occasionally
and
maybe
hosts
in
big
markets
like
my
friend
are
different
from
hosts
in
smaller
markets
indeed
when
i
looked
at
listings
from
big
and
small
markets
separately
i
found
that
they
behaved
quite
differently
hosts
in
big
markets
care
a
lot
about
their
occupancy
—
a
request
with
no
gaps
is
almost
6%
likelier
to
be
accepted
than
one
with
7
gap
nights
for
small
markets
i
found
the
opposite
effect
hosts
prefer
to
have
a
small
number
of
nights
between
requests
so
hosts
in
different
markets
have
different
preferences
but
it
seems
likely
that
even
within
a
market
hosts
may
prefer
different
stays
a
similar
story
revealed
itself
when
i
looked
at
hosts’
tendency
to
accept
based
on
other
characteristics
of
the
accommodation
request
for
example
on
average
airbnb
hosts
prefer
accommodation
requests
that
are
at
least
a
week
in
advance
over
last
minute
requests
but
perhaps
some
hosts
prefer
short
notice?
the
plot
below
looks
at
the
dispersion
of
hosts’
preferences
for
last
minute
stays
less
than
7
days
versus
far
in
advance
stays
more
than
7
days
indeed
the
dispersion
in
preferences
reveals
that
some
hosts
like
last
minute
stays
better
than
far
in
advance
stays
—
those
in
the
bottom
right
—
even
though
on
average
hosts
prefer
longer
notice
i
found
similar
dispersion
in
hosts’
tendency
to
accept
other
trip
characteristics
like
the
number
of
guests
whether
it
is
a
weekend
trip
etc
all
these
findings
pointed
to
the
same
conclusion:
if
we
could
promote
in
our
search
results
hosts
who
would
be
more
likely
to
accept
an
accommodation
request
resulting
from
that
search
query
we
would
expect
to
see
happier
guests
and
hosts
and
more
matches
that
turned
into
fun
vacations
or
productive
business
trips
in
other
words
we
could
personalize
our
search
results
but
not
in
the
way
you
might
expect
typically
personalized
search
results
promote
results
that
would
fit
the
unique
preferences
of
the
searcher
—
the
guest
at
a
two-sided
marketplace
like
airbnb
we
also
wanted
to
personalize
search
by
the
preference
of
the
hosts
whose
listings
would
appear
in
the
search
results
encouraged
by
my
findings
i
joined
forces
with
another
data
scientist
and
a
software
engineer
to
create
a
personalized
search
signal
we
set
out
to
associate
hosts’
prior
acceptance
and
decline
decisions
by
the
following
characteristics
of
the
trip:
check-in
date
check-out
date
and
number
of
guests
by
adding
host
preferences
to
our
existing
ranking
model
capturing
guest
preferences
we
hoped
to
enable
more
and
better
matches
at
first
glance
this
seems
like
a
perfect
case
for
collaborative
filtering
—
we
have
users
hosts
and
items
trips
and
we
want
to
understand
the
preference
for
those
items
by
combining
historical
ratings
acceptdecline
with
statistical
learning
from
similar
hosts
however
the
application
does
not
fully
fit
in
the
collaborative
filtering
framework
for
two
reasons
with
these
points
in
mind
we
decided
to
massage
the
problem
into
something
resembling
collaborative
filtering
we
used
the
multiplicity
of
responses
for
the
same
trip
to
reduce
the
noise
coming
from
the
latent
factors
in
the
guest-host
interaction
to
do
so
we
considered
hosts’
average
response
to
a
certain
trip
characteristic
in
isolation
instead
of
looking
at
the
combination
of
trip
length
size
of
guest
party
size
of
calendar
gap
and
so
on
we
looked
at
each
of
these
trip
characteristics
by
itself
with
this
coarser
structure
of
preferences
we
were
able
to
resolve
some
of
the
noise
in
our
data
as
well
as
the
potentially
conflicting
labels
for
the
same
trip
we
used
the
mean
acceptance
rate
for
each
trip
characteristic
as
a
proxy
for
preference
still
our
data-set
was
relatively
sparse
on
average
for
each
trip
characteristic
we
could
not
determine
the
preference
for
about
26%
of
hosts
because
they
never
received
an
accommodation
request
that
met
those
trip
characteristics
as
a
method
of
imputation
we
smoothed
the
preference
using
a
weight
function
that
for
each
trip
characteristic
averages
the
median
preference
of
hosts
in
the
region
with
the
host’s
preference
the
weight
on
the
median
preference
is
1
when
the
host
has
no
data
points
and
goes
to
0
monotonically
the
more
data
points
the
host
has
using
these
newly
defined
preferences
we
created
predictions
for
host
acceptances
using
a
l-2
regularized
logistic
regression
essentially
we
combine
the
preferences
for
different
trip
characteristics
into
a
single
prediction
for
the
probability
of
acceptance
the
weight
the
preference
of
each
trip
characteristic
has
on
the
acceptance
decision
is
the
coefficient
that
comes
out
of
the
logistic
regression
to
improve
the
prediction
we
include
a
few
more
geographic
and
host
specific
features
in
the
logistic
regression
this
flow
chart
summarizes
the
modeling
technique
we
ran
this
model
on
segments
of
hosts
on
our
cluster
using
a
user-generated-function
udf
on
hive
the
udf
is
written
in
python
its
inputs
are
accommodation
requests
hosts’
response
to
them
and
a
few
other
host
features
depending
on
the
flag
passed
to
it
the
udf
either
builds
the
preferences
for
the
different
trip
characteristics
or
trains
the
logistic
regression
model
using
scikit-learn
our
main
off-line
evaluation
metric
for
the
model
was
mean
squared
error
mse
which
is
more
appropriate
in
a
setting
when
we
care
about
the
predicted
probability
more
than
about
classification
in
our
off-line
evaluation
of
the
model
we
were
able
to
get
a
10%
decrease
in
mse
over
our
previous
model
that
captured
host
acceptance
probability
this
was
a
promising
result
but
we
still
had
to
test
the
performance
of
the
model
live
on
our
site
to
test
the
online
performance
of
the
model
we
launched
an
experiment
that
used
the
predicted
probability
of
host
acceptance
as
a
significant
weight
in
our
ranking
algorithm
that
also
includes
many
other
features
that
capture
guests’
preferences
every
time
a
guest
in
the
treatment
group
entered
a
search
query
our
model
predicted
the
probability
of
acceptance
for
all
relevant
hosts
and
influenced
the
order
in
which
listings
were
presented
to
the
guest
ranking
likelier
matches
higher
we
evaluated
the
experiment
by
looking
at
multiple
metrics
but
the
most
important
one
was
the
likelihood
that
a
guest
requesting
accommodation
would
get
a
booking
booking
conversion
we
found
a
375%
lift
in
our
booking
conversion
and
a
significant
increase
in
the
number
of
successful
matches
between
guests
and
hosts
after
concluding
the
initial
experiment
we
made
a
few
more
optimizations
that
improved
conversion
by
approximately
another
1%
and
then
launched
the
experiment
to
100%
of
users
this
was
an
exciting
outcome
for
our
first
full-fledged
personalization
search
signal
and
a
sizable
contributor
to
our
success
first
this
project
taught
us
that
in
a
two
sided
marketplace
personalization
can
be
effective
on
the
buyer
as
well
as
the
seller
side
second
the
project
taught
us
that
sometimes
you
have
to
roll
up
your
sleeves
and
build
a
machine
learning
model
tailored
for
your
own
application
in
this
case
the
application
did
not
quite
fit
in
the
collaborative
filtering
and
a
multilevel
model
with
host
fixed-effect
was
too
computationally
demanding
and
not
suited
for
a
sparse
data-set
while
building
our
own
model
took
more
time
it
was
a
fun
learning
experience
finally
this
project
would
not
have
succeeded
without
the
fantastic
work
of
spencer
de
mars
and
lukasz
dziurzynski
originally
published
at
nerdsairbnbcom
on
april
14
2015
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
creative
engineers
and
data
scientists
building
a
world
where
you
can
belong
anywhere
http:airbnbio
creative
engineers
and
data
scientists
building
a
world
where
you
can
belong
anywhere
http:airbnbio
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
普通话
русский
한국어
português
tiếng
việt
or
italiano
are
you
tired
of
reading
endless
news
stories
about
deep
learning
and
not
really
knowing
what
that
means?
let’s
change
that!
this
time
we
are
going
to
learn
how
to
write
programs
that
recognize
objects
in
images
using
deep
learning
in
other
words
we’re
going
to
explain
the
black
magic
that
allows
google
photos
to
search
your
photos
based
on
what
is
in
the
picture:
just
like
part
1
and
part
2
this
guide
is
for
anyone
who
is
curious
about
machine
learning
but
has
no
idea
where
to
start
the
goal
is
be
accessible
to
anyone
—
which
means
that
there’s
a
lot
of
generalizations
and
we
skip
lots
of
details
but
who
cares?
if
this
gets
anyone
more
interested
in
ml
then
mission
accomplished!
if
you
haven’t
already
read
part
1
and
part
2
read
them
now!
you
might
have
seen
this
famous
xkcd
comic
before
the
goof
is
based
on
the
idea
that
any
3-year-old
child
can
recognize
a
photo
of
a
bird
but
figuring
out
how
to
make
a
computer
recognize
objects
has
puzzled
the
very
best
computer
scientists
for
over
50
years
in
the
last
few
years
we’ve
finally
found
a
good
approach
to
object
recognition
using
deep
convolutional
neural
networks
that
sounds
like
a
a
bunch
of
made
up
words
from
a
william
gibson
sci-fi
novel
but
the
ideas
are
totally
understandable
if
you
break
them
down
one
by
one
so
let’s
do
it
—
let’s
write
a
program
that
can
recognize
birds!
before
we
learn
how
to
recognize
pictures
of
birds
let’s
learn
how
to
recognize
something
much
simpler
—
the
handwritten
number
8
in
part
2
we
learned
about
how
neural
networks
can
solve
complex
problems
by
chaining
together
lots
of
simple
neurons
we
created
a
small
neural
network
to
estimate
the
price
of
a
house
based
on
how
many
bedrooms
it
had
how
big
it
was
and
which
neighborhood
it
was
in:
we
also
know
that
the
idea
of
machine
learning
is
that
the
same
generic
algorithms
can
be
reused
with
different
data
to
solve
different
problems
so
let’s
modify
this
same
neural
network
to
recognize
handwritten
text
but
to
make
the
job
really
simple
we’ll
only
try
to
recognize
one
letter
—
the
numeral
8
machine
learning
only
works
when
you
have
data
—
preferably
a
lot
of
data
so
we
need
lots
and
lots
of
handwritten
8s
to
get
started
luckily
researchers
created
the
mnist
data
set
of
handwritten
numbers
for
this
very
purpose
mnist
provides
60000
images
of
handwritten
digits
each
as
an
18x18
image
here
are
some
8s
from
the
data
set:
the
neural
network
we
made
in
part
2
only
took
in
a
three
numbers
as
the
input
3
bedrooms
2000
sq
feet
""
etc
but
now
we
want
to
process
images
with
our
neural
network
how
in
the
world
do
we
feed
images
into
a
neural
network
instead
of
just
numbers?
the
answer
is
incredible
simple
a
neural
network
takes
numbers
as
input
to
a
computer
an
image
is
really
just
a
grid
of
numbers
that
represent
how
dark
each
pixel
is:
to
feed
an
image
into
our
neural
network
we
simply
treat
the
18x18
pixel
image
as
an
array
of
324
numbers:
the
handle
324
inputs
we’ll
just
enlarge
our
neural
network
to
have
324
input
nodes:
notice
that
our
neural
network
also
has
two
outputs
now
instead
of
just
one
the
first
output
will
predict
the
likelihood
that
the
image
is
an
8
and
thee
second
output
will
predict
the
likelihood
it
isn’t
an
8
by
having
a
separate
output
for
each
type
of
object
we
want
to
recognize
we
can
use
a
neural
network
to
classify
objects
into
groups
our
neural
network
is
a
lot
bigger
than
last
time
324
inputs
instead
of
3!
but
any
modern
computer
can
handle
a
neural
network
with
a
few
hundred
nodes
without
blinking
this
would
even
work
fine
on
your
cell
phone
all
that’s
left
is
to
train
the
neural
network
with
images
of
8s
and
not-8s
so
it
learns
to
tell
them
apart
when
we
feed
in
an
8
we’ll
tell
it
the
probability
the
image
is
an
8
is
100%
and
the
probability
it’s
not
an
8
is
0%
vice
versa
for
the
counter-example
images
here’s
some
of
our
training
data:
we
can
train
this
kind
of
neural
network
in
a
few
minutes
on
a
modern
laptop
when
it’s
done
we’ll
have
a
neural
network
that
can
recognize
pictures
of
8s
with
a
pretty
high
accuracy
welcome
to
the
world
of
late
1980’s-era
image
recognition!
it’s
really
neat
that
simply
feeding
pixels
into
a
neural
network
actually
worked
to
build
image
recognition!
machine
learning
is
magic!
right?
well
of
course
it’s
not
that
simple
first
the
good
news
is
that
our
8
recognizer
really
does
work
well
on
simple
images
where
the
letter
is
right
in
the
middle
of
the
image:
but
now
the
really
bad
news:
our
8
recognizer
totally
fails
to
work
when
the
letter
isn’t
perfectly
centered
in
the
image
just
the
slightest
position
change
ruins
everything:
this
is
because
our
network
only
learned
the
pattern
of
a
perfectly-centered
8
it
has
absolutely
no
idea
what
an
off-center
8
is
it
knows
exactly
one
pattern
and
one
pattern
only
that’s
not
very
useful
in
the
real
world
real
world
problems
are
never
that
clean
and
simple
so
we
need
to
figure
out
how
to
make
our
neural
network
work
in
cases
where
the
8
isn’t
perfectly
centered
we
already
created
a
really
good
program
for
finding
an
8
centered
in
an
image
what
if
we
just
scan
all
around
the
image
for
possible
8s
in
smaller
sections
one
section
at
a
time
until
we
find
one?
this
approach
called
a
sliding
window
it’s
the
brute
force
solution
it
works
well
in
some
limited
cases
but
it’s
really
inefficient
you
have
to
check
the
same
image
over
and
over
looking
for
objects
of
different
sizes
we
can
do
better
than
this!
when
we
trained
our
network
we
only
showed
it
8s
that
were
perfectly
centered
what
if
we
train
it
with
more
data
including
8s
in
all
different
positions
and
sizes
all
around
the
image?
we
don’t
even
need
to
collect
new
training
data
we
can
just
write
a
script
to
generate
new
images
with
the
8s
in
all
kinds
of
different
positions
in
the
image:
using
this
technique
we
can
easily
create
an
endless
supply
of
training
data
more
data
makes
the
problem
harder
for
our
neural
network
to
solve
but
we
can
compensate
for
that
by
making
our
network
bigger
and
thus
able
to
learn
more
complicated
patterns
to
make
the
network
bigger
we
just
stack
up
layer
upon
layer
of
nodes:
we
call
this
a
deep
neural
network
because
it
has
more
layers
than
a
traditional
neural
network
this
idea
has
been
around
since
the
late
1960s
but
until
recently
training
this
large
of
a
neural
network
was
just
too
slow
to
be
useful
but
once
we
figured
out
how
to
use
3d
graphics
cards
which
were
designed
to
do
matrix
multiplication
really
fast
instead
of
normal
computer
processors
working
with
large
neural
networks
suddenly
became
practical
in
fact
the
exact
same
nvidia
geforce
gtx
1080
video
card
that
you
use
to
play
overwatch
can
be
used
to
train
neural
networks
incredibly
quickly
but
even
though
we
can
make
our
neural
network
really
big
and
train
it
quickly
with
a
3d
graphics
card
that
still
isn’t
going
to
get
us
all
the
way
to
a
solution
we
need
to
be
smarter
about
how
we
process
images
into
our
neural
network
think
about
it
it
doesn’t
make
sense
to
train
a
network
to
recognize
an
8
at
the
top
of
a
picture
separately
from
training
it
to
recognize
an
8
at
the
bottom
of
a
picture
as
if
those
were
two
totally
different
objects
there
should
be
some
way
to
make
the
neural
network
smart
enough
to
know
that
an
8
anywhere
in
the
picture
is
the
same
thing
without
all
that
extra
training
luckily
there
is!
as
a
human
you
intuitively
know
that
pictures
have
a
hierarchy
or
conceptual
structure
consider
this
picture:
as
a
human
you
instantly
recognize
the
hierarchy
in
this
picture:
most
importantly
we
recognize
the
idea
of
a
child
no
matter
what
surface
the
child
is
on
we
don’t
have
to
re-learn
the
idea
of
child
for
every
possible
surface
it
could
appear
on
but
right
now
our
neural
network
can’t
do
this
it
thinks
that
an
8
in
a
different
part
of
the
image
is
an
entirely
different
thing
it
doesn’t
understand
that
moving
an
object
around
in
the
picture
doesn’t
make
it
something
different
this
means
it
has
to
re-learn
the
identify
of
each
object
in
every
possible
position
that
sucks
we
need
to
give
our
neural
network
understanding
of
translation
invariance
—
an
8
is
an
8
no
matter
where
in
the
picture
it
shows
up
we’ll
do
this
using
a
process
called
convolution
the
idea
of
convolution
is
inspired
partly
by
computer
science
and
partly
by
biology
ie
mad
scientists
literally
poking
cat
brains
with
weird
probes
to
figure
out
how
cats
process
images
instead
of
feeding
entire
images
into
our
neural
network
as
one
grid
of
numbers
we’re
going
to
do
something
a
lot
smarter
that
takes
advantage
of
the
idea
that
an
object
is
the
same
no
matter
where
it
appears
in
a
picture
here’s
how
it’s
going
to
work
step
by
step
—
similar
to
our
sliding
window
search
above
let’s
pass
a
sliding
window
over
the
entire
original
image
and
save
each
result
as
a
separate
tiny
picture
tile:
by
doing
this
we
turned
our
original
image
into
77
equally-sized
tiny
image
tiles
earlier
we
fed
a
single
image
into
a
neural
network
to
see
if
it
was
an
8
we’ll
do
the
exact
same
thing
here
but
we’ll
do
it
for
each
individual
image
tile:
however
there’s
one
big
twist:
we’ll
keep
the
same
neural
network
weights
for
every
single
tile
in
the
same
original
image
in
other
words
we
are
treating
every
image
tile
equally
if
something
interesting
appears
in
any
given
tile
we’ll
mark
that
tile
as
interesting
we
don’t
want
to
lose
track
of
the
arrangement
of
the
original
tiles
so
we
save
the
result
from
processing
each
tile
into
a
grid
in
the
same
arrangement
as
the
original
image
it
looks
like
this:
in
other
words
we’ve
started
with
a
large
image
and
we
ended
with
a
slightly
smaller
array
that
records
which
sections
of
our
original
image
were
the
most
interesting
the
result
of
step
3
was
an
array
that
maps
out
which
parts
of
the
original
image
are
the
most
interesting
but
that
array
is
still
pretty
big:
to
reduce
the
size
of
the
array
we
downsample
it
using
an
algorithm
called
max
pooling
it
sounds
fancy
but
it
isn’t
at
all!
we’ll
just
look
at
each
2x2
square
of
the
array
and
keep
the
biggest
number:
the
idea
here
is
that
if
we
found
something
interesting
in
any
of
the
four
input
tiles
that
makes
up
each
2x2
grid
square
we’ll
just
keep
the
most
interesting
bit
this
reduces
the
size
of
our
array
while
keeping
the
most
important
bits
so
far
we’ve
reduced
a
giant
image
down
into
a
fairly
small
array
guess
what?
that
array
is
just
a
bunch
of
numbers
so
we
can
use
that
small
array
as
input
into
another
neural
network
this
final
neural
network
will
decide
if
the
image
is
or
isn’t
a
match
to
differentiate
it
from
the
convolution
step
we
call
it
a
fully
connected
network
so
from
start
to
finish
our
whole
five-step
pipeline
looks
like
this:
our
image
processing
pipeline
is
a
series
of
steps:
convolution
max-pooling
and
finally
a
fully-connected
network
when
solving
problems
in
the
real
world
these
steps
can
be
combined
and
stacked
as
many
times
as
you
want!
you
can
have
two
three
or
even
ten
convolution
layers
you
can
throw
in
max
pooling
wherever
you
want
to
reduce
the
size
of
your
data
the
basic
idea
is
to
start
with
a
large
image
and
continually
boil
it
down
step-by-step
until
you
finally
have
a
single
result
the
more
convolution
steps
you
have
the
more
complicated
features
your
network
will
be
able
to
learn
to
recognize
for
example
the
first
convolution
step
might
learn
to
recognize
sharp
edges
the
second
convolution
step
might
recognize
beaks
using
it’s
knowledge
of
sharp
edges
the
third
step
might
recognize
entire
birds
using
it’s
knowledge
of
beaks
etc
here’s
what
a
more
realistic
deep
convolutional
network
like
you
would
find
in
a
research
paper
looks
like:
in
this
case
they
start
a
224
x
224
pixel
image
apply
convolution
and
max
pooling
twice
apply
convolution
3
more
times
apply
max
pooling
and
then
have
two
fully-connected
layers
the
end
result
is
that
the
image
is
classified
into
one
of
1000
categories!
so
how
do
you
know
which
steps
you
need
to
combine
to
make
your
image
classifier
work?
honestly
you
have
to
answer
this
by
doing
a
lot
of
experimentation
and
testing
you
might
have
to
train
100
networks
before
you
find
the
optimal
structure
and
parameters
for
the
problem
you
are
solving
machine
learning
involves
a
lot
of
trial
and
error!
now
finally
we
know
enough
to
write
a
program
that
can
decide
if
a
picture
is
a
bird
or
not
as
always
we
need
some
data
to
get
started
the
free
cifar10
data
set
contains
6000
pictures
of
birds
and
52000
pictures
of
things
that
are
not
birds
but
to
get
even
more
data
we’ll
also
add
in
the
caltech-ucsd
birds-200–2011
data
set
that
has
another
12000
bird
pics
here’s
a
few
of
the
birds
from
our
combined
data
set:
and
here’s
some
of
the
52000
non-bird
images:
this
data
set
will
work
fine
for
our
purposes
but
72000
low-res
images
is
still
pretty
small
for
real-world
applications
if
you
want
google-level
performance
you
need
millions
of
large
images
in
machine
learning
having
more
data
is
almost
always
more
important
that
having
better
algorithms
now
you
know
why
google
is
so
happy
to
offer
you
unlimited
photo
storage
they
want
your
sweet
sweet
data!
to
build
our
classifier
we’ll
use
tflearn
tflearn
is
a
wrapper
around
google’s
tensorflow
deep
learning
library
that
exposes
a
simplified
api
it
makes
building
convolutional
neural
networks
as
easy
as
writing
a
few
lines
of
code
to
define
the
layers
of
our
network
here’s
the
code
to
define
and
train
the
network:
if
you
are
training
with
a
good
video
card
with
enough
ram
like
an
nvidia
geforce
gtx
980
ti
or
better
this
will
be
done
in
less
than
an
hour
if
you
are
training
with
a
normal
cpu
it
might
take
a
lot
longer
as
it
trains
the
accuracy
will
increase
after
the
first
pass
i
got
754%
accuracy
after
just
10
passes
it
was
already
up
to
917%
after
50
or
so
passes
it
capped
out
around
955%
accuracy
and
additional
training
didn’t
help
so
i
stopped
it
there
congrats!
our
program
can
now
recognize
birds
in
images!
now
that
we
have
a
trained
neural
network
we
can
use
it!
here’s
a
simple
script
that
takes
in
a
single
image
file
and
predicts
if
it
is
a
bird
or
not
but
to
really
see
how
effective
our
network
is
we
need
to
test
it
with
lots
of
images
the
data
set
i
created
held
back
15000
images
for
validation
when
i
ran
those
15000
images
through
the
network
it
predicted
the
correct
answer
95%
of
the
time
that
seems
pretty
good
right?
well
it
depends!
our
network
claims
to
be
95%
accurate
but
the
devil
is
in
the
details
that
could
mean
all
sorts
of
different
things
for
example
what
if
5%
of
our
training
images
were
birds
and
the
other
95%
were
not
birds?
a
program
that
guessed
not
a
bird
every
single
time
would
be
95%
accurate!
but
it
would
also
be
100%
useless
we
need
to
look
more
closely
at
the
numbers
than
just
the
overall
accuracy
to
judge
how
good
a
classification
system
really
is
we
need
to
look
closely
at
how
it
failed
not
just
the
percentage
of
the
time
that
it
failed
instead
of
thinking
about
our
predictions
as
right
and
wrong
let’s
break
them
down
into
four
separate
categories
—
using
our
validation
set
of
15000
images
here’s
how
many
times
our
predictions
fell
into
each
category:
why
do
we
break
our
results
down
like
this?
because
not
all
mistakes
are
created
equal
imagine
if
we
were
writing
a
program
to
detect
cancer
from
an
mri
image
if
we
were
detecting
cancer
we’d
rather
have
false
positives
than
false
negatives
false
negatives
would
be
the
worse
possible
case
—
that’s
when
the
program
told
someone
they
definitely
didn’t
have
cancer
but
they
actually
did
instead
of
just
looking
at
overall
accuracy
we
calculate
precision
and
recall
metrics
precision
and
recall
metrics
give
us
a
clearer
picture
of
how
well
we
did:
this
tells
us
that
97%
of
the
time
we
guessed
bird
we
were
right!
but
it
also
tells
us
that
we
only
found
90%
of
the
actual
birds
in
the
data
set
in
other
words
we
might
not
find
every
bird
but
we
are
pretty
sure
about
it
when
we
do
find
one!
now
that
you
know
the
basics
of
deep
convolutional
networks
you
can
try
out
some
of
the
examples
that
come
with
tflearn
to
get
your
hands
dirty
with
different
neural
network
architectures
it
even
comes
with
built-in
data
sets
so
you
don’t
even
have
to
find
your
own
images
you
also
know
enough
now
to
start
branching
and
learning
about
other
areas
of
machine
learning
why
not
learn
how
to
use
algorithms
to
train
computers
how
to
play
atari
games
next?
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
email
list
i’ll
only
email
you
when
i
have
something
new
and
awesome
to
share
it’s
the
best
way
to
find
out
when
i
write
more
articles
like
this
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun
part
4
part
5
and
part
6!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
普通话
русский
한국어
português
tiếng
việt
or
italiano
have
you
noticed
that
facebook
has
developed
an
uncanny
ability
to
recognize
your
friends
in
your
photographs?
in
the
old
days
facebook
used
to
make
you
to
tag
your
friends
in
photos
by
clicking
on
them
and
typing
in
their
name
now
as
soon
as
you
upload
a
photo
facebook
tags
everyone
for
you
like
magic:
this
technology
is
called
face
recognition
facebook’s
algorithms
are
able
to
recognize
your
friends’
faces
after
they
have
been
tagged
only
a
few
times
it’s
pretty
amazing
technology
—
facebook
can
recognize
faces
with
98%
accuracy
which
is
pretty
much
as
good
as
humans
can
do!
let’s
learn
how
modern
face
recognition
works!
but
just
recognizing
your
friends
would
be
too
easy
we
can
push
this
tech
to
the
limit
to
solve
a
more
challenging
problem
—
telling
will
ferrell
famous
actor
apart
from
chad
smith
famous
rock
musician!
so
far
in
part
1
2
and
3
we’ve
used
machine
learning
to
solve
isolated
problems
that
have
only
one
step
—
estimating
the
price
of
a
house
generating
new
data
based
on
existing
data
and
telling
if
an
image
contains
a
certain
object
all
of
those
problems
can
be
solved
by
choosing
one
machine
learning
algorithm
feeding
in
data
and
getting
the
result
but
face
recognition
is
really
a
series
of
several
related
problems:
as
a
human
your
brain
is
wired
to
do
all
of
this
automatically
and
instantly
in
fact
humans
are
too
good
at
recognizing
faces
and
end
up
seeing
faces
in
everyday
objects:
computers
are
not
capable
of
this
kind
of
high-level
generalization
at
least
not
yet
so
we
have
to
teach
them
how
to
do
each
step
in
this
process
separately
we
need
to
build
a
pipeline
where
we
solve
each
step
of
face
recognition
separately
and
pass
the
result
of
the
current
step
to
the
next
step
in
other
words
we
will
chain
together
several
machine
learning
algorithms:
let’s
tackle
this
problem
one
step
at
a
time
for
each
step
we’ll
learn
about
a
different
machine
learning
algorithm
i’m
not
going
to
explain
every
single
algorithm
completely
to
keep
this
from
turning
into
a
book
but
you’ll
learn
the
main
ideas
behind
each
one
and
you’ll
learn
how
you
can
build
your
own
facial
recognition
system
in
python
using
openface
and
dlib
the
first
step
in
our
pipeline
is
face
detection
obviously
we
need
to
locate
the
faces
in
a
photograph
before
we
can
try
to
tell
them
apart!
if
you’ve
used
any
camera
in
the
last
10
years
you’ve
probably
seen
face
detection
in
action:
face
detection
is
a
great
feature
for
cameras
when
the
camera
can
automatically
pick
out
faces
it
can
make
sure
that
all
the
faces
are
in
focus
before
it
takes
the
picture
but
we’ll
use
it
for
a
different
purpose
—
finding
the
areas
of
the
image
we
want
to
pass
on
to
the
next
step
in
our
pipeline
face
detection
went
mainstream
in
the
early
2000's
when
paul
viola
and
michael
jones
invented
a
way
to
detect
faces
that
was
fast
enough
to
run
on
cheap
cameras
however
much
more
reliable
solutions
exist
now
we’re
going
to
use
a
method
invented
in
2005
called
histogram
of
oriented
gradients
—
or
just
hog
for
short
to
find
faces
in
an
image
we’ll
start
by
making
our
image
black
and
white
because
we
don’t
need
color
data
to
find
faces:
then
we’ll
look
at
every
single
pixel
in
our
image
one
at
a
time
for
every
single
pixel
we
want
to
look
at
the
pixels
that
directly
surrounding
it:
our
goal
is
to
figure
out
how
dark
the
current
pixel
is
compared
to
the
pixels
directly
surrounding
it
then
we
want
to
draw
an
arrow
showing
in
which
direction
the
image
is
getting
darker:
if
you
repeat
that
process
for
every
single
pixel
in
the
image
you
end
up
with
every
pixel
being
replaced
by
an
arrow
these
arrows
are
called
gradients
and
they
show
the
flow
from
light
to
dark
across
the
entire
image:
this
might
seem
like
a
random
thing
to
do
but
there’s
a
really
good
reason
for
replacing
the
pixels
with
gradients
if
we
analyze
pixels
directly
really
dark
images
and
really
light
images
of
the
same
person
will
have
totally
different
pixel
values
but
by
only
considering
the
direction
that
brightness
changes
both
really
dark
images
and
really
bright
images
will
end
up
with
the
same
exact
representation
that
makes
the
problem
a
lot
easier
to
solve!
but
saving
the
gradient
for
every
single
pixel
gives
us
way
too
much
detail
we
end
up
missing
the
forest
for
the
trees
it
would
be
better
if
we
could
just
see
the
basic
flow
of
lightnessdarkness
at
a
higher
level
so
we
could
see
the
basic
pattern
of
the
image
to
do
this
we’ll
break
up
the
image
into
small
squares
of
16x16
pixels
each
in
each
square
we’ll
count
up
how
many
gradients
point
in
each
major
direction
how
many
point
up
point
up-right
point
right
etc
then
we’ll
replace
that
square
in
the
image
with
the
arrow
directions
that
were
the
strongest
the
end
result
is
we
turn
the
original
image
into
a
very
simple
representation
that
captures
the
basic
structure
of
a
face
in
a
simple
way:
to
find
faces
in
this
hog
image
all
we
have
to
do
is
find
the
part
of
our
image
that
looks
the
most
similar
to
a
known
hog
pattern
that
was
extracted
from
a
bunch
of
other
training
faces:
using
this
technique
we
can
now
easily
find
faces
in
any
image:
if
you
want
to
try
this
step
out
yourself
using
python
and
dlib
here’s
code
showing
how
to
generate
and
view
hog
representations
of
images
whew
we
isolated
the
faces
in
our
image
but
now
we
have
to
deal
with
the
problem
that
faces
turned
different
directions
look
totally
different
to
a
computer:
to
account
for
this
we
will
try
to
warp
each
picture
so
that
the
eyes
and
lips
are
always
in
the
sample
place
in
the
image
this
will
make
it
a
lot
easier
for
us
to
compare
faces
in
the
next
steps
to
do
this
we
are
going
to
use
an
algorithm
called
face
landmark
estimation
there
are
lots
of
ways
to
do
this
but
we
are
going
to
use
the
approach
invented
in
2014
by
vahid
kazemi
and
josephine
sullivan
the
basic
idea
is
we
will
come
up
with
68
specific
points
called
landmarks
that
exist
on
every
face
—
the
top
of
the
chin
the
outside
edge
of
each
eye
the
inner
edge
of
each
eyebrow
etc
then
we
will
train
a
machine
learning
algorithm
to
be
able
to
find
these
68
specific
points
on
any
face:
here’s
the
result
of
locating
the
68
face
landmarks
on
our
test
image:
now
that
we
know
were
the
eyes
and
mouth
are
we’ll
simply
rotate
scale
and
shear
the
image
so
that
the
eyes
and
mouth
are
centered
as
best
as
possible
we
won’t
do
any
fancy
3d
warps
because
that
would
introduce
distortions
into
the
image
we
are
only
going
to
use
basic
image
transformations
like
rotation
and
scale
that
preserve
parallel
lines
called
affine
transformations:
now
no
matter
how
the
face
is
turned
we
are
able
to
center
the
eyes
and
mouth
are
in
roughly
the
same
position
in
the
image
this
will
make
our
next
step
a
lot
more
accurate
if
you
want
to
try
this
step
out
yourself
using
python
and
dlib
here’s
the
code
for
finding
face
landmarks
and
here’s
the
code
for
transforming
the
image
using
those
landmarks
now
we
are
to
the
meat
of
the
problem
—
actually
telling
faces
apart
this
is
where
things
get
really
interesting!
the
simplest
approach
to
face
recognition
is
to
directly
compare
the
unknown
face
we
found
in
step
2
with
all
the
pictures
we
have
of
people
that
have
already
been
tagged
when
we
find
a
previously
tagged
face
that
looks
very
similar
to
our
unknown
face
it
must
be
the
same
person
seems
like
a
pretty
good
idea
right?
there’s
actually
a
huge
problem
with
that
approach
a
site
like
facebook
with
billions
of
users
and
a
trillion
photos
can’t
possibly
loop
through
every
previous-tagged
face
to
compare
it
to
every
newly
uploaded
picture
that
would
take
way
too
long
they
need
to
be
able
to
recognize
faces
in
milliseconds
not
hours
what
we
need
is
a
way
to
extract
a
few
basic
measurements
from
each
face
then
we
could
measure
our
unknown
face
the
same
way
and
find
the
known
face
with
the
closest
measurements
for
example
we
might
measure
the
size
of
each
ear
the
spacing
between
the
eyes
the
length
of
the
nose
etc
if
you’ve
ever
watched
a
bad
crime
show
like
csi
you
know
what
i
am
talking
about:
ok
so
which
measurements
should
we
collect
from
each
face
to
build
our
known
face
database?
ear
size?
nose
length?
eye
color?
something
else?
it
turns
out
that
the
measurements
that
seem
obvious
to
us
humans
like
eye
color
don’t
really
make
sense
to
a
computer
looking
at
individual
pixels
in
an
image
researchers
have
discovered
that
the
most
accurate
approach
is
to
let
the
computer
figure
out
the
measurements
to
collect
itself
deep
learning
does
a
better
job
than
humans
at
figuring
out
which
parts
of
a
face
are
important
to
measure
the
solution
is
to
train
a
deep
convolutional
neural
network
just
like
we
did
in
part
3
but
instead
of
training
the
network
to
recognize
pictures
objects
like
we
did
last
time
we
are
going
to
train
it
to
generate
128
measurements
for
each
face
the
training
process
works
by
looking
at
3
face
images
at
a
time:
then
the
algorithm
looks
at
the
measurements
it
is
currently
generating
for
each
of
those
three
images
it
then
tweaks
the
neural
network
slightly
so
that
it
makes
sure
the
measurements
it
generates
for
#1
and
#2
are
slightly
closer
while
making
sure
the
measurements
for
#2
and
#3
are
slightly
further
apart:
after
repeating
this
step
millions
of
times
for
millions
of
images
of
thousands
of
different
people
the
neural
network
learns
to
reliably
generate
128
measurements
for
each
person
any
ten
different
pictures
of
the
same
person
should
give
roughly
the
same
measurements
machine
learning
people
call
the
128
measurements
of
each
face
an
embedding
the
idea
of
reducing
complicated
raw
data
like
a
picture
into
a
list
of
computer-generated
numbers
comes
up
a
lot
in
machine
learning
especially
in
language
translation
the
exact
approach
for
faces
we
are
using
was
invented
in
2015
by
researchers
at
google
but
many
similar
approaches
exist
this
process
of
training
a
convolutional
neural
network
to
output
face
embeddings
requires
a
lot
of
data
and
computer
power
even
with
an
expensive
nvidia
telsa
video
card
it
takes
about
24
hours
of
continuous
training
to
get
good
accuracy
but
once
the
network
has
been
trained
it
can
generate
measurements
for
any
face
even
ones
it
has
never
seen
before!
so
this
step
only
needs
to
be
done
once
lucky
for
us
the
fine
folks
at
openface
already
did
this
and
they
published
several
trained
networks
which
we
can
directly
use
thanks
brandon
amos
and
team!
so
all
we
need
to
do
ourselves
is
run
our
face
images
through
their
pre-trained
network
to
get
the
128
measurements
for
each
face
here’s
the
measurements
for
our
test
image:
so
what
parts
of
the
face
are
these
128
numbers
measuring
exactly?
it
turns
out
that
we
have
no
idea
it
doesn’t
really
matter
to
us
all
that
we
care
is
that
the
network
generates
nearly
the
same
numbers
when
looking
at
two
different
pictures
of
the
same
person
if
you
want
to
try
this
step
yourself
openface
provides
a
lua
script
that
will
generate
embeddings
all
images
in
a
folder
and
write
them
to
a
csv
file
you
run
it
like
this
this
last
step
is
actually
the
easiest
step
in
the
whole
process
all
we
have
to
do
is
find
the
person
in
our
database
of
known
people
who
has
the
closest
measurements
to
our
test
image
you
can
do
that
by
using
any
basic
machine
learning
classification
algorithm
no
fancy
deep
learning
tricks
are
needed
we’ll
use
a
simple
linear
svm
classifier
but
lots
of
classification
algorithms
could
work
all
we
need
to
do
is
train
a
classifier
that
can
take
in
the
measurements
from
a
new
test
image
and
tells
which
known
person
is
the
closest
match
running
this
classifier
takes
milliseconds
the
result
of
the
classifier
is
the
name
of
the
person!
so
let’s
try
out
our
system
first
i
trained
a
classifier
with
the
embeddings
of
about
20
pictures
each
of
will
ferrell
chad
smith
and
jimmy
falon:
then
i
ran
the
classifier
on
every
frame
of
the
famous
youtube
video
of
will
ferrell
and
chad
smith
pretending
to
be
each
other
on
the
jimmy
fallon
show:
it
works!
and
look
how
well
it
works
for
faces
in
different
poses
—
even
sideways
faces!
let’s
review
the
steps
we
followed:
now
that
you
know
how
this
all
works
here’s
instructions
from
start-to-finish
of
how
run
this
entire
face
recognition
pipeline
on
your
own
computer:
update
492017:
you
can
still
follow
the
steps
below
to
use
openface
however
i’ve
released
a
new
python-based
face
recognition
library
called
face_recognition
that
is
much
easier
to
install
and
use
so
i’d
recommend
trying
out
face_recognition
first
instead
of
continuing
below!
i
even
put
together
a
pre-configured
virtual
machine
with
face_recognition
opencv
tensorflow
and
lots
of
other
deep
learning
tools
pre-installed
you
can
download
and
run
it
on
your
computer
very
easily
give
the
virtual
machine
a
shot
if
you
don’t
want
to
install
all
these
libraries
yourself!
original
openface
instructions:
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
newsletter:
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun
part
5!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
italiano
español
français
türkçe
русский
한국어
português
فارسی
tiếng
việt
or
普通话
in
part
1
we
said
that
machine
learning
is
using
generic
algorithms
to
tell
you
something
interesting
about
your
data
without
writing
any
code
specific
to
the
problem
you
are
solving
if
you
haven’t
already
read
part
1
read
it
now!
this
time
we
are
going
to
see
one
of
these
generic
algorithms
do
something
really
cool
—
create
video
game
levels
that
look
like
they
were
made
by
humans
we’ll
build
a
neural
network
feed
it
existing
super
mario
levels
and
watch
new
ones
pop
out!
just
like
part
1
this
guide
is
for
anyone
who
is
curious
about
machine
learning
but
has
no
idea
where
to
start
the
goal
is
be
accessible
to
anyone
—
which
means
that
there’s
a
lot
of
generalizations
and
we
skip
lots
of
details
but
who
cares?
if
this
gets
anyone
more
interested
in
ml
then
mission
accomplished
back
in
part
1
we
created
a
simple
algorithm
that
estimated
the
value
of
a
house
based
on
its
attributes
given
data
about
a
house
like
this:
we
ended
up
with
this
simple
estimation
function:
in
other
words
we
estimated
the
value
of
the
house
by
multiplying
each
of
its
attributes
by
a
weight
then
we
just
added
those
numbers
up
to
get
the
house’s
value
instead
of
using
code
let’s
represent
that
same
function
as
a
simple
diagram:
however
this
algorithm
only
works
for
simple
problems
where
the
result
has
a
linear
relationship
with
the
input
what
if
the
truth
behind
house
prices
isn’t
so
simple?
for
example
maybe
the
neighborhood
matters
a
lot
for
big
houses
and
small
houses
but
doesn’t
matter
at
all
for
medium-sized
houses
how
could
we
capture
that
kind
of
complicated
detail
in
our
model?
to
be
more
clever
we
could
run
this
algorithm
multiple
times
with
different
of
weights
that
each
capture
different
edge
cases:
now
we
have
four
different
price
estimates
let’s
combine
those
four
price
estimates
into
one
final
estimate
we’ll
run
them
through
the
same
algorithm
again
but
using
another
set
of
weights!
our
new
super
answer
combines
the
estimates
from
our
four
different
attempts
to
solve
the
problem
because
of
this
it
can
model
more
cases
than
we
could
capture
in
one
simple
model
let’s
combine
our
four
attempts
to
guess
into
one
big
diagram:
this
is
a
neural
network!
each
node
knows
how
to
take
in
a
set
of
inputs
apply
weights
to
them
and
calculate
an
output
value
by
chaining
together
lots
of
these
nodes
we
can
model
complex
functions
there’s
a
lot
that
i’m
skipping
over
to
keep
this
brief
including
feature
scaling
and
the
activation
function
but
the
most
important
part
is
that
these
basic
ideas
click:
it’s
just
like
lego!
we
can’t
model
much
with
one
single
lego
block
but
we
can
model
anything
if
we
have
enough
basic
lego
blocks
to
stick
together:
the
neural
network
we’ve
seen
always
returns
the
same
answer
when
you
give
it
the
same
inputs
it
has
no
memory
in
programming
terms
it’s
a
stateless
algorithm
in
many
cases
like
estimating
the
price
of
house
that’s
exactly
what
you
want
but
the
one
thing
this
kind
of
model
can’t
do
is
respond
to
patterns
in
data
over
time
imagine
i
handed
you
a
keyboard
and
asked
you
to
write
a
story
but
before
you
start
my
job
is
to
guess
the
very
first
letter
that
you
will
type
what
letter
should
i
guess?
i
can
use
my
knowledge
of
english
to
increase
my
odds
of
guessing
the
right
letter
for
example
you
will
probably
type
a
letter
that
is
common
at
the
beginning
of
words
if
i
looked
at
stories
you
wrote
in
the
past
i
could
narrow
it
down
further
based
on
the
words
you
usually
use
at
the
beginning
of
your
stories
once
i
had
all
that
data
i
could
use
it
to
build
a
neural
network
to
model
how
likely
it
is
that
you
would
start
with
any
given
letter
our
model
might
look
like
this:
but
let’s
make
the
problem
harder
let’s
say
i
need
to
guess
the
next
letter
you
are
going
to
type
at
any
point
in
your
story
this
is
a
much
more
interesting
problem
let’s
use
the
first
few
words
of
ernest
hemingway’s
the
sun
also
rises
as
an
example:
what
letter
is
going
to
come
next?
you
probably
guessed
’n’
—
the
word
is
probably
going
to
be
boxing
we
know
this
based
on
the
letters
we’ve
already
seen
in
the
sentence
and
our
knowledge
of
common
words
in
english
also
the
word
‘middleweight’
gives
us
an
extra
clue
that
we
are
talking
about
boxing
in
other
words
it’s
easy
to
guess
the
next
letter
if
we
take
into
account
the
sequence
of
letters
that
came
right
before
it
and
combine
that
with
our
knowledge
of
the
rules
of
english
to
solve
this
problem
with
a
neural
network
we
need
to
add
state
to
our
model
each
time
we
ask
our
neural
network
for
an
answer
we
also
save
a
set
of
our
intermediate
calculations
and
re-use
them
the
next
time
as
part
of
our
input
that
way
our
model
will
adjust
its
predictions
based
on
the
input
that
it
has
seen
recently
keeping
track
of
state
in
our
model
makes
it
possible
to
not
just
predict
the
most
likely
first
letter
in
the
story
but
to
predict
the
most
likely
next
letter
given
all
previous
letters
this
is
the
basic
idea
of
a
recurrent
neural
network
we
are
updating
the
network
each
time
we
use
it
this
allows
it
to
update
its
predictions
based
on
what
it
saw
most
recently
it
can
even
model
patterns
over
time
as
long
as
we
give
it
enough
of
a
memory
predicting
the
next
letter
in
a
story
might
seem
pretty
useless
what’s
the
point?
one
cool
use
might
be
auto-predict
for
a
mobile
phone
keyboard:
but
what
if
we
took
this
idea
to
the
extreme?
what
if
we
asked
the
model
to
predict
the
next
most
likely
character
over
and
over
—
forever?
we’d
be
asking
it
to
write
a
complete
story
for
us!
we
saw
how
we
could
guess
the
next
letter
in
hemingway’s
sentence
let’s
try
generating
a
whole
story
in
the
style
of
hemingway
to
do
this
we
are
going
to
use
the
recurrent
neural
network
implementation
that
andrej
karpathy
wrote
andrej
is
a
deep-learning
researcher
at
stanford
and
he
wrote
an
excellent
introduction
to
generating
text
with
rnns
you
can
view
all
the
code
for
the
model
on
github
we’ll
create
our
model
from
the
complete
text
of
the
sun
also
rises
—
362239
characters
using
84
unique
letters
including
punctuation
uppercaselowercase
etc
this
data
set
is
actually
really
small
compared
to
typical
real-world
applications
to
generate
a
really
good
model
of
hemingway’s
style
it
would
be
much
better
to
have
at
several
times
as
much
sample
text
but
this
is
good
enough
to
play
around
with
as
an
example
as
we
just
start
to
train
the
rnn
it’s
not
very
good
at
predicting
letters
here’s
what
it
generates
after
a
100
loops
of
training:
you
can
see
that
it
has
figured
out
that
sometimes
words
have
spaces
between
them
but
that’s
about
it
after
about
1000
iterations
things
are
looking
more
promising:
the
model
has
started
to
identify
the
patterns
in
basic
sentence
structure
it’s
adding
periods
at
the
ends
of
sentences
and
even
quoting
dialog
a
few
words
are
recognizable
but
there’s
also
still
a
lot
of
nonsense
but
after
several
thousand
more
training
iterations
it
looks
pretty
good:
at
this
point
the
algorithm
has
captured
the
basic
pattern
of
hemingway’s
short
direct
dialog
a
few
sentences
even
sort
of
make
sense
compare
that
with
some
real
text
from
the
book:
even
by
only
looking
for
patterns
one
character
at
a
time
our
algorithm
has
reproduced
plausible-looking
prose
with
proper
formatting
that
is
kind
of
amazing!
we
don’t
have
to
generate
text
completely
from
scratch
either
we
can
seed
the
algorithm
by
supplying
the
first
few
letters
and
just
let
it
find
the
next
few
letters
for
fun
let’s
make
a
fake
book
cover
for
our
imaginary
book
by
generating
a
new
author
name
and
a
new
title
using
the
seed
text
of
er
he
and
the
s:
not
bad!
but
the
really
mind-blowing
part
is
that
this
algorithm
can
figure
out
patterns
in
any
sequence
of
data
it
can
easily
generate
real-looking
recipes
or
fake
obama
speeches
but
why
limit
ourselves
human
language?
we
can
apply
this
same
idea
to
any
kind
of
sequential
data
that
has
a
pattern
in
2015
nintendo
released
super
mario
makertm
for
the
wii
u
gaming
system
this
game
lets
you
draw
out
your
own
super
mario
brothers
levels
on
the
gamepad
and
then
upload
them
to
the
internet
so
you
friends
can
play
through
them
you
can
include
all
the
classic
power-ups
and
enemies
from
the
original
mario
games
in
your
levels
it’s
like
a
virtual
lego
set
for
people
who
grew
up
playing
super
mario
brothers
can
we
use
the
same
model
that
generated
fake
hemingway
text
to
generate
fake
super
mario
brothers
levels?
first
we
need
a
data
set
for
training
our
model
let’s
take
all
the
outdoor
levels
from
the
original
super
mario
brothers
game
released
in
1985:
this
game
has
32
levels
and
about
70%
of
them
have
the
same
outdoor
style
so
we’ll
stick
to
those
to
get
the
designs
for
each
level
i
took
an
original
copy
of
the
game
and
wrote
a
program
to
pull
the
level
designs
out
of
the
game’s
memory
super
mario
bros
is
a
30-year-old
game
and
there
are
lots
of
resources
online
that
help
you
figure
out
how
the
levels
were
stored
in
the
game’s
memory
extracting
level
data
from
an
old
video
game
is
a
fun
programming
exercise
that
you
should
try
sometime
here’s
the
first
level
from
the
game
which
you
probably
remember
if
you
ever
played
it:
if
we
look
closely
we
can
see
the
level
is
made
of
a
simple
grid
of
objects:
we
could
just
as
easily
represent
this
grid
as
a
sequence
of
characters
with
one
character
representing
each
object:
we’ve
replaced
each
object
in
the
level
with
a
letter:
and
so
on
using
a
different
letter
for
each
different
kind
of
object
in
the
level
i
ended
up
with
text
files
that
looked
like
this:
looking
at
the
text
file
you
can
see
that
mario
levels
don’t
really
have
much
of
a
pattern
if
you
read
them
line-by-line:
the
patterns
in
a
level
really
emerge
when
you
think
of
the
level
as
a
series
of
columns:
so
in
order
for
the
algorithm
to
find
the
patterns
in
our
data
we
need
to
feed
the
data
in
column-by-column
figuring
out
the
most
effective
representation
of
your
input
data
called
feature
selection
is
one
of
the
keys
of
using
machine
learning
algorithms
well
to
train
the
model
i
needed
to
rotate
my
text
files
by
90
degrees
this
made
sure
the
characters
were
fed
into
the
model
in
an
order
where
a
pattern
would
more
easily
show
up:
just
like
we
saw
when
creating
the
model
of
hemingway’s
prose
a
model
improves
as
we
train
it
after
a
little
training
our
model
is
generating
junk:
it
sort
of
has
an
idea
that
‘-’s
and
‘=’s
should
show
up
a
lot
but
that’s
about
it
it
hasn’t
figured
out
the
pattern
yet
after
several
thousand
iterations
it’s
starting
to
look
like
something:
the
model
has
almost
figured
out
that
each
line
should
be
the
same
length
it
has
even
started
to
figure
out
some
of
the
logic
of
mario:
the
pipes
in
mario
are
always
two
blocks
wide
and
at
least
two
blocks
high
so
the
ps
in
the
data
should
appear
in
2x2
clusters
that’s
pretty
cool!
with
a
lot
more
training
the
model
gets
to
the
point
where
it
generates
perfectly
valid
data:
let’s
sample
an
entire
level’s
worth
of
data
from
our
model
and
rotate
it
back
horizontal:
this
data
looks
great!
there
are
several
awesome
things
to
notice:
finally
let’s
take
this
level
and
recreate
it
in
super
mario
maker:
play
it
yourself!
if
you
have
super
mario
maker
you
can
play
this
level
by
bookmarking
it
online
or
by
looking
it
up
using
level
code
4ac9–0000–0157-f3c3
the
recurrent
neural
network
algorithm
we
used
to
train
our
model
is
the
same
kind
of
algorithm
used
by
real-world
companies
to
solve
hard
problems
like
speech
detection
and
language
translation
what
makes
our
model
a
‘toy’
instead
of
cutting-edge
is
that
our
model
is
generated
from
very
little
data
there
just
aren’t
enough
levels
in
the
original
super
mario
brothers
game
to
provide
enough
data
for
a
really
good
model
if
we
could
get
access
to
the
hundreds
of
thousands
of
user-created
super
mario
maker
levels
that
nintendo
has
we
could
make
an
amazing
model
but
we
can’t
—
because
nintendo
won’t
let
us
have
them
big
companies
don’t
give
away
their
data
for
free
as
machine
learning
becomes
more
important
in
more
industries
the
difference
between
a
good
program
and
a
bad
program
will
be
how
much
data
you
have
to
train
your
models
that’s
why
companies
like
google
and
facebook
need
your
data
so
badly!
for
example
google
recently
open
sourced
tensorflow
its
software
toolkit
for
building
large-scale
machine
learning
applications
it
was
a
pretty
big
deal
that
google
gave
away
such
important
capable
technology
for
free
this
is
the
same
stuff
that
powers
google
translate
but
without
google’s
massive
trove
of
data
in
every
language
you
can’t
create
a
competitor
to
google
translate
data
is
what
gives
google
its
edge
think
about
that
the
next
time
you
open
up
your
google
maps
location
history
or
facebook
location
history
and
notice
that
it
stores
every
place
you’ve
ever
been
in
machine
learning
there’s
never
a
single
way
to
solve
a
problem
you
have
limitless
options
when
deciding
how
to
pre-process
your
data
and
which
algorithms
to
use
often
combining
multiple
approaches
will
give
you
better
results
than
any
single
approach
readers
have
sent
me
links
to
other
interesting
approaches
to
generating
super
mario
levels:
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
email
list
i’ll
only
email
you
when
i
have
something
new
and
awesome
to
share
it’s
the
best
way
to
find
out
when
i
write
more
articles
like
this
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun
part
3!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
for
this
tutorial
in
my
reinforcement
learning
series
we
are
going
to
be
exploring
a
family
of
rl
algorithms
called
q-learning
algorithms
these
are
a
little
different
than
the
policy-based
algorithms
that
will
be
looked
at
in
the
the
following
tutorials
parts
1–3
instead
of
starting
with
a
complex
and
unwieldy
deep
neural
network
we
will
begin
by
implementing
a
simple
lookup-table
version
of
the
algorithm
and
then
show
how
to
implement
a
neural-network
equivalent
using
tensorflow
given
that
we
are
going
back
to
basics
it
may
be
best
to
think
of
this
as
part-0
of
the
series
it
will
hopefully
give
an
intuition
into
what
is
really
happening
in
q-learning
that
we
can
then
build
on
going
forward
when
we
eventually
combine
the
policy
gradient
and
q-learning
approaches
to
build
state-of-the-art
rl
agents
if
you
are
more
interested
in
policy
networks
or
already
have
a
grasp
on
q-learning
feel
free
to
start
the
tutorial
series
here
instead
unlike
policy
gradient
methods
which
attempt
to
learn
functions
which
directly
map
an
observation
to
an
action
q-learning
attempts
to
learn
the
value
of
being
in
a
given
state
and
taking
a
specific
action
there
while
both
approaches
ultimately
allow
us
to
take
intelligent
actions
given
a
situation
the
means
of
getting
to
that
action
differ
significantly
you
may
have
heard
about
deepq-networks
which
can
play
atari
games
these
are
really
just
larger
and
more
complex
implementations
of
the
q-learning
algorithm
we
are
going
to
discuss
here
for
this
tutorial
we
are
going
to
be
attempting
to
solve
the
frozenlake
environment
from
the
openai
gym
for
those
unfamiliar
the
openai
gym
provides
an
easy
way
for
people
to
experiment
with
their
learning
agents
in
an
array
of
provided
toy
games
the
frozenlake
environment
consists
of
a
4x4
grid
of
blocks
each
one
either
being
the
start
block
the
goal
block
a
safe
frozen
block
or
a
dangerous
hole
the
objective
is
to
have
an
agent
learn
to
navigate
from
the
start
to
the
goal
without
moving
onto
a
hole
at
any
given
time
the
agent
can
choose
to
move
either
up
down
left
or
right
the
catch
is
that
there
is
a
wind
which
occasionally
blows
the
agent
onto
a
space
they
didn’t
choose
as
such
perfect
performance
every
time
is
impossible
but
learning
to
avoid
the
holes
and
reach
the
goal
are
certainly
still
doable
the
reward
at
every
step
is
0
except
for
entering
the
goal
which
provides
a
reward
of
1
thus
we
will
need
an
algorithm
that
learns
long-term
expected
rewards
this
is
exactly
what
q-learning
is
designed
to
provide
in
it’s
simplest
implementation
q-learning
is
a
table
of
values
for
every
state
row
and
action
column
possible
in
the
environment
within
each
cell
of
the
table
we
learn
a
value
for
how
good
it
is
to
take
a
given
action
within
a
given
state
in
the
case
of
the
frozenlake
environment
we
have
16
possible
states
one
for
each
block
and
4
possible
actions
the
four
directions
of
movement
giving
us
a
16x4
table
of
q-values
we
start
by
initializing
the
table
to
be
uniform
all
zeros
and
then
as
we
observe
the
rewards
we
obtain
for
various
actions
we
update
the
table
accordingly
we
make
updates
to
our
q-table
using
something
called
the
bellman
equation
which
states
that
the
expected
long-term
reward
for
a
given
action
is
equal
to
the
immediate
reward
from
the
current
action
combined
with
the
expected
reward
from
the
best
future
action
taken
at
the
following
state
in
this
way
we
reuse
our
own
q-table
when
estimating
how
to
update
our
table
for
future
actions!
in
equation
form
the
rule
looks
like
this:
this
says
that
the
q-value
for
a
given
state
s
and
action
a
should
represent
the
current
reward
r
plus
the
maximum
discounted
γ
future
reward
expected
according
to
our
own
table
for
the
next
state
s’
we
would
end
up
in
the
discount
variable
allows
us
to
decide
how
important
the
possible
future
rewards
are
compared
to
the
present
reward
by
updating
in
this
way
the
table
slowly
begins
to
obtain
accurate
measures
of
the
expected
future
reward
for
a
given
action
in
a
given
state
below
is
a
python
walkthrough
of
the
q-table
algorithm
implemented
in
the
frozenlake
environment:
thanks
to
praneet
d
for
finding
the
optimal
hyperparameters
for
this
approach
now
you
may
be
thinking:
tables
are
great
but
they
don’t
really
scale
do
they?
while
it
is
easy
to
have
a
16x4
table
for
a
simple
grid
world
the
number
of
possible
states
in
any
modern
game
or
real-world
environment
is
nearly
infinitely
larger
for
most
interesting
problems
tables
simply
don’t
work
we
instead
need
some
way
to
take
a
description
of
our
state
and
produce
q-values
for
actions
without
a
table:
that
is
where
neural
networks
come
in
by
acting
as
a
function
approximator
we
can
take
any
number
of
possible
states
that
can
be
represented
as
a
vector
and
learn
to
map
them
to
q-values
in
the
case
of
the
frozenlake
example
we
will
be
using
a
one-layer
network
which
takes
the
state
encoded
in
a
one-hot
vector
1x16
and
produces
a
vector
of
4
q-values
one
for
each
action
such
a
simple
network
acts
kind
of
like
a
glorified
table
with
the
network
weights
serving
as
the
old
cells
the
key
difference
is
that
we
can
easily
expand
the
tensorflow
network
with
added
layers
activation
functions
and
different
input
types
whereas
all
that
is
impossible
with
a
regular
table
the
method
of
updating
is
a
little
different
as
well
instead
of
directly
updating
our
table
with
a
network
we
will
be
using
backpropagation
and
a
loss
function
our
loss
function
will
be
sum-of-squares
loss
where
the
difference
between
the
current
predicted
q-values
and
the
target
value
is
computed
and
the
gradients
passed
through
the
network
in
this
case
our
q-target
for
the
chosen
action
is
the
equivalent
to
the
q-value
computed
in
equation
1
above
below
is
the
tensorflow
walkthrough
of
implementing
our
simple
q-network:
while
the
network
learns
to
solve
the
frozenlake
problem
it
turns
out
it
doesn’t
do
so
quite
as
efficiently
as
the
q-table
while
neural
networks
allow
for
greater
flexibility
they
do
so
at
the
cost
of
stability
when
it
comes
to
q-learning
there
are
a
number
of
possible
extensions
to
our
simple
q-network
which
allow
for
greater
performance
and
more
robust
learning
two
tricks
in
particular
are
referred
to
as
experience
replay
and
freezing
target
networks
those
improvements
and
other
tweaks
were
the
key
to
getting
atari-playing
deep
q-networks
and
we
will
be
exploring
those
additions
in
the
future
for
more
info
on
the
theory
behind
q-learning
see
this
great
post
by
tambet
matiisen
i
hope
this
tutorial
has
been
helpful
for
those
curious
about
how
to
implement
simple
q-learning
algorithms!
if
this
post
has
been
valuable
to
you
please
consider
donating
to
help
support
future
tutorials
articles
and
implementations
any
contribution
is
greatly
appreciated!
if
you’d
like
to
follow
my
work
on
deep
learning
ai
and
cognitive
science
follow
me
on
medium
@arthur
juliani
or
on
twitter
@awjliani
more
from
my
simple
reinforcement
learning
with
tensorflow
series:
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
deep
learning
@unity3d
""
cognitive
neuroscience
phd
student
exploring
frontier
technology
through
the
lens
of
artificial
intelligence
data
science
and
the
shape
of
things
to
come
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
普通话
""
한국어
tiếng
việt
or
русский
speech
recognition
is
invading
our
lives
it’s
built
into
our
phones
our
game
consoles
and
our
smart
watches
it’s
even
automating
our
homes
for
just
$50
you
can
get
an
amazon
echo
dot
—
a
magic
box
that
allows
you
to
order
pizza
get
a
weather
report
or
even
buy
trash
bags
—
just
by
speaking
out
loud:
the
echo
dot
has
been
so
popular
this
holiday
season
that
amazon
can’t
seem
to
keep
them
in
stock!
but
speech
recognition
has
been
around
for
decades
so
why
is
it
just
now
hitting
the
mainstream?
the
reason
is
that
deep
learning
finally
made
speech
recognition
accurate
enough
to
be
useful
outside
of
carefully
controlled
environments
andrew
ng
has
long
predicted
that
as
speech
recognition
goes
from
95%
accurate
to
99%
accurate
it
will
become
a
primary
way
that
we
interact
with
computers
the
idea
is
that
this
4%
accuracy
gap
is
the
difference
between
annoyingly
unreliable
and
incredibly
useful
thanks
to
deep
learning
we’re
finally
cresting
that
peak
let’s
learn
how
to
do
speech
recognition
with
deep
learning!
if
you
know
how
neural
machine
translation
works
you
might
guess
that
we
could
simply
feed
sound
recordings
into
a
neural
network
and
train
it
to
produce
text:
that’s
the
holy
grail
of
speech
recognition
with
deep
learning
but
we
aren’t
quite
there
yet
at
least
at
the
time
that
i
wrote
this
—
i
bet
that
we
will
be
in
a
couple
of
years
the
big
problem
is
that
speech
varies
in
speed
one
person
might
say
hello!
very
quickly
and
another
person
might
say
heeeelllllllllllllooooo!
very
slowly
producing
a
much
longer
sound
file
with
much
more
data
both
both
sound
files
should
be
recognized
as
exactly
the
same
text
—
hello!
automatically
aligning
audio
files
of
various
lengths
to
a
fixed-length
piece
of
text
turns
out
to
be
pretty
hard
to
work
around
this
we
have
to
use
some
special
tricks
and
extra
precessing
in
addition
to
a
deep
neural
network
let’s
see
how
it
works!
the
first
step
in
speech
recognition
is
obvious
—
we
need
to
feed
sound
waves
into
a
computer
in
part
3
we
learned
how
to
take
an
image
and
treat
it
as
an
array
of
numbers
so
that
we
can
feed
directly
into
a
neural
network
for
image
recognition:
but
sound
is
transmitted
as
waves
how
do
we
turn
sound
waves
into
numbers?
let’s
use
this
sound
clip
of
me
saying
hello:
sound
waves
are
one-dimensional
at
every
moment
in
time
they
have
a
single
value
based
on
the
height
of
the
wave
let’s
zoom
in
on
one
tiny
part
of
the
sound
wave
and
take
a
look:
to
turn
this
sound
wave
into
numbers
we
just
record
of
the
height
of
the
wave
at
equally-spaced
points:
this
is
called
sampling
we
are
taking
a
reading
thousands
of
times
a
second
and
recording
a
number
representing
the
height
of
the
sound
wave
at
that
point
in
time
that’s
basically
all
an
uncompressed
wav
audio
file
is
cd
quality
audio
is
sampled
at
441khz
44100
readings
per
second
but
for
speech
recognition
a
sampling
rate
of
16khz
16000
samples
per
second
is
enough
to
cover
the
frequency
range
of
human
speech
lets
sample
our
hello
sound
wave
16000
times
per
second
here’s
the
first
100
samples:
you
might
be
thinking
that
sampling
is
only
creating
a
rough
approximation
of
the
original
sound
wave
because
it’s
only
taking
occasional
readings
there’s
gaps
in
between
our
readings
so
we
must
be
losing
data
right?
but
thanks
to
the
nyquist
theorem
we
know
that
we
can
use
math
to
perfectly
reconstruct
the
original
sound
wave
from
the
spaced-out
samples
—
as
long
as
we
sample
at
least
twice
as
fast
as
the
highest
frequency
we
want
to
record
i
mention
this
only
because
nearly
everyone
gets
this
wrong
and
assumes
that
using
higher
sampling
rates
always
leads
to
better
audio
quality
it
doesn’t
<end
rant>
we
now
have
an
array
of
numbers
with
each
number
representing
the
sound
wave’s
amplitude
at
116000th
of
a
second
intervals
we
could
feed
these
numbers
right
into
a
neural
network
but
trying
to
recognize
speech
patterns
by
processing
these
samples
directly
is
difficult
instead
we
can
make
the
problem
easier
by
doing
some
pre-processing
on
the
audio
data
let’s
start
by
grouping
our
sampled
audio
into
20-millisecond-long
chunks
here’s
our
first
20
milliseconds
of
audio
ie
our
first
320
samples:
plotting
those
numbers
as
a
simple
line
graph
gives
us
a
rough
approximation
of
the
original
sound
wave
for
that
20
millisecond
period
of
time:
this
recording
is
only
150th
of
a
second
long
but
even
this
short
recording
is
a
complex
mish-mash
of
different
frequencies
of
sound
there’s
some
low
sounds
some
mid-range
sounds
and
even
some
high-pitched
sounds
sprinkled
in
but
taken
all
together
these
different
frequencies
mix
together
to
make
up
the
complex
sound
of
human
speech
to
make
this
data
easier
for
a
neural
network
to
process
we
are
going
to
break
apart
this
complex
sound
wave
into
it’s
component
parts
we’ll
break
out
the
low-pitched
parts
the
next-lowest-pitched-parts
and
so
on
then
by
adding
up
how
much
energy
is
in
each
of
those
frequency
bands
from
low
to
high
we
create
a
fingerprint
of
sorts
for
this
audio
snippet
imagine
you
had
a
recording
of
someone
playing
a
c
major
chord
on
a
piano
that
sound
is
the
combination
of
three
musical
notes—
c
e
and
g
—
all
mixed
together
into
one
complex
sound
we
want
to
break
apart
that
complex
sound
into
the
individual
notes
to
discover
that
they
were
c
e
and
g
this
is
the
exact
same
idea
we
do
this
using
a
mathematic
operation
called
a
fourier
transform
it
breaks
apart
the
complex
sound
wave
into
the
simple
sound
waves
that
make
it
up
once
we
have
those
individual
sound
waves
we
add
up
how
much
energy
is
contained
in
each
one
the
end
result
is
a
score
of
how
important
each
frequency
range
is
from
low
pitch
ie
bass
notes
to
high
pitch
each
number
below
represents
how
much
energy
was
in
each
50hz
band
of
our
20
millisecond
audio
clip:
but
this
is
a
lot
easier
to
see
when
you
draw
this
as
a
chart:
if
we
repeat
this
process
on
every
20
millisecond
chunk
of
audio
we
end
up
with
a
spectrogram
each
column
from
left-to-right
is
one
20ms
chunk:
a
spectrogram
is
cool
because
you
can
actually
see
musical
notes
and
other
pitch
patterns
in
audio
data
a
neural
network
can
find
patterns
in
this
kind
of
data
more
easily
than
raw
sound
waves
so
this
is
the
data
representation
we’ll
actually
feed
into
our
neural
network
now
that
we
have
our
audio
in
a
format
that’s
easy
to
process
we
will
feed
it
into
a
deep
neural
network
the
input
to
the
neural
network
will
be
20
millisecond
audio
chunks
for
each
little
audio
slice
it
will
try
to
figure
out
the
letter
that
corresponds
the
sound
currently
being
spoken
we’ll
use
a
recurrent
neural
network
—
that
is
a
neural
network
that
has
a
memory
that
influences
future
predictions
that’s
because
each
letter
it
predicts
should
affect
the
likelihood
of
the
next
letter
it
will
predict
too
for
example
if
we
have
said
hel
so
far
it’s
very
likely
we
will
say
lo
next
to
finish
out
the
word
hello
it’s
much
less
likely
that
we
will
say
something
unpronounceable
next
like
xyz
so
having
that
memory
of
previous
predictions
helps
the
neural
network
make
more
accurate
predictions
going
forward
after
we
run
our
entire
audio
clip
through
the
neural
network
one
chunk
at
a
time
we’ll
end
up
with
a
mapping
of
each
audio
chunk
to
the
letters
most
likely
spoken
during
that
chunk
here’s
what
that
mapping
looks
like
for
me
saying
hello:
our
neural
net
is
predicting
that
one
likely
thing
i
said
was
hhhee_ll_lllooo
but
it
also
thinks
that
it
was
possible
that
i
said
hhhuu_ll_lllooo
or
even
aaauu_ll_lllooo
we
have
some
steps
we
follow
to
clean
up
this
output
first
we’ll
replace
any
repeated
characters
a
single
character:
then
we’ll
remove
any
blanks:
that
leaves
us
with
three
possible
transcriptions
—
hello
hullo
and
aullo
if
you
say
them
out
loud
all
of
these
sound
similar
to
hello
because
it’s
predicting
one
character
at
a
time
the
neural
network
will
come
up
with
these
very
sounded-out
transcriptions
for
example
if
you
say
he
would
not
go
it
might
give
one
possible
transcription
as
he
wud
net
go
the
trick
is
to
combine
these
pronunciation-based
predictions
with
likelihood
scores
based
on
large
database
of
written
text
books
news
articles
etc
you
throw
out
transcriptions
that
seem
the
least
likely
to
be
real
and
keep
the
transcription
that
seems
the
most
realistic
of
our
possible
transcriptions
hello
hullo
and
aullo
obviously
hello
will
appear
more
frequently
in
a
database
of
text
not
to
mention
in
our
original
audio-based
training
data
and
thus
is
probably
correct
so
we’ll
pick
hello
as
our
final
transcription
instead
of
the
others
done!
you
might
be
thinking
but
what
if
someone
says
‘hullo’?
it’s
a
valid
word
maybe
‘hello’
is
the
wrong
transcription!
of
course
it
is
possible
that
someone
actually
said
hullo
instead
of
hello
but
a
speech
recognition
system
like
this
trained
on
american
english
will
basically
never
produce
hullo
as
the
transcription
it’s
just
such
an
unlikely
thing
for
a
user
to
say
compared
to
hello
that
it
will
always
think
you
are
saying
hello
no
matter
how
much
you
emphasize
the
‘u’
sound
try
it
out!
if
your
phone
is
set
to
american
english
try
to
get
your
phone’s
digital
assistant
to
recognize
the
world
hullo
you
can’t!
it
refuses!
it
will
always
understand
it
as
hello
not
recognizing
hullo
is
a
reasonable
behavior
but
sometimes
you’ll
find
annoying
cases
where
your
phone
just
refuses
to
understand
something
valid
you
are
saying
that’s
why
these
speech
recognition
models
are
always
being
retrained
with
more
data
to
fix
these
edge
cases
one
of
the
coolest
things
about
machine
learning
is
how
simple
it
sometimes
seems
you
get
a
bunch
of
data
feed
it
into
a
machine
learning
algorithm
and
then
magically
you
have
a
world-class
ai
system
running
on
your
gaming
laptop’s
video
card
right?
that
sort
of
true
in
some
cases
but
not
for
speech
recognizing
speech
is
a
hard
problem
you
have
to
overcome
almost
limitless
challenges:
bad
quality
microphones
background
noise
reverb
and
echo
accent
variations
and
on
and
on
all
of
these
issues
need
to
be
present
in
your
training
data
to
make
sure
the
neural
network
can
deal
with
them
here’s
another
example:
did
you
know
that
when
you
speak
in
a
loud
room
you
unconsciously
raise
the
pitch
of
your
voice
to
be
able
to
talk
over
the
noise?
humans
have
no
problem
understanding
you
either
way
but
neural
networks
need
to
be
trained
to
handle
this
special
case
so
you
need
training
data
with
people
yelling
over
noise!
to
build
a
voice
recognition
system
that
performs
on
the
level
of
siri
google
now!
or
alexa
you
will
need
a
lot
of
training
data
—
far
more
data
than
you
can
likely
get
without
hiring
hundreds
of
people
to
record
it
for
you
and
since
users
have
low
tolerance
for
poor
quality
voice
recognition
systems
you
can’t
skimp
on
this
no
one
wants
a
voice
recognition
system
that
works
80%
of
the
time
for
a
company
like
google
or
amazon
hundreds
of
thousands
of
hours
of
spoken
audio
recorded
in
real-life
situations
is
gold
that’s
the
single
biggest
thing
that
separates
their
world-class
speech
recognition
system
from
your
hobby
system
the
whole
point
of
putting
google
now!
and
siri
on
every
cell
phone
for
free
or
selling
$50
alexa
units
that
have
no
subscription
fee
is
to
get
you
to
use
them
as
much
as
possible
every
single
thing
you
say
into
one
of
these
systems
is
recorded
forever
and
used
as
training
data
for
future
versions
of
speech
recognition
algorithms
that’s
the
whole
game!
don’t
believe
me?
if
you
have
an
android
phone
with
google
now!
click
here
to
listen
to
actual
recordings
of
yourself
saying
every
dumb
thing
you’ve
ever
said
into
it:
so
if
you
are
looking
for
a
start-up
idea
i
wouldn’t
recommend
trying
to
build
your
own
speech
recognition
system
to
compete
with
google
instead
figure
out
a
way
to
get
people
to
give
you
recordings
of
themselves
talking
for
hours
the
data
can
be
your
product
instead
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
email
list
i’ll
only
email
you
when
i
have
something
new
and
awesome
to
share
it’s
the
best
way
to
find
out
when
i
write
more
articles
like
this
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun!
part
7!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
普通话
русский
한국어
tiếng
việt
or
italiano
we
all
know
and
love
google
translate
the
website
that
can
instantly
translate
between
100
different
human
languages
as
if
by
magic
it
is
even
available
on
our
phones
and
smartwatches:
the
technology
behind
google
translate
is
called
machine
translation
it
has
changed
the
world
by
allowing
people
to
communicate
when
it
wouldn’t
otherwise
be
possible
but
we
all
know
that
high
school
students
have
been
using
google
translate
to
umm
assist
with
their
spanish
homework
for
15
years
isn’t
this
old
news?
it
turns
out
that
over
the
past
two
years
deep
learning
has
totally
rewritten
our
approach
to
machine
translation
deep
learning
researchers
who
know
almost
nothing
about
language
translation
are
throwing
together
relatively
simple
machine
learning
solutions
that
are
beating
the
best
expert-built
language
translation
systems
in
the
world
the
technology
behind
this
breakthrough
is
called
sequence-to-sequence
learning
it’s
very
powerful
technique
that
be
used
to
solve
many
kinds
problems
after
we
see
how
it
is
used
for
translation
we’ll
also
learn
how
the
exact
same
algorithm
can
be
used
to
write
ai
chat
bots
and
describe
pictures
let’s
go!
so
how
do
we
program
a
computer
to
translate
human
language?
the
simplest
approach
is
to
replace
every
word
in
a
sentence
with
the
translated
word
in
the
target
language
here’s
a
simple
example
of
translating
from
spanish
to
english
word-by-word:
this
is
easy
to
implement
because
all
you
need
is
a
dictionary
to
look
up
each
word’s
translation
but
the
results
are
bad
because
it
ignores
grammar
and
context
so
the
next
thing
you
might
do
is
start
adding
language-specific
rules
to
improve
the
results
for
example
you
might
translate
common
two-word
phrases
as
a
single
group
and
you
might
swap
the
order
nouns
and
adjectives
since
they
usually
appear
in
reverse
order
in
spanish
from
how
they
appear
in
english:
that
worked!
if
we
just
keep
adding
more
rules
until
we
can
handle
every
part
of
grammar
our
program
should
be
able
to
translate
any
sentence
right?
this
is
how
the
earliest
machine
translation
systems
worked
linguists
came
up
with
complicated
rules
and
programmed
them
in
one-by-one
some
of
the
smartest
linguists
in
the
world
labored
for
years
during
the
cold
war
to
create
translation
systems
as
a
way
to
interpret
russian
communications
more
easily
unfortunately
this
only
worked
for
simple
plainly-structured
documents
like
weather
reports
it
didn’t
work
reliably
for
real-world
documents
the
problem
is
that
human
language
doesn’t
follow
a
fixed
set
of
rules
human
languages
are
full
of
special
cases
regional
variations
and
just
flat
out
rule-breaking
the
way
we
speak
english
more
influenced
by
who
invaded
who
hundreds
of
years
ago
than
it
is
by
someone
sitting
down
and
defining
grammar
rules
after
the
failure
of
rule-based
systems
new
translation
approaches
were
developed
using
models
based
on
probability
and
statistics
instead
of
grammar
rules
building
a
statistics-based
translation
system
requires
lots
of
training
data
where
the
exact
same
text
is
translated
into
at
least
two
languages
this
double-translated
text
is
called
parallel
corpora
in
the
same
way
that
the
rosetta
stone
was
used
by
scientists
in
the
1800s
to
figure
out
egyptian
hieroglyphs
from
greek
computers
can
use
parallel
corpora
to
guess
how
to
convert
text
from
one
language
to
another
luckily
there’s
lots
of
double-translated
text
already
sitting
around
in
strange
places
for
example
the
european
parliament
translates
their
proceedings
into
21
languages
so
researchers
often
use
that
data
to
help
build
translation
systems
the
fundamental
difference
with
statistical
translation
systems
is
that
they
don’t
try
to
generate
one
exact
translation
instead
they
generate
thousands
of
possible
translations
and
then
they
rank
those
translations
by
likely
each
is
to
be
correct
they
estimate
how
correct
something
is
by
how
similar
it
is
to
the
training
data
here’s
how
it
works:
first
we
break
up
our
sentence
into
simple
chunks
that
can
each
be
easily
translated:
next
we
will
translate
each
of
these
chunks
by
finding
all
the
ways
humans
have
translated
those
same
chunks
of
words
in
our
training
data
it’s
important
to
note
that
we
are
not
just
looking
up
these
chunks
in
a
simple
translation
dictionary
instead
we
are
seeing
how
actual
people
translated
these
same
chunks
of
words
in
real-world
sentences
this
helps
us
capture
all
of
the
different
ways
they
can
be
used
in
different
contexts:
some
of
these
possible
translations
are
used
more
frequently
than
others
based
on
how
frequently
each
translation
appears
in
our
training
data
we
can
give
it
a
score
for
example
it’s
much
more
common
for
someone
to
say
quiero
to
mean
i
want
than
to
mean
i
try
so
we
can
use
how
frequently
quiero
was
translated
to
i
want
in
our
training
data
to
give
that
translation
more
weight
than
a
less
frequent
translation
next
we
will
use
every
possible
combination
of
these
chunks
to
generate
a
bunch
of
possible
sentences
just
from
the
chunk
translations
we
listed
in
step
2
we
can
already
generate
nearly
2500
different
variations
of
our
sentence
by
combining
the
chunks
in
different
ways
here
are
some
examples:
but
in
a
real-world
system
there
will
be
even
more
possible
chunk
combinations
because
we’ll
also
try
different
orderings
of
words
and
different
ways
of
chunking
the
sentence:
now
need
to
scan
through
all
of
these
generated
sentences
to
find
the
one
that
is
that
sounds
the
most
human
to
do
this
we
compare
each
generated
sentence
to
millions
of
real
sentences
from
books
and
news
stories
written
in
english
the
more
english
text
we
can
get
our
hands
on
the
better
take
this
possible
translation:
it’s
likely
that
no
one
has
ever
written
a
sentence
like
this
in
english
so
it
would
not
be
very
similar
to
any
sentences
in
our
data
set
we’ll
give
this
possible
translation
a
low
probability
score
but
look
at
this
possible
translation:
this
sentence
will
be
similar
to
something
in
our
training
set
so
it
will
get
a
high
probability
score
after
trying
all
possible
sentences
we’ll
pick
the
sentence
that
has
the
most
likely
chunk
translations
while
also
being
the
most
similar
overall
to
real
english
sentences
our
final
translation
would
be
i
want
to
go
to
the
prettiest
beach
not
bad!
statistical
machine
translation
systems
perform
much
better
than
rule-based
systems
if
you
give
them
enough
training
data
franz
josef
och
improved
on
these
ideas
and
used
them
to
build
google
translate
in
the
early
2000s
machine
translation
was
finally
available
to
the
world
in
the
early
days
it
was
surprising
to
everyone
that
the
dumb
approach
to
translating
based
on
probability
worked
better
than
rule-based
systems
designed
by
linguists
this
led
to
a
somewhat
mean
saying
among
researchers
in
the
80s:
statistical
machine
translation
systems
work
well
but
they
are
complicated
to
build
and
maintain
every
new
pair
of
languages
you
want
to
translate
requires
experts
to
tweak
and
tune
a
new
multi-step
translation
pipeline
because
it
is
so
much
work
to
build
these
different
pipelines
trade-offs
have
to
be
made
if
you
are
asking
google
to
translate
georgian
to
telegu
it
has
to
internally
translate
it
into
english
as
an
intermediate
step
because
there’s
not
enough
georgain-to-telegu
translations
happening
to
justify
investing
heavily
in
that
language
pair
and
it
might
do
that
translation
using
a
less
advanced
translation
pipeline
than
if
you
had
asked
it
for
the
more
common
choice
of
french-to-english
wouldn’t
it
be
cool
if
we
could
have
the
computer
do
all
that
annoying
development
work
for
us?
the
holy
grail
of
machine
translation
is
a
black
box
system
that
learns
how
to
translate
by
itself—
just
by
looking
at
training
data
with
statistical
machine
translation
humans
are
still
needed
to
build
and
tweak
the
multi-step
statistical
models
in
2014
kyunghyun
cho’s
team
made
a
breakthrough
they
found
a
way
to
apply
deep
learning
to
build
this
black
box
system
their
deep
learning
model
takes
in
a
parallel
corpora
and
and
uses
it
to
learn
how
to
translate
between
those
two
languages
without
any
human
intervention
two
big
ideas
make
this
possible
—
recurrent
neural
networks
and
encodings
by
combining
these
two
ideas
in
a
clever
way
we
can
build
a
self-learning
translation
system
we’ve
already
talked
about
recurrent
neural
networks
in
part
2
but
let’s
quickly
review
a
regular
non-recurrent
neural
network
is
a
generic
machine
learning
algorithm
that
takes
in
a
list
of
numbers
and
calculates
a
result
based
on
previous
training
neural
networks
can
be
used
as
a
black
box
to
solve
lots
of
problems
for
example
we
can
use
a
neural
network
to
calculate
the
approximate
value
of
a
house
based
on
attributes
of
that
house:
but
like
most
machine
learning
algorithms
neural
networks
are
stateless
you
pass
in
a
list
of
numbers
and
the
neural
network
calculates
a
result
if
you
pass
in
those
same
numbers
again
it
will
always
calculate
the
same
result
it
has
no
memory
of
past
calculations
in
other
words
2
""
2
always
equals
4
a
recurrent
neural
network
or
rnn
for
short
is
a
slightly
tweaked
version
of
a
neural
network
where
the
previous
state
of
the
neural
network
is
one
of
the
inputs
to
the
next
calculation
this
means
that
previous
calculations
change
the
results
of
future
calculations!
why
in
the
world
would
we
want
to
do
this?
shouldn’t
2
""
2
always
equal
4
no
matter
what
we
last
calculated?
this
trick
allows
neural
networks
to
learn
patterns
in
a
sequence
of
data
for
example
you
can
use
it
to
predict
the
next
most
likely
word
in
a
sentence
based
on
the
first
few
words:
rnns
are
useful
any
time
you
want
to
learn
patterns
in
data
because
human
language
is
just
one
big
complicated
pattern
rnns
are
increasingly
used
in
many
areas
of
natural
language
processing
if
you
want
to
learn
more
about
rnns
you
can
read
part
2
where
we
used
one
to
generate
a
fake
ernest
hemingway
book
and
then
used
another
one
to
generate
fake
super
mario
brothers
levels
the
other
idea
we
need
to
review
is
encodings
we
talked
about
encodings
in
part
4
as
part
of
face
recognition
to
explain
encodings
let’s
take
a
slight
detour
into
how
we
can
tell
two
different
people
apart
with
a
computer
when
you
are
trying
to
tell
two
faces
apart
with
a
computer
you
collect
different
measurements
from
each
face
and
use
those
measurements
to
compare
faces
for
example
we
might
measure
the
size
of
each
ear
or
the
spacing
between
the
eyes
and
compare
those
measurements
from
two
pictures
to
see
if
they
are
the
same
person
you’re
probably
already
familiar
with
this
idea
from
watching
any
primetime
detective
show
like
csi:
the
idea
of
turning
a
face
into
a
list
of
measurements
is
an
example
of
an
encoding
we
are
taking
raw
data
a
picture
of
a
face
and
turning
it
into
a
list
of
measurements
that
represent
it
the
encoding
but
like
we
saw
in
part
4
we
don’t
have
to
come
up
with
a
specific
list
of
facial
features
to
measure
ourselves
instead
we
can
use
a
neural
network
to
generate
measurements
from
a
face
the
computer
can
do
a
better
job
than
us
in
figuring
out
which
measurements
are
best
able
to
differentiate
two
similar
people:
this
is
our
encoding
it
lets
us
represent
something
very
complicated
a
picture
of
a
face
with
something
simple
128
numbers
now
comparing
two
different
faces
is
much
easier
because
we
only
have
to
compare
these
128
numbers
for
each
face
instead
of
comparing
full
images
guess
what?
we
can
do
the
same
thing
with
sentences!
we
can
come
up
with
an
encoding
that
represents
every
possible
different
sentence
as
a
series
of
unique
numbers:
to
generate
this
encoding
we’ll
feed
the
sentence
into
the
rnn
one
word
at
time
the
final
result
after
the
last
word
is
processed
will
be
the
values
that
represent
the
entire
sentence:
great
so
now
we
have
a
way
to
represent
an
entire
sentence
as
a
set
of
unique
numbers!
we
don’t
know
what
each
number
in
the
encoding
means
but
it
doesn’t
really
matter
as
long
as
each
sentence
is
uniquely
identified
by
it’s
own
set
of
numbers
we
don’t
need
to
know
exactly
how
those
numbers
were
generated
ok
so
we
know
how
to
use
an
rnn
to
encode
a
sentence
into
a
set
of
unique
numbers
how
does
that
help
us?
here’s
where
things
get
really
cool!
what
if
we
took
two
rnns
and
hooked
them
up
end-to-end?
the
first
rnn
could
generate
the
encoding
that
represents
a
sentence
then
the
second
rnn
could
take
that
encoding
and
just
do
the
same
logic
in
reverse
to
decode
the
original
sentence
again:
of
course
being
able
to
encode
and
then
decode
the
original
sentence
again
isn’t
very
useful
but
what
if
and
here’s
the
big
idea!
we
could
train
the
second
rnn
to
decode
the
sentence
into
spanish
instead
of
english?
we
could
use
our
parallel
corpora
training
data
to
train
it
to
do
that:
and
just
like
that
we
have
a
generic
way
of
converting
a
sequence
of
english
words
into
an
equivalent
sequence
of
spanish
words!
this
is
a
powerful
idea:
note
that
we
glossed
over
some
things
that
are
required
to
make
this
work
with
real-world
data
for
example
there’s
additional
work
you
have
to
do
to
deal
with
different
lengths
of
input
and
output
sentences
see
bucketing
and
padding
there’s
also
issues
with
translating
rare
words
correctly
if
you
want
to
build
your
own
language
translation
system
there’s
a
working
demo
included
with
tensorflow
that
will
translate
between
english
and
french
however
this
is
not
for
the
faint
of
heart
or
for
those
with
limited
budgets
this
technology
is
still
new
and
very
resource
intensive
even
if
you
have
a
fast
computer
with
a
high-end
video
card
it
might
take
about
a
month
of
continuous
processing
time
to
train
your
own
language
translation
system
also
sequence-to-sequence
language
translation
techniques
are
improving
so
rapidly
that
it’s
hard
to
keep
up
many
recent
improvements
like
adding
an
attention
mechanism
or
tracking
context
are
significantly
improving
results
but
these
developments
are
so
new
that
there
aren’t
even
wikipedia
pages
for
them
yet
if
you
want
to
do
anything
serious
with
sequence-to-sequence
learning
you’ll
need
to
keep
with
new
developments
as
they
occur
so
what
else
can
we
do
with
sequence-to-sequence
models?
about
a
year
ago
researchers
at
google
showed
that
you
can
use
sequence-to-sequence
models
to
build
ai
bots
the
idea
is
so
simple
that
it’s
amazing
it
works
at
all
first
they
captured
chat
logs
between
google
employees
and
google’s
tech
support
team
then
they
trained
a
sequence-to-sequence
model
where
the
employee’s
question
was
the
input
sentence
and
the
tech
support
team’s
response
was
the
translation
of
that
sentence
when
a
user
interacted
with
the
bot
they
would
translate
each
of
the
user’s
messages
with
this
system
to
get
the
bot’s
response
the
end
result
was
a
semi-intelligent
bot
that
could
sometimes
answer
real
tech
support
questions
here’s
part
of
a
sample
conversation
between
a
user
and
the
bot
from
their
paper:
they
also
tried
building
a
chat
bot
based
on
millions
of
movie
subtitles
the
idea
was
to
use
conversations
between
movie
characters
as
a
way
to
train
a
bot
to
talk
like
a
human
the
input
sentence
is
a
line
of
dialog
said
by
one
character
and
the
translation
is
what
the
next
character
said
in
response:
this
produced
really
interesting
results
not
only
did
the
bot
converse
like
a
human
but
it
displayed
a
small
bit
of
intelligence:
this
is
only
the
beginning
of
the
possibilities
we
aren’t
limited
to
converting
one
sentence
into
another
sentence
it’s
also
possible
to
make
an
image-to-sequence
model
that
can
turn
an
image
into
text!
a
different
team
at
google
did
this
by
replacing
the
first
rnn
with
a
convolutional
neural
network
like
we
learned
about
in
part
3
this
allows
the
input
to
be
a
picture
instead
of
a
sentence
the
rest
works
basically
the
same
way:
and
just
like
that
we
can
turn
pictures
into
words
as
long
as
we
have
lots
and
lots
of
training
data!
andrej
karpathy
expanded
on
these
ideas
to
build
a
system
capable
of
describing
images
in
great
detail
by
processing
multiple
regions
of
an
image
separately:
this
makes
it
possible
to
build
image
search
engines
that
are
capable
of
finding
images
that
match
oddly
specific
search
queries:
there’s
even
researchers
working
on
the
reverse
problem
generating
an
entire
picture
based
on
just
a
text
description!
just
from
these
examples
you
can
start
to
imagine
the
possibilities
so
far
there
have
been
sequence-to-sequence
applications
in
everything
from
speech
recognition
to
computer
vision
i
bet
there
will
be
a
lot
more
over
the
next
year
if
you
want
to
learn
more
in
depth
about
sequence-to-sequence
models
and
translation
here’s
some
recommended
resources:
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
email
list
i’ll
only
email
you
when
i
have
something
new
and
awesome
to
share
it’s
the
best
way
to
find
out
when
i
write
more
articles
like
this
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun!
part
6!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
update
25117
—
took
me
a
while
but
here
is
an
ipython
notebook
with
a
rough
implementation
in
the
past
few
months
i’ve
been
fascinated
with
deep
learning
especially
its
applications
to
language
and
text
i’ve
spent
the
bulk
of
my
career
in
financial
technologies
mostly
in
algorithmic
trading
and
alternative
data
services
you
can
see
where
this
is
going
i
wrote
this
to
get
my
ideas
straight
in
my
head
while
i’ve
become
a
deep
learning
enthusiast
i
don’t
have
too
many
opportunities
to
brain
dump
an
idea
in
most
of
its
messy
glory
i
think
that
a
decent
indication
of
a
clear
thought
is
the
ability
to
articulate
it
to
people
not
from
the
field
i
hope
that
i’ve
succeeded
in
doing
that
and
that
my
articulation
is
also
a
pleasurable
read
why
nlp
is
relevant
to
stock
prediction
in
many
nlp
problems
we
end
up
taking
a
sequence
and
encoding
it
into
a
single
fixed
size
representation
then
decoding
that
representation
into
another
sequence
for
example
we
might
tag
entities
in
the
text
translate
from
english
to
french
or
convert
audio
frequencies
to
text
there
is
a
torrent
of
work
coming
out
in
these
areas
and
a
lot
of
the
results
are
achieving
state
of
the
art
performance
in
my
mind
the
biggest
difference
between
the
nlp
and
financial
analysis
is
that
language
has
some
guarantee
of
structure
it’s
just
that
the
rules
of
the
structure
are
vague
markets
on
the
other
hand
don’t
come
with
a
promise
of
a
learnable
structure
that
such
a
structure
exists
is
the
assumption
that
this
project
would
prove
or
disprove
rather
it
might
prove
or
disprove
if
i
can
find
that
structure
assuming
the
structure
is
there
the
idea
of
summarizing
the
current
state
of
the
market
in
the
same
way
we
encode
the
semantics
of
a
paragraph
seems
plausible
to
me
if
that
doesn’t
make
sense
yet
keep
reading
it
will
you
shall
know
a
word
by
the
company
it
keeps
firth
j
r
1957:11
there
is
tons
of
literature
on
word
embeddings
richard
socher’s
lecture
is
a
great
place
to
start
in
short
we
can
make
a
geometry
of
all
the
words
in
our
language
and
that
geometry
captures
the
meaning
of
words
and
relationships
between
them
you
may
have
seen
the
example
of
king-man
woman=queen
or
something
of
the
sort
embeddings
are
cool
because
they
let
us
represent
information
in
a
condensed
way
the
old
way
of
representing
words
was
holding
a
vector
a
big
list
of
numbers
that
was
as
long
as
the
number
of
words
we
know
and
setting
a
1
in
a
particular
place
if
that
was
the
current
word
we
are
looking
at
that
is
not
an
efficient
approach
nor
does
it
capture
any
meaning
with
embeddings
we
can
represent
all
of
the
words
in
a
fixed
number
of
dimensions
300
seems
to
be
plenty
50
works
great
and
then
leverage
their
higher
dimensional
geometry
to
understand
them
the
picture
below
shows
an
example
an
embedding
was
trained
on
more
or
less
the
entire
internet
after
a
few
days
of
intensive
calculations
each
word
was
embedded
in
some
high
dimensional
space
this
space
has
a
geometry
concepts
like
distance
and
so
we
can
ask
which
words
are
close
together
the
authorsinventors
of
that
method
made
an
example
here
are
the
words
that
are
closest
to
frog
but
we
can
embed
more
than
just
words
we
can
do
say
""
stock
market
embeddings
market2vec
the
first
word
embedding
algorithm
i
heard
about
was
word2vec
i
want
to
get
the
same
effect
for
the
market
though
i’ll
be
using
a
different
algorithm
my
input
data
is
a
csv
the
first
column
is
the
date
and
there
are
4*1000
columns
corresponding
to
the
high
low
open
closing
price
of
1000
stocks
that
is
my
input
vector
is
4000
dimensional
which
is
too
big
so
the
first
thing
i’m
going
to
do
is
stuff
it
into
a
lower
dimensional
space
say
300
because
i
liked
the
movie
taking
something
in
4000
dimensions
and
stuffing
it
into
a
300-dimensional
space
my
sound
hard
but
its
actually
easy
we
just
need
to
multiply
matrices
a
matrix
is
a
big
excel
spreadsheet
that
has
numbers
in
every
cell
and
no
formatting
problems
imagine
an
excel
table
with
4000
columns
and
300
rows
and
when
we
basically
bang
it
against
the
vector
a
new
vector
comes
out
that
is
only
of
size
300
i
wish
that’s
how
they
would
have
explained
it
in
college
the
fanciness
starts
here
as
we’re
going
to
set
the
numbers
in
our
matrix
at
random
and
part
of
the
deep
learning
is
to
update
those
numbers
so
that
our
excel
spreadsheet
changes
eventually
this
matrix
spreadsheet
i’ll
stick
with
matrix
from
now
on
will
have
numbers
in
it
that
bang
our
original
4000
dimensional
vector
into
a
concise
300
dimensional
summary
of
itself
we’re
going
to
get
a
little
fancier
here
and
apply
what
they
call
an
activation
function
we’re
going
to
take
a
function
and
apply
it
to
each
number
in
the
vector
individually
so
that
they
all
end
up
between
0
and
1
or
0
and
infinity
it
depends
why
?
it
makes
our
vector
more
special
and
makes
our
learning
process
able
to
understand
more
complicated
things
how?
so
what?
what
i’m
expecting
to
find
is
that
that
new
embedding
of
the
market
prices
the
vector
into
a
smaller
space
captures
all
the
essential
information
for
the
task
at
hand
without
wasting
time
on
the
other
stuff
so
i’d
expect
they’d
capture
correlations
between
other
stocks
perhaps
notice
when
a
certain
sector
is
declining
or
when
the
market
is
very
hot
i
don’t
know
what
traits
it
will
find
but
i
assume
they’ll
be
useful
now
what
lets
put
aside
our
market
vectors
for
a
moment
and
talk
about
language
models
andrej
karpathy
wrote
the
epic
post
the
unreasonable
effectiveness
of
recurrent
neural
networks
if
i’d
summarize
in
the
most
liberal
fashion
the
post
boils
down
to
and
then
as
a
punchline
he
generated
a
bunch
of
text
that
looks
like
shakespeare
and
then
he
did
it
again
with
the
linux
source
code
and
then
again
with
a
textbook
on
algebraic
geometry
so
i’ll
get
back
to
the
mechanics
of
that
magic
box
in
a
second
but
let
me
remind
you
that
we
want
to
predict
the
future
market
based
on
the
past
just
like
he
predicted
the
next
word
based
on
the
previous
one
where
karpathy
used
characters
we’re
going
to
use
our
market
vectors
and
feed
them
into
the
magic
black
box
we
haven’t
decided
what
we
want
it
to
predict
yet
but
that
is
okay
we
won’t
be
feeding
its
output
back
into
it
either
going
deeper
i
want
to
point
out
that
this
is
where
we
start
to
get
into
the
deep
part
of
deep
learning
so
far
we
just
have
a
single
layer
of
learning
that
excel
spreadsheet
that
condenses
the
market
now
we’re
going
to
add
a
few
more
layers
and
stack
them
to
make
a
deep
something
that’s
the
deep
in
deep
learning
so
karpathy
shows
us
some
sample
output
from
the
linux
source
code
this
is
stuff
his
black
box
wrote
notice
that
it
knows
how
to
open
and
close
parentheses
and
respects
indentation
conventions
the
contents
of
the
function
are
properly
indented
and
the
multi-line
printk
statement
has
an
inner
indentation
that
means
that
this
magic
box
understands
long
range
dependencies
when
it’s
indenting
within
the
print
statement
it
knows
it’s
in
a
print
statement
and
also
remembers
that
it’s
in
a
function
or
at
least
another
indented
scope
that’s
nuts
it’s
easy
to
gloss
over
that
but
an
algorithm
that
has
the
ability
to
capture
and
remember
long
term
dependencies
is
super
useful
because
we
want
to
find
long
term
dependencies
in
the
market
inside
the
magical
black
box
what’s
inside
this
magical
black
box?
it
is
a
type
of
recurrent
neural
network
rnn
called
an
lstm
an
rnn
is
a
deep
learning
algorithm
that
operates
on
sequences
like
sequences
of
characters
at
every
step
it
takes
a
representation
of
the
next
character
like
the
embeddings
we
talked
about
before
and
operates
on
the
representation
with
a
matrix
like
we
saw
before
the
thing
is
the
rnn
has
some
form
of
internal
memory
so
it
remembers
what
it
saw
previously
it
uses
that
memory
to
decide
how
exactly
it
should
operate
on
the
next
input
using
that
memory
the
rnn
can
remember
that
it
is
inside
of
an
intended
scope
and
that
is
how
we
get
properly
nested
output
text
a
fancy
version
of
an
rnn
is
called
a
long
short
term
memory
lstm
lstm
has
cleverly
designed
memory
that
allows
it
to
so
an
lstm
can
see
a
{
and
say
to
itself
oh
yeah
that’s
important
i
should
remember
that
and
when
it
does
it
essentially
remembers
an
indication
that
it
is
in
a
nested
scope
once
it
sees
the
corresponding
}
it
can
decide
to
forget
the
original
opening
brace
and
thus
forget
that
it
is
in
a
nested
scope
we
can
have
the
lstm
learn
more
abstract
concepts
by
stacking
a
few
of
them
on
top
of
each
other
that
would
make
us
deep
again
now
each
output
of
the
previous
lstm
becomes
the
inputs
of
the
next
lstm
and
each
one
goes
on
to
learn
higher
abstractions
of
the
data
coming
in
in
the
example
above
and
this
is
just
illustrative
speculation
the
first
layer
of
lstms
might
learn
that
characters
separated
by
a
space
are
words
the
next
layer
might
learn
word
types
like
static
void
action_new_functionthe
next
layer
might
learn
the
concept
of
a
function
and
its
arguments
and
so
on
it’s
hard
to
tell
exactly
what
each
layer
is
doing
though
karpathy’s
blog
has
a
really
nice
example
of
how
he
did
visualize
exactly
that
connecting
market2vec
and
lstms
the
studious
reader
will
notice
that
karpathy
used
characters
as
his
inputs
not
embeddings
technically
a
one-hot
encoding
of
characters
but
lars
eidnes
actually
used
word
embeddings
when
he
wrote
auto-generating
clickbait
with
recurrent
neural
network
the
figure
above
shows
the
network
he
used
ignore
the
softmax
part
we’ll
get
to
it
later
for
the
moment
check
out
how
on
the
bottom
he
puts
in
a
sequence
of
words
vectors
at
the
bottom
and
each
one
remember
a
word
vector
is
a
representation
of
a
word
in
the
form
of
a
bunch
of
numbers
like
we
saw
in
the
beginning
of
this
post
lars
inputs
a
sequence
of
word
vectors
and
each
one
of
them:
we’re
going
to
do
the
same
thing
with
one
difference
instead
of
word
vectors
we’ll
input
marketvectors
those
market
vectors
we
described
before
to
recap
the
marketvectors
should
contain
a
summary
of
what’s
happening
in
the
market
at
a
given
point
in
time
by
putting
a
sequence
of
them
through
lstms
i
hope
to
capture
the
long
term
dynamics
that
have
been
happening
in
the
market
by
stacking
together
a
few
layers
of
lstms
i
hope
to
capture
higher
level
abstractions
of
the
market’s
behavior
what
comes
out
thus
far
we
haven’t
talked
at
all
about
how
the
algorithm
actually
learns
anything
we
just
talked
about
all
the
clever
transformations
we’ll
do
on
the
data
we’ll
defer
that
conversation
to
a
few
paragraphs
down
but
please
keep
this
part
in
mind
as
it
is
the
se
up
for
the
punch
line
that
makes
everything
else
worthwhile
in
karpathy’s
example
the
output
of
the
lstms
is
a
vector
that
represents
the
next
character
in
some
abstract
representation
in
eidnes’
example
the
output
of
the
lstms
is
a
vector
that
represents
what
the
next
word
will
be
in
some
abstract
space
the
next
step
in
both
cases
is
to
change
that
abstract
representation
into
a
probability
vector
that
is
a
list
that
says
how
likely
each
character
or
word
respectively
is
likely
to
appear
next
that’s
the
job
of
the
softmax
function
once
we
have
a
list
of
likelihoods
we
select
the
character
or
word
that
is
the
most
likely
to
appear
next
in
our
case
of
predicting
the
market
we
need
to
ask
ourselves
what
exactly
we
want
to
market
to
predict?
some
of
the
options
that
i
thought
about
were:
1
and
2
are
regression
problems
where
we
have
to
predict
an
actual
number
instead
of
the
likelihood
of
a
specific
event
like
the
letter
n
appearing
or
the
market
going
up
those
are
fine
but
not
what
i
want
to
do
3
and
4
are
fairly
similar
they
both
ask
to
predict
an
event
in
technical
jargon
—
a
class
label
an
event
could
be
the
letter
n
appearing
next
or
it
could
be
moved
up
5%
while
not
going
down
more
than
3%
in
the
last
10
minutes
the
trade-off
between
3
and
4
is
that
3
is
much
more
common
and
thus
easier
to
learn
about
while
4
is
more
valuable
as
not
only
is
it
an
indicator
of
profit
but
also
has
some
constraint
on
risk
5
is
the
one
we’ll
continue
with
for
this
article
because
it’s
similar
to
3
and
4
but
has
mechanics
that
are
easier
to
follow
the
vix
is
sometimes
called
the
fear
index
and
it
represents
how
volatile
the
stocks
in
the
sp500
are
it
is
derived
by
observing
the
implied
volatility
for
specific
options
on
each
of
the
stocks
in
the
index
sidenote
—
why
predict
the
vix
what
makes
the
vix
an
interesting
target
is
that
back
to
our
lstm
outputs
and
the
softmax
how
do
we
use
the
formulations
we
saw
before
to
predict
changes
in
the
vix
a
few
minutes
in
the
future?
for
each
point
in
our
dataset
we’ll
look
what
happened
to
the
vix
5
minutes
later
if
it
went
up
by
more
than
1%
without
going
down
more
than
05%
during
that
time
we’ll
output
a
1
otherwise
a
0
then
we’ll
get
a
sequence
that
looks
like:
we
want
to
take
the
vector
that
our
lstms
output
and
squish
it
so
that
it
gives
us
the
probability
of
the
next
item
in
our
sequence
being
a
1
the
squishing
happens
in
the
softmax
part
of
the
diagram
above
technically
since
we
only
have
1
class
now
we
use
a
sigmoid
""
so
before
we
get
into
how
this
thing
learns
let’s
recap
what
we’ve
done
so
far
how
does
this
thing
learn?
now
the
fun
part
everything
we
did
until
now
was
called
the
forward
pass
we’d
do
all
of
those
steps
while
we
train
the
algorithm
and
also
when
we
use
it
in
production
here
we’ll
talk
about
the
backward
pass
the
part
we
do
only
while
in
training
that
makes
our
algorithm
learn
so
during
training
not
only
did
we
prepare
years
worth
of
historical
data
we
also
prepared
a
sequence
of
prediction
targets
that
list
of
0
and
1
that
showed
if
the
vix
moved
the
way
we
want
it
to
or
not
after
each
observation
in
our
data
to
learn
we’ll
feed
the
market
data
to
our
network
and
compare
its
output
to
what
we
calculated
comparing
in
our
case
will
be
simple
subtraction
that
is
we’ll
say
that
our
model’s
error
is
or
in
english
the
square
root
of
the
square
of
the
difference
between
what
actually
happened
and
what
we
predicted
here’s
the
beauty
that’s
a
differential
function
that
is
we
can
tell
by
how
much
the
error
would
have
changed
if
our
prediction
would
have
changed
a
little
our
prediction
is
the
outcome
of
a
differentiable
function
the
softmax
the
inputs
to
the
softmax
the
lstms
are
all
mathematical
functions
that
are
differentiable
now
all
of
these
functions
are
full
of
parameters
those
big
excel
spreadsheets
i
talked
about
ages
ago
so
at
this
stage
what
we
do
is
take
the
derivative
of
the
error
with
respect
to
every
one
of
the
millions
of
parameters
in
all
of
those
excel
spreadsheets
we
have
in
our
model
when
we
do
that
we
can
see
how
the
error
will
change
when
we
change
each
parameter
so
we’ll
change
each
parameter
in
a
way
that
will
reduce
the
error
this
procedure
propagates
all
the
way
to
the
beginning
of
the
model
it
tweaks
the
way
we
embed
the
inputs
into
marketvectors
so
that
our
marketvectors
represent
the
most
significant
information
for
our
task
it
tweaks
when
and
what
each
lstm
chooses
to
remember
so
that
their
outputs
are
the
most
relevant
to
our
task
it
tweaks
the
abstractions
our
lstms
learn
so
that
they
learn
the
most
important
abstractions
for
our
task
which
in
my
opinion
is
amazing
because
we
have
all
of
this
complexity
and
abstraction
that
we
never
had
to
specify
anywhere
it’s
all
inferred
mathamagically
from
the
specification
of
what
we
consider
to
be
an
error
what’s
next
now
that
i’ve
laid
this
out
in
writing
and
it
still
makes
sense
to
me
i
want
so
if
you’ve
come
this
far
please
point
out
my
errors
and
share
your
inputs
other
thoughts
here
are
some
mostly
more
advanced
thoughts
about
this
project
what
other
things
i
might
try
and
why
it
makes
sense
to
me
that
this
may
actually
work
liquidity
and
efficient
use
of
capital
generally
the
more
liquid
a
particular
market
is
the
more
efficient
that
is
i
think
this
is
due
to
a
chicken
and
egg
cycle
whereas
a
market
becomes
more
liquid
it
is
able
to
absorb
more
capital
moving
in
and
out
without
that
capital
hurting
itself
as
a
market
becomes
more
liquid
and
more
capital
can
be
used
in
it
you’ll
find
more
sophisticated
players
moving
in
this
is
because
it
is
expensive
to
be
sophisticated
so
you
need
to
make
returns
on
a
large
chunk
of
capital
in
order
to
justify
your
operational
costs
a
quick
corollary
is
that
in
less
liquid
markets
the
competition
isn’t
quite
as
sophisticated
and
so
the
opportunities
a
system
like
this
can
bring
may
not
have
been
traded
away
the
point
being
were
i
to
try
and
trade
this
i
would
try
and
trade
it
on
less
liquid
segments
of
the
market
that
is
maybe
the
tase
100
instead
of
the
sp
500
this
stuff
is
new
the
knowledge
of
these
algorithms
the
frameworks
to
execute
them
and
the
computing
power
to
train
them
are
all
new
at
least
in
the
sense
that
they
are
available
to
the
average
joe
such
as
myself
i’d
assume
that
top
players
have
figured
this
stuff
out
years
ago
and
have
had
the
capacity
to
execute
for
as
long
but
as
i
mention
in
the
above
paragraph
they
are
likely
executing
in
liquid
markets
that
can
support
their
size
the
next
tier
of
market
participants
i
assume
have
a
slower
velocity
of
technological
assimilation
and
in
that
sense
there
is
or
soon
will
be
a
race
to
execute
on
this
in
as
yet
untapped
markets
multiple
time
frames
while
i
mentioned
a
single
stream
of
inputs
in
the
above
i
imagine
that
a
more
efficient
way
to
train
would
be
to
train
market
vectors
at
least
on
multiple
time
frames
and
feed
them
in
at
the
inference
stage
that
is
my
lowest
time
frame
would
be
sampled
every
30
seconds
and
i’d
expect
the
network
to
learn
dependencies
that
stretch
hours
at
most
i
don’t
know
if
they
are
relevant
or
not
but
i
think
there
are
patterns
on
multiple
time
frames
and
if
the
cost
of
computation
can
be
brought
low
enough
then
it
is
worthwhile
to
incorporate
them
into
the
model
i’m
still
wrestling
with
how
best
to
represent
these
on
the
computational
graph
and
perhaps
it
is
not
mandatory
to
start
with
marketvectors
when
using
word
vectors
in
nlp
we
usually
start
with
a
pretrained
model
and
continue
adjusting
the
embeddings
during
training
of
our
model
in
my
case
there
are
no
pretrained
market
vector
available
nor
is
tehre
a
clear
algorithm
for
training
them
my
original
consideration
was
to
use
an
auto-encoder
like
in
this
paper
but
end
to
end
training
is
cooler
a
more
serious
consideration
is
the
success
of
sequence
to
sequence
models
in
translation
and
speech
recognition
where
a
sequence
is
eventually
encoded
as
a
single
vector
and
then
decoded
into
a
different
representation
like
from
speech
to
text
or
from
english
to
french
in
that
view
the
entire
architecture
i
described
is
essentially
the
encoder
and
i
haven’t
really
laid
out
a
decoder
but
i
want
to
achieve
something
specific
with
the
first
layer
the
one
that
takes
as
input
the
4000
dimensional
vector
and
outputs
a
300
dimensional
one
i
want
it
to
find
correlations
or
relations
between
various
stocks
and
compose
features
about
them
the
alternative
is
to
run
each
input
through
an
lstm
perhaps
concatenate
all
of
the
output
vectors
and
consider
that
output
of
the
encoder
stage
i
think
this
will
be
inefficient
as
the
interactions
and
correlations
between
instruments
and
their
features
will
be
lost
and
thre
will
be
10x
more
computation
required
on
the
other
hand
such
an
architecture
could
naively
be
paralleled
across
multiple
gpus
and
hosts
which
is
an
advantage
cnns
recently
there
has
been
a
spur
of
papers
on
character
level
machine
translation
this
paper
caught
my
eye
as
they
manage
to
capture
long
range
dependencies
with
a
convolutional
layer
rather
than
an
rnn
i
haven’t
given
it
more
than
a
brief
read
but
i
think
that
a
modification
where
i’d
treat
each
stock
as
a
channel
and
convolve
over
channels
first
like
in
rgb
images
would
be
another
way
to
capture
the
market
dynamics
in
the
same
way
that
they
essentially
encode
semantic
meaning
from
characters
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
founder
of
https:lighttagio
platform
to
annotate
text
for
nlp
google
developer
expert
in
ml
i
do
deep
learning
on
text
for
a
living
and
for
fun
""
when
we
offered
cs231n
deep
learning
class
at
stanford
we
intentionally
designed
the
programming
assignments
to
include
explicit
calculations
involved
in
backpropagation
on
the
lowest
level
the
students
had
to
implement
the
forward
and
the
backward
pass
of
each
layer
in
raw
numpy
inevitably
some
students
complained
on
the
class
message
boards:
this
is
seemingly
a
perfectly
sensible
appeal
-
if
you’re
never
going
to
write
backward
passes
once
the
class
is
over
why
practice
writing
them?
are
we
just
torturing
the
students
for
our
own
amusement?
some
easy
answers
could
make
arguments
along
the
lines
of
it’s
worth
knowing
what’s
under
the
hood
as
an
intellectual
curiosity
or
perhaps
you
might
want
to
improve
on
the
core
algorithm
later
but
there
is
a
much
stronger
and
practical
argument
which
i
wanted
to
devote
a
whole
post
to:
>
the
problem
with
backpropagation
is
that
it
is
a
leaky
abstraction
in
other
words
it
is
easy
to
fall
into
the
trap
of
abstracting
away
the
learning
process
—
believing
that
you
can
simply
stack
arbitrary
layers
together
and
backprop
will
magically
make
them
work
on
your
data
so
lets
look
at
a
few
explicit
examples
where
this
is
not
the
case
in
quite
unintuitive
ways
we’re
starting
off
easy
here
at
one
point
it
was
fashionable
to
use
sigmoid
or
tanh
non-linearities
in
the
fully
connected
layers
the
tricky
part
people
might
not
realize
until
they
think
about
the
backward
pass
is
that
if
you
are
sloppy
with
the
weight
initialization
or
data
preprocessing
these
non-linearities
can
saturate
and
entirely
stop
learning
—
your
training
loss
will
be
flat
and
refuse
to
go
down
for
example
a
fully
connected
layer
with
sigmoid
non-linearity
computes
using
raw
numpy:
if
your
weight
matrix
w
is
initialized
too
large
the
output
of
the
matrix
multiply
could
have
a
very
large
range
eg
numbers
between
-400
and
400
which
will
make
all
outputs
in
the
vector
z
almost
binary:
either
1
or
0
but
if
that
is
the
case
z*1-z
which
is
local
gradient
of
the
sigmoid
non-linearity
will
in
both
cases
become
zero
vanish
making
the
gradient
for
both
x
and
w
be
zero
the
rest
of
the
backward
pass
will
come
out
all
zero
from
this
point
on
due
to
multiplication
in
the
chain
rule
another
non-obvious
fun
fact
about
sigmoid
is
that
its
local
gradient
z*1-z
achieves
a
maximum
at
025
when
z
=
05
that
means
that
every
time
the
gradient
signal
flows
through
a
sigmoid
gate
its
magnitude
always
diminishes
by
one
quarter
or
more
if
you’re
using
basic
sgd
this
would
make
the
lower
layers
of
a
network
train
much
slower
than
the
higher
ones
tldr:
if
you’re
using
sigmoids
or
tanh
non-linearities
in
your
network
and
you
understand
backpropagation
you
should
always
be
nervous
about
making
sure
that
the
initialization
doesn’t
cause
them
to
be
fully
saturated
see
a
longer
explanation
in
this
cs231n
lecture
video
another
fun
non-linearity
is
the
relu
which
thresholds
neurons
at
zero
from
below
the
forward
and
backward
pass
for
a
fully
connected
layer
that
uses
relu
would
at
the
core
include:
if
you
stare
at
this
for
a
while
you’ll
see
that
if
a
neuron
gets
clamped
to
zero
in
the
forward
pass
ie
z=0
it
doesn’t
fire
then
its
weights
will
get
zero
gradient
this
can
lead
to
what
is
called
the
dead
relu
problem
where
if
a
relu
neuron
is
unfortunately
initialized
such
that
it
never
fires
or
if
a
neuron’s
weights
ever
get
knocked
off
with
a
large
update
during
training
into
this
regime
then
this
neuron
will
remain
permanently
dead
it’s
like
permanent
irrecoverable
brain
damage
sometimes
you
can
forward
the
entire
training
set
through
a
trained
network
and
find
that
a
large
fraction
eg
40%
of
your
neurons
were
zero
the
entire
time
tldr:
if
you
understand
backpropagation
and
your
network
has
relus
you’re
always
nervous
about
dead
relus
these
are
neurons
that
never
turn
on
for
any
example
in
your
entire
training
set
and
will
remain
permanently
dead
neurons
can
also
die
during
training
usually
as
a
symptom
of
aggressive
learning
rates
see
a
longer
explanation
in
cs231n
lecture
video
vanilla
rnns
feature
another
good
example
of
unintuitive
effects
of
backpropagation
i’ll
copy
paste
a
slide
from
cs231n
that
has
a
simplified
rnn
that
does
not
take
any
input
x
and
only
computes
the
recurrence
on
the
hidden
state
equivalently
the
input
x
could
always
be
zero:
this
rnn
is
unrolled
for
t
time
steps
when
you
stare
at
what
the
backward
pass
is
doing
you’ll
see
that
the
gradient
signal
going
backwards
in
time
through
all
the
hidden
states
is
always
being
multiplied
by
the
same
matrix
the
recurrence
matrix
whh
interspersed
with
non-linearity
backprop
what
happens
when
you
take
one
number
a
and
start
multiplying
it
by
some
other
number
b
ie
a*b*b*b*b*b*b?
this
sequence
either
goes
to
zero
if
|b|
<
1
or
explodes
to
infinity
when
|b|>1
the
same
thing
happens
in
the
backward
pass
of
an
rnn
except
b
is
a
matrix
and
not
just
a
number
so
we
have
to
reason
about
its
largest
eigenvalue
instead
tldr:
if
you
understand
backpropagation
and
you’re
using
rnns
you
are
nervous
about
having
to
do
gradient
clipping
or
you
prefer
to
use
an
lstm
see
a
longer
explanation
in
this
cs231n
lecture
video
lets
look
at
one
more
—
the
one
that
actually
inspired
this
post
yesterday
i
was
browsing
for
a
deep
q
learning
implementation
in
tensorflow
to
see
how
others
deal
with
computing
the
numpy
equivalent
of
q[:
a]
where
a
is
an
integer
vector
—
turns
out
this
trivial
operation
is
not
supported
in
tf
anyway
i
searched
dqn
tensorflow
clicked
the
first
link
and
found
the
core
code
here
is
an
excerpt:
if
you’re
familiar
with
dqn
you
can
see
that
there
is
the
target_q_t
which
is
just
[reward
*
\gamma
\argmax_a
qs’a]
and
then
there
is
q_acted
which
is
qsa
of
the
action
that
was
taken
the
authors
here
subtract
the
two
into
variable
delta
which
they
then
want
to
minimize
on
line
295
with
the
l2
loss
with
tfreduce_meantfsquare
so
far
so
good
the
problem
is
on
line
291
the
authors
are
trying
to
be
robust
to
outliers
so
if
the
delta
is
too
large
they
clip
it
with
tfclip_by_value
this
is
well-intentioned
and
looks
sensible
from
the
perspective
of
the
forward
pass
but
it
introduces
a
major
bug
if
you
think
about
the
backward
pass
the
clip_by_value
function
has
a
local
gradient
of
zero
outside
of
the
range
min_delta
to
max_delta
so
whenever
the
delta
is
above
minmax_delta
the
gradient
becomes
exactly
zero
during
backprop
the
authors
are
clipping
the
raw
q
delta
when
they
are
likely
trying
to
clip
the
gradient
for
added
robustness
in
that
case
the
correct
thing
to
do
is
to
use
the
huber
loss
in
place
of
tfsquare:
it’s
a
bit
gross
in
tensorflow
because
all
we
want
to
do
is
clip
the
gradient
if
it
is
above
a
threshold
but
since
we
can’t
meddle
with
the
gradients
directly
we
have
to
do
it
in
this
round-about
way
of
defining
the
huber
loss
in
torch
this
would
be
much
more
simple
i
submitted
an
issue
on
the
dqn
repo
and
this
was
promptly
fixed
backpropagation
is
a
leaky
abstraction
it
is
a
credit
assignment
scheme
with
non-trivial
consequences
if
you
try
to
ignore
how
it
works
under
the
hood
because
tensorflow
automagically
makes
my
networks
learn
you
will
not
be
ready
to
wrestle
with
the
dangers
it
presents
and
you
will
be
much
less
effective
at
building
and
debugging
neural
networks
the
good
news
is
that
backpropagation
is
not
that
difficult
to
understand
if
presented
properly
i
have
relatively
strong
feelings
on
this
topic
because
it
seems
to
me
that
95%
of
backpropagation
materials
out
there
present
it
all
wrong
filling
pages
with
mechanical
math
instead
i
would
recommend
the
cs231n
lecture
on
backprop
which
emphasizes
intuition
yay
for
shameless
self-advertising
and
if
you
can
spare
the
time
as
a
bonus
work
through
the
cs231n
assignments
which
get
you
to
write
backprop
manually
and
help
you
solidify
your
understanding
that’s
it
for
now!
i
hope
you’ll
be
much
more
suspicious
of
backpropagation
going
forward
and
think
carefully
through
what
the
backward
pass
is
doing
also
i’m
aware
that
this
post
has
unintentionally!
turned
into
several
cs231n
ads
apologies
for
that
:
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
director
of
ai
at
tesla
previously
research
scientist
at
openai
and
phd
student
at
stanford
i
like
to
train
deep
neural
nets
on
large
datasets
""
this
is
a
follow
up
to
an
article
i
wrote
last
year
machine
learning
in
a
week
on
how
i
kickstarted
my
way
into
machine
learning
ml
by
devoting
five
days
to
the
subject
after
this
highly
effective
introduction
i
continued
learning
on
my
spare
time
and
almost
exactly
one
year
later
i
did
my
first
ml
project
at
work
which
involved
using
various
ml
and
natural
language
processing
nlp
techniques
to
qualify
sales
leads
at
xeneta
this
felt
like
a
blessing:
getting
paid
to
do
something
i
normally
did
for
fun!
it
also
ripped
me
out
of
the
delusion
that
only
people
with
masters
degrees
or
phd’s
work
with
ml
professionally
in
this
post
i
want
to
share
my
journey
as
it
might
inspire
others
to
do
the
same
my
interest
in
ml
stems
back
to
2014
when
i
started
reading
articles
about
it
on
hacker
news
i
simply
found
the
idea
of
teaching
machines
stuff
by
looking
at
data
appealing
at
the
time
i
wasn’t
even
a
professional
developer
but
a
hobby
coder
who’d
done
a
couple
of
small
projects
so
i
began
watching
the
first
few
chapters
of
udacity’s
supervised
learning
course
while
also
reading
all
articles
i
came
across
on
the
subject
this
gave
me
a
little
bit
of
conceptual
understanding
though
no
practical
skills
i
also
didn’t
finish
it
as
i
rarely
do
with
mooc’s
in
january
2015
i
joined
the
founders
and
coders
fac
bootcamp
in
london
in
order
to
become
a
developer
a
few
weeks
in
i
wanted
to
learn
how
to
actually
code
machine
learning
algorithms
so
i
started
a
study
group
with
a
few
of
my
peers
every
tuesday
evening
we’d
watch
lectures
from
coursera’s
machine
learning
course
it’s
a
fantastic
course
and
i
learned
a
hell
of
a
lot
but
it’s
tough
for
a
beginner
i
had
to
watch
the
lectures
over
and
over
again
before
grasping
the
concepts
the
octave
coding
task
are
challenging
as
well
especially
if
you
don’t
know
octave
as
a
result
of
the
difficulty
one
by
one
fell
off
the
study
group
as
the
weeks
passed
eventually
i
fell
off
it
myself
as
well
in
hindsight
i
should
have
started
with
a
course
that
either
used
ml
libraries
for
the
coding
tasks
—
as
opposed
to
building
the
algorithms
from
scratch
—
or
at
least
used
a
programming
language
i
knew
if
i
could
go
back
in
time
i’d
choose
udacity’s
intro
to
machine
learning
as
it’s
easier
and
uses
python
and
scikit
learn
this
way
we
would
have
gotten
our
hands
dirty
as
soon
as
possible
gained
confidence
and
had
more
fun
one
of
the
last
things
i
did
at
fac
was
the
ml
week
stunt
my
goal
was
to
be
able
to
apply
machine
learning
to
actual
problems
at
the
end
of
the
week
which
i
managed
to
do
throughout
the
week
i
did
the
following:
it’s
by
far
the
steepest
ml
learning
curve
i’ve
ever
experienced
go
ahead
and
read
the
article
if
you
want
a
more
detailed
overview
after
i
finished
fac
in
london
and
moved
back
to
norway
i
tried
to
repeat
the
success
from
the
ml
week
but
for
neural
networks
instead
this
failed
there
were
simply
too
many
distractions
to
spend
10
hours
of
coding
and
learning
every
day
i
had
underestimated
how
important
it
was
to
be
surrounded
by
peers
at
fac
however
i
got
started
with
neural
nets
at
least
and
slowly
started
to
grasp
the
concept
by
july
i
managed
to
code
my
first
net
it’s
probably
the
crappiest
implementation
ever
created
and
i
actually
find
it
embarrassing
to
show
off
but
it
did
the
trick
i
proved
to
myself
that
i
understood
concepts
like
backpropagation
and
gradient
descent
in
the
second
half
of
the
year
my
progression
slowed
down
as
i
started
a
new
job
the
most
important
takeaway
from
this
period
was
the
leap
from
non-vectorized
to
vectorized
implementations
of
neural
networks
which
involved
repeating
linear
algebra
from
university
by
the
end
of
the
year
i
wrote
an
article
as
a
summary
of
how
i
learned
this:
during
the
christmas
vacation
of
2015
i
got
a
motivational
boost
again
and
decided
try
out
kaggle
so
i
spent
quite
some
time
experimenting
with
various
algorithms
for
their
homesite
quote
conversion
otto
group
product
classification
and
bike
sharing
demand
contests
the
main
takeaway
from
this
was
the
experience
of
iteratively
improving
the
results
by
experimenting
with
the
algorithms
and
the
data
i
learned
to
trust
my
logic
when
doing
machine
learning
if
tweaking
a
parameter
or
engineering
a
new
feature
seems
like
a
good
idea
logically
it’s
quite
likely
that
it
actually
will
help
back
at
work
in
january
2016
i
wanted
to
continue
in
the
flow
i’d
gotten
into
during
christmas
so
i
asked
my
manager
if
i
could
spend
some
time
learning
stuff
during
my
work
hours
as
well
which
he
happily
approved
having
gotten
a
basic
understanding
of
neural
networks
at
this
point
i
wanted
to
move
on
to
deep
learning
my
first
attempt
was
udacity’s
deep
learning
course
which
ended
up
as
a
big
disappointment
the
contents
of
the
video
lectures
are
good
but
they
are
too
short
and
shallow
to
me
and
the
ipython
notebook
assignments
ended
up
being
too
frustrating
as
i
spent
most
of
my
time
debugging
code
errors
which
is
the
most
effective
way
to
kill
motivation
so
after
doing
that
for
a
couple
of
sessions
at
work
i
simply
gave
up
to
their
defense
i’m
a
total
noob
when
it
comes
to
ipython
notebooks
so
it
might
not
be
as
bad
for
you
as
it
was
for
me
so
it
might
be
that
i
simply
wasn’t
ready
for
the
course
luckily
i
then
discovered
stanford’s
cs224d
and
decided
to
give
it
a
shot
it
is
a
fantastic
course
and
though
it’s
difficult
i
never
end
up
debugging
when
doing
the
problem
sets
secondly
they
actually
give
you
the
solution
code
as
well
which
i
often
look
at
when
i’m
stuck
so
that
i
can
work
my
way
backwards
to
understand
the
steps
needed
to
reach
a
solution
though
i’ve
haven’t
finished
it
yet
it
has
significantly
boosted
my
knowledge
in
nlp
and
neural
networks
so
far
however
it’s
been
tough
really
tough
at
one
point
i
realized
i
needed
help
from
someone
better
than
me
so
i
came
in
touch
with
a
phd
student
who
was
willing
to
help
me
out
for
40
usd
per
hour
both
with
the
problem
sets
as
well
as
the
overall
understanding
this
has
been
critical
for
me
in
order
to
move
on
as
he
has
uncovered
a
lot
of
black
holes
in
my
knowledge
in
addition
to
this
xeneta
also
hired
a
data
scientist
recently
he’s
got
a
masters
degree
in
math
so
i
often
ask
him
for
help
when
i’m
stuck
with
various
linear
algebra
an
calculus
tasks
or
ml
in
general
so
be
sure
to
check
out
which
resources
you
have
internally
in
your
company
as
well
after
doing
all
this
i
finally
felt
ready
to
do
a
ml
project
at
work
it
basically
involved
training
an
algorithm
to
qualify
sales
leads
by
reading
company
descriptions
and
has
actually
proven
to
be
a
big
time
saver
for
the
sales
guys
using
the
tool
check
out
out
article
i
wrote
about
it
below
or
head
over
to
github
to
dive
straight
into
the
code
getting
to
this
point
has
surely
been
a
long
journey
but
also
a
fast
one
when
i
started
my
machine
learning
in
a
week
project
i
certainly
didn’t
have
any
hopes
of
actually
using
it
professionally
within
a
year
but
it’s
100
percent
possible
and
if
i
can
do
it
so
can
anybody
else
thanks
for
reading!
my
name
is
per
i’m
a
co-founder
of
scrimba
—
a
better
way
to
teach
and
learn
code
if
you’ve
read
this
far
i’d
recommend
you
to
check
out
this
demo!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
co-founder
of
scrimba
the
next-generation
platform
for
teaching
and
learning
code
https:scrimbacom
a
publication
about
improving
your
technical
skills
""
in
the
five
days
from
july
24th
to
28th
2017
i
interviewed
at
linkedin
salesforce
einstein
google
airbnb
and
facebook
and
got
all
five
job
offers
it
was
a
great
experience
and
i
feel
fortunate
that
my
efforts
paid
off
so
i
decided
to
write
something
about
it
i
will
discuss
how
i
prepared
review
the
interview
process
and
share
my
impressions
about
the
five
companies
i
had
been
at
groupon
for
almost
three
years
it’s
my
first
job
and
i
have
been
working
with
an
amazing
team
and
on
awesome
projects
we’ve
been
building
cool
stuff
making
impact
within
the
company
publishing
papers
and
all
that
but
i
felt
my
learning
rate
was
being
annealed
read:
slowing
down
yet
my
mind
was
craving
more
also
as
a
software
engineer
in
chicago
there
are
so
many
great
companies
that
all
attract
me
in
the
bay
area
life
is
short
and
professional
life
shorter
still
after
talking
with
my
wife
and
gaining
her
full
support
i
decided
to
take
actions
and
make
my
first
ever
career
change
although
i’m
interested
in
machine
learning
positions
the
positions
at
the
five
companies
are
slightly
different
in
the
title
and
the
interviewing
process
three
are
machine
learning
engineer
linkedin
google
facebook
one
is
data
engineer
salesforce
and
one
is
software
engineer
in
general
airbnb
therefore
i
needed
to
prepare
for
three
different
areas:
coding
machine
learning
and
system
design
since
i
also
have
a
full
time
job
it
took
me
2–3
months
in
total
to
prepare
here
is
how
i
prepared
for
the
three
areas
while
i
agree
that
coding
interviews
might
not
be
the
best
way
to
assess
all
your
skills
as
a
developer
there
is
arguably
no
better
way
to
tell
if
you
are
a
good
engineer
in
a
short
period
of
time
imo
it
is
the
necessary
evil
to
get
you
that
job
i
mainly
used
leetcode
and
geeksforgeeks
for
practicing
but
hackerrank
and
lintcode
are
also
good
places
i
spent
several
weeks
going
over
common
data
structures
and
algorithms
then
focused
on
areas
i
wasn’t
too
familiar
with
and
finally
did
some
frequently
seen
problems
due
to
my
time
constraints
i
usually
did
two
problems
per
day
here
are
some
thoughts:
this
area
is
more
closely
related
to
the
actual
working
experience
many
questions
can
be
asked
during
system
design
interviews
including
but
not
limited
to
system
architecture
object
oriented
designdatabase
schema
designdistributed
system
designscalability
etc
there
are
many
resources
online
that
can
help
you
with
the
preparation
for
the
most
part
i
read
articles
on
system
design
interviews
architectures
of
large-scale
systems
and
case
studies
here
are
some
resources
that
i
found
really
helpful:
although
system
design
interviews
can
cover
a
lot
of
topics
there
are
some
general
guidelines
for
how
to
approach
the
problem:
with
all
that
said
the
best
way
to
practice
for
system
design
interviews
is
to
actually
sit
down
and
design
a
system
ie
your
day-to-day
work
instead
of
doing
the
minimal
work
go
deeper
into
the
tools
frameworks
and
libraries
you
use
for
example
if
you
use
hbase
rather
than
simply
using
the
client
to
run
some
ddl
and
do
some
fetches
try
to
understand
its
overall
architecture
such
as
the
readwrite
flow
how
hbase
ensures
strong
consistency
what
minormajor
compactions
do
and
where
lru
cache
and
bloom
filter
are
used
in
the
system
you
can
even
compare
hbase
with
cassandra
and
see
the
similarities
and
differences
in
their
design
then
when
you
are
asked
to
design
a
distributed
key-value
store
you
won’t
feel
ambushed
many
blogs
are
also
a
great
source
of
knowledge
such
as
hacker
noon
and
engineering
blogs
of
some
companies
as
well
as
the
official
documentation
of
open
source
projects
the
most
important
thing
is
to
keep
your
curiosity
and
modesty
be
a
sponge
that
absorbs
everything
it
is
submerged
into
machine
learning
interviews
can
be
divided
into
two
aspects
theory
and
product
design
unless
you
are
have
experience
in
machine
learning
research
or
did
really
well
in
your
ml
course
it
helps
to
read
some
textbooks
classical
ones
such
as
the
elements
of
statistical
learning
and
pattern
recognition
and
machine
learning
are
great
choices
and
if
you
are
interested
in
specific
areas
you
can
read
more
on
those
make
sure
you
understand
basic
concepts
such
as
bias-variance
trade-off
overfitting
gradient
descent
l1l2
regularizationbayes
theorembaggingboostingcollaborative
filteringdimension
reduction
etc
familiarize
yourself
with
common
formulas
such
as
bayes
theorem
and
the
derivation
of
popular
models
such
as
logistic
regression
and
svm
try
to
implement
simple
models
such
as
decision
trees
and
k-means
clustering
if
you
put
some
models
on
your
resume
make
sure
you
understand
it
thoroughly
and
can
comment
on
its
pros
and
cons
for
ml
product
design
understand
the
general
process
of
building
a
ml
product
here’s
what
i
tried
to
do:
here
i
want
to
emphasize
again
on
the
importance
of
remaining
curious
and
learning
continuously
try
not
to
merely
using
the
api
for
spark
mllib
or
xgboost
and
calling
it
done
but
try
to
understand
why
stochastic
gradient
descent
is
appropriate
for
distributed
training
or
understand
how
xgboost
differs
from
traditional
gbdt
eg
what
is
special
about
its
loss
function
why
it
needs
to
compute
the
second
order
derivative
etc
i
started
by
replying
to
hr’s
messages
on
linkedin
and
asking
for
referrals
after
a
failed
attempt
at
a
rock
star
startup
which
i
will
touch
upon
later
i
prepared
hard
for
several
months
and
with
help
from
my
recruiters
i
scheduled
a
full
week
of
onsites
in
the
bay
area
i
flew
in
on
sunday
had
five
full
days
of
interviews
with
around
30
interviewers
at
some
best
tech
companies
in
the
world
and
very
luckily
got
job
offers
from
all
five
of
them
all
phone
screenings
are
standard
the
only
difference
is
in
the
duration:
for
some
companies
like
linkedin
it’s
one
hour
while
for
facebook
and
airbnb
it’s
45
minutes
proficiency
is
the
key
here
since
you
are
under
the
time
gun
and
usually
you
only
get
one
chance
you
would
have
to
very
quickly
recognize
the
type
of
problem
and
give
a
high-level
solution
be
sure
to
talk
to
the
interviewer
about
your
thinking
and
intentions
it
might
slow
you
down
a
little
at
the
beginning
but
communication
is
more
important
than
anything
and
it
only
helps
with
the
interview
do
not
recite
the
solution
as
the
interviewer
would
almost
certainly
see
through
it
for
machine
learning
positions
some
companies
would
ask
ml
questions
if
you
are
interviewing
for
those
make
sure
you
brush
up
your
ml
skills
as
well
to
make
better
use
of
my
time
i
scheduled
three
phone
screenings
in
the
same
afternoon
one
hour
apart
from
each
the
upside
is
that
you
might
benefit
from
the
hot
hand
and
the
downside
is
that
the
later
ones
might
be
affected
if
the
first
one
does
not
go
well
so
i
don’t
recommend
it
for
everyone
one
good
thing
about
interviewing
with
multiple
companies
at
the
same
time
is
that
it
gives
you
certain
advantages
i
was
able
to
skip
the
second
round
phone
screening
with
airbnb
and
salesforce
because
i
got
the
onsite
at
linkedin
and
facebook
after
only
one
phone
screening
more
surprisingly
google
even
let
me
skip
their
phone
screening
entirely
and
schedule
my
onsite
to
fill
the
vacancy
after
learning
i
had
four
onsites
coming
in
the
next
week
i
knew
it
was
going
to
make
it
extremely
tiring
but
hey
nobody
can
refuse
a
google
onsite
invitation!
linkedin
this
is
my
first
onsite
and
i
interviewed
at
the
sunnyvale
location
the
office
is
very
neat
and
people
look
very
professional
as
always
the
sessions
are
one
hour
each
coding
questions
are
standard
but
the
ml
questions
can
get
a
bit
tough
that
said
i
got
an
email
from
my
hr
containing
the
preparation
material
which
was
very
helpful
and
in
the
end
i
did
not
see
anything
that
was
too
surprising
i
heard
the
rumor
that
linkedin
has
the
best
meals
in
the
silicon
valley
and
from
what
i
saw
if
it’s
not
true
it’s
not
too
far
from
the
truth
acquisition
by
microsoft
seems
to
have
lifted
the
financial
burden
from
linkedin
and
freed
them
up
to
do
really
cool
things
new
features
such
as
videos
and
professional
advertisements
are
exciting
as
a
company
focusing
on
professional
development
linkedin
prioritizes
the
growth
of
its
own
employees
a
lot
of
teams
such
as
ads
relevance
and
feed
ranking
are
expanding
so
act
quickly
if
you
want
to
join
salesforce
einstein
rock
star
project
by
rock
star
team
the
team
is
pretty
new
and
feels
very
much
like
a
startup
the
product
is
built
on
the
scala
stack
so
type
safety
is
a
real
thing
there!
great
talks
on
the
optimus
prime
library
by
matthew
tovbin
at
scala
days
chicago
2017
and
leah
mcguire
at
spark
summit
west
2017
i
interviewed
at
their
palo
alto
office
the
team
has
a
cohesive
culture
and
work
life
balance
is
great
there
everybody
is
passionate
about
what
they
are
doing
and
really
enjoys
it
with
four
sessions
it
is
shorter
compared
to
the
other
onsite
interviews
but
i
wish
i
could
have
stayed
longer
after
the
interview
matthew
even
took
me
for
a
walk
to
the
hp
garage
:
google
absolutely
the
industry
leader
and
nothing
to
say
about
it
that
people
don’t
already
know
but
it’s
huge
like
really
really
huge
it
took
me
20
minutes
to
ride
a
bicycle
to
meet
my
friends
there
also
lines
for
food
can
be
too
long
forever
a
great
place
for
developers
i
interviewed
at
one
of
the
many
buildings
on
the
mountain
view
campus
and
i
don’t
know
which
one
it
is
because
it’s
huge
my
interviewers
all
look
very
smart
and
once
they
start
talking
they
are
even
smarter
it
would
be
very
enjoyable
to
work
with
these
people
one
thing
that
i
felt
special
about
google’s
interviews
is
that
the
analysis
of
algorithm
complexity
is
really
important
make
sure
you
really
understand
what
big
o
notation
means!
airbnb
fast
expanding
unicorn
with
a
unique
culture
and
arguably
the
most
beautiful
office
in
the
silicon
valley
new
products
such
as
experiences
and
restaurant
reservation
high
end
niche
market
and
expansion
into
china
all
contribute
to
a
positive
prospect
perfect
choice
if
you
are
risk
tolerant
and
want
a
fast
growing
pre-ipo
experience
airbnb’s
coding
interview
is
a
bit
unique
because
you’ll
be
coding
in
an
ide
instead
of
whiteboarding
so
your
code
needs
to
compile
and
give
the
right
answer
some
problems
can
get
really
hard
and
they’ve
got
the
one-of-a-kind
cross
functional
interviews
this
is
how
airbnb
takes
culture
seriously
and
being
technically
excellent
doesn’t
guarantee
a
job
offer
for
me
the
two
cross
functionals
were
really
enjoyable
i
had
casual
conversations
with
the
interviewers
and
we
all
felt
happy
at
the
end
of
the
session
overall
i
think
airbnb’s
onsite
is
the
hardest
due
to
the
difficulty
of
the
problems
longer
duration
and
unique
cross-functional
interviews
if
you
are
interested
be
sure
to
understand
their
culture
and
core
values
facebook
another
giant
that
is
still
growing
fast
and
smaller
and
faster-paced
compared
to
google
with
its
product
lines
dominating
the
social
network
market
and
big
investments
in
ai
and
vr
i
can
only
see
more
growth
potential
for
facebook
in
the
future
with
stars
like
yann
lecun
and
yangqing
jia
it’s
the
perfect
place
if
you
are
interested
in
machine
learning
i
interviewed
at
building
20
the
one
with
the
rooftop
garden
and
ocean
view
and
also
where
zuckerberg’s
office
is
located
i’m
not
sure
if
the
interviewers
got
instructions
but
i
didn’t
get
clear
signs
whether
my
solutions
were
correct
although
i
believed
they
were
by
noon
the
prior
four
days
started
to
take
its
toll
and
i
was
having
a
headache
i
persisted
through
the
afternoon
sessions
but
felt
i
didn’t
do
well
at
all
i
was
a
bit
surprised
to
learn
that
i
was
getting
an
offer
from
them
as
well
generally
i
felt
people
there
believe
the
company’s
vision
and
are
proud
of
what
they
are
building
being
a
company
with
half
a
trillion
market
cap
and
growing
facebook
is
a
perfect
place
to
grow
your
career
at
this
is
a
big
topic
that
i
won’t
cover
in
this
post
but
i
found
this
article
to
be
very
helpful
some
things
that
i
do
think
are
important:
all
successes
start
with
failures
including
interviews
before
i
started
interviewing
for
these
companies
i
failed
my
interview
at
databricks
in
may
back
in
april
xiangrui
contacted
me
via
linkedin
asking
me
if
i
was
interested
in
a
position
on
the
spark
mllib
team
i
was
extremely
thrilled
because
1
i
use
spark
and
love
scala
2
databricks
engineers
are
top-notch
and
3
spark
is
revolutionizing
the
whole
big
data
world
it
is
an
opportunity
i
couldn’t
miss
so
i
started
interviewing
after
a
few
days
the
bar
is
very
high
and
the
process
is
quite
long
including
one
pre-screening
questionnaire
one
phone
screening
one
coding
assignment
and
one
full
onsite
i
managed
to
get
the
onsite
invitation
and
visited
their
office
in
downtown
san
francisco
where
treasure
island
can
be
seen
my
interviewer
were
incredibly
intelligent
yet
equally
modest
during
the
interviews
i
often
felt
being
pushed
to
the
limits
it
was
fine
until
one
disastrous
session
where
i
totally
messed
up
due
to
insufficient
skills
and
preparation
and
it
ended
up
a
fiasco
xiangrui
was
very
kind
and
walked
me
to
where
i
wanted
to
go
after
the
interview
was
over
and
i
really
enjoyed
talking
to
him
i
got
the
rejection
several
days
later
it
was
expected
but
i
felt
frustrated
for
a
few
days
nonetheless
although
i
missed
the
opportunity
to
work
there
i
wholeheartedly
wish
they
will
continue
to
make
greater
impact
and
achievements
from
the
first
interview
in
may
to
finally
accepting
the
job
offer
in
late
september
my
first
career
change
was
long
and
not
easy
it
was
difficult
for
me
to
prepare
because
i
needed
to
keep
doing
well
at
my
current
job
for
several
weeks
i
was
on
a
regular
schedule
of
preparing
for
the
interview
till
1am
getting
up
at
8:30am
the
next
day
and
fully
devoting
myself
to
another
day
at
work
interviewing
at
five
companies
in
five
days
was
also
highly
stressful
and
risky
and
i
don’t
recommend
doing
it
unless
you
have
a
very
tight
schedule
but
it
does
give
you
a
good
advantage
during
negotiation
should
you
secure
multiple
offers
i’d
like
to
thank
all
my
recruiters
who
patiently
walked
me
through
the
process
the
people
who
spend
their
precious
time
talking
to
me
and
all
the
companies
that
gave
me
the
opportunities
to
interview
and
extended
me
offers
lastly
but
most
importantly
i
want
to
thank
my
family
for
their
love
and
support
—
my
parents
for
watching
me
taking
the
first
and
every
step
my
dear
wife
for
everything
she
has
done
for
me
and
my
daughter
for
her
warming
smile
thanks
for
reading
through
this
long
post
you
can
find
me
on
linkedin
or
twitter
xiaohan
zeng
102217
ps:
since
the
publication
of
this
post
it
has
unexpectedly
received
some
attention
i
would
like
to
thank
everybody
for
the
congratulations
and
shares
and
apologize
for
not
being
able
to
respond
to
each
of
them
this
post
has
been
translated
into
some
other
languages:
it
has
been
reposted
in
tech
in
asia
breaking
into
startups
invited
me
to
a
live
video
streaming
together
with
sophia
ciocca
covershr
did
a
short
qna
with
me
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
critical
mind
""
romantic
heart
""
disclaimer:
i’m
not
an
expert
in
neural
networks
or
machine
learning
since
originally
writing
this
article
many
people
with
far
more
expertise
in
these
fields
than
myself
have
indicated
that
while
impressive
what
google
have
achieved
is
evolutionary
not
revolutionary
in
the
very
least
it’s
fair
to
say
that
i’m
guilty
of
anthropomorphising
in
parts
of
the
text
i’ve
left
the
article’s
content
unchanged
because
i
think
it’s
interesting
to
compare
the
gut
reaction
i
had
with
the
subsequent
comments
of
experts
in
the
field
i
strongly
encourage
readers
to
browse
the
comments
after
reading
the
article
for
some
perspectives
more
sober
and
informed
than
my
own
in
the
closing
weeks
of
2016
google
published
an
article
that
quietly
sailed
under
most
people’s
radars
which
is
a
shame
because
it
may
just
be
the
most
astonishing
article
about
machine
learning
that
i
read
last
year
don’t
feel
bad
if
you
missed
it
not
only
was
the
article
competing
with
the
pre-christmas
rush
that
most
of
us
were
navigating
—
it
was
also
tucked
away
on
google’s
research
blog
beneath
the
geektastic
headline
zero-shot
translation
with
google’s
multilingual
neural
machine
translation
system
this
doesn’t
exactly
scream
must
read
does
it?
especially
when
you’ve
got
projects
to
wind
up
gifts
to
buy
and
family
feuds
to
be
resolved
—
all
while
the
advent
calendar
relentlessly
counts
down
the
days
until
christmas
like
some
kind
of
chocolate-filled
yuletide
doomsday
clock
luckily
i’m
here
to
bring
you
up
to
speed
here’s
the
deal
up
until
september
of
last
year
google
translate
used
phrase-based
translation
it
basically
did
the
same
thing
you
and
i
do
when
we
look
up
key
words
and
phrases
in
our
lonely
planet
language
guides
it’s
effective
enough
and
blisteringly
fast
compared
to
awkwardly
thumbing
your
way
through
a
bunch
of
pages
looking
for
the
french
equivalent
of
please
bring
me
all
of
your
cheese
and
don’t
stop
until
i
fall
over
but
it
lacks
nuance
phrase-based
translation
is
a
blunt
instrument
it
does
the
job
well
enough
to
get
by
but
mapping
roughly
equivalent
words
and
phrases
without
an
understanding
of
linguistic
structures
can
only
produce
crude
results
this
approach
is
also
limited
by
the
extent
of
an
available
vocabulary
phrase-based
translation
has
no
capacity
to
make
educated
guesses
at
words
it
doesn’t
recognize
and
can’t
learn
from
new
input
all
that
changed
in
september
when
google
gave
their
translation
tool
a
new
engine:
the
google
neural
machine
translation
system
gnmt
this
new
engine
comes
fully
loaded
with
all
the
hot
2016
buzzwords
like
neural
network
and
machine
learning
the
short
version
is
that
google
translate
got
smart
it
developed
the
ability
to
learn
from
the
people
who
used
it
it
learned
how
to
make
educated
guesses
about
the
content
tone
and
meaning
of
phrases
based
on
the
context
of
other
words
and
phrases
around
them
and
—
here’s
the
bit
that
should
make
your
brain
explode
—
it
got
creative
google
translate
invented
its
own
language
to
help
it
translate
more
effectively
what’s
more
nobody
told
it
to
it
didn’t
develop
a
language
or
interlingua
as
google
call
it
because
it
was
coded
to
it
developed
a
new
language
because
the
software
determined
over
time
that
this
was
the
most
efficient
way
to
solve
the
problem
of
translation
stop
and
think
about
that
for
a
moment
let
it
sink
in
a
neural
computing
system
designed
to
translate
content
from
one
human
language
into
another
developed
its
own
internal
language
to
make
the
task
more
efficient
without
being
told
to
do
so
in
a
matter
of
weeks
i’ve
added
a
correctionretraction
of
this
paragraph
in
the
notes
to
understand
what’s
going
on
we
need
to
understand
what
zero-shot
translation
capability
is
here’s
google’s
mike
schuster
nikhil
thorat
and
melvin
johnson
from
the
original
blog
post:
here
you
can
see
an
advantage
of
google’s
new
neural
machine
over
the
old
phrase-based
approach
the
gmnt
is
able
to
learn
how
to
translate
between
two
languages
without
being
explicitly
taught
this
wouldn’t
be
possible
in
a
phrase-based
model
where
translation
is
dependent
upon
an
explicit
dictionary
to
map
words
and
phrases
between
each
pair
of
languages
being
translated
and
this
leads
the
google
engineers
onto
that
truly
astonishing
discovery
of
creation:
so
there
you
have
it
in
the
last
weeks
of
2016
as
journos
around
the
world
started
penning
their
was
this
the
worst
year
in
living
memory
thinkpieces
google
engineers
were
quietly
documenting
a
genuinely
astonishing
breakthrough
in
software
engineering
and
linguistics
i
just
thought
maybe
you’d
want
to
know
ok
to
really
understand
what’s
going
on
we
probably
need
multiple
computer
science
and
linguistics
degrees
i’m
just
barely
scraping
the
surface
here
if
you’ve
got
time
to
get
a
few
degrees
or
if
you’ve
already
got
them
please
drop
me
a
line
and
explain
it
all
me
to
slowly
update
1:
in
my
excitement
it’s
fair
to
say
that
i’ve
exaggerated
the
idea
of
this
as
an
‘intelligent’
system
—
at
least
so
far
as
we
would
think
about
human
intelligence
and
decision
making
make
sure
you
read
chris
mcdonald’s
comment
after
the
article
for
a
more
sober
perspective
update
2:
nafrondel’s
excellent
detailed
reply
is
also
a
must
read
for
an
expert
explanation
of
how
neural
networks
function
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
a
tinkerer
our
community
publishes
stories
worth
reading
on
development
design
and
data
science
""
a
year
and
a
half
ago
i
dropped
out
of
one
of
the
best
computer
science
programs
in
canada
i
started
creating
my
own
data
science
master’s
program
using
online
resources
i
realized
that
i
could
learn
everything
i
needed
through
edx
coursera
and
udacity
instead
and
i
could
learn
it
faster
more
efficiently
and
for
a
fraction
of
the
cost
i’m
almost
finished
now
i’ve
taken
many
data
science-related
courses
and
audited
portions
of
many
more
i
know
the
options
out
there
and
what
skills
are
needed
for
learners
preparing
for
a
data
analyst
or
data
scientist
role
so
i
started
creating
a
review-driven
guide
that
recommends
the
best
courses
for
each
subject
within
data
science
for
the
first
guide
in
the
series
i
recommended
a
few
coding
classes
for
the
beginner
data
scientist
then
it
was
statistics
and
probability
classes
then
introductions
to
data
science
also
data
visualization
for
this
guide
i
spent
a
dozen
hours
trying
to
identify
every
online
machine
learning
course
offered
as
of
may
2017
extracting
key
bits
of
information
from
their
syllabi
and
reviews
and
compiling
their
ratings
my
end
goal
was
to
identify
the
three
best
courses
available
and
present
them
to
you
below
for
this
task
i
turned
to
none
other
than
the
open
source
class
central
community
and
its
database
of
thousands
of
course
ratings
and
reviews
since
2011
class
central
founder
dhawal
shah
has
kept
a
closer
eye
on
online
courses
than
arguably
anyone
else
in
the
world
dhawal
personally
helped
me
assemble
this
list
of
resources
each
course
must
fit
three
criteria:
we
believe
we
covered
every
notable
course
that
fits
the
above
criteria
since
there
are
seemingly
hundreds
of
courses
on
udemy
we
chose
to
consider
the
most-reviewed
and
highest-rated
ones
only
there’s
always
a
chance
that
we
missed
something
though
so
please
let
us
know
in
the
comments
section
if
we
left
a
good
course
out
we
compiled
average
ratings
and
number
of
reviews
from
class
central
and
other
review
sites
to
calculate
a
weighted
average
rating
for
each
course
we
read
text
reviews
and
used
this
feedback
to
supplement
the
numerical
ratings
we
made
subjective
syllabus
judgment
calls
based
on
three
factors:
a
popular
definition
originates
from
arthur
samuel
in
1959:
machine
learning
is
a
subfield
of
computer
science
that
gives
computers
the
ability
to
learn
without
being
explicitly
programmed
in
practice
this
means
developing
computer
programs
that
can
make
predictions
based
on
data
just
as
humans
can
learn
from
experience
so
can
computers
where
data
=
experience
a
machine
learning
workflow
is
the
process
required
for
carrying
out
a
machine
learning
project
though
individual
projects
can
differ
most
workflows
share
several
common
tasks:
problem
evaluation
data
exploration
data
preprocessing
model
trainingtestingdeployment
etc
below
you’ll
find
helpful
visualization
of
these
core
steps:
the
ideal
course
introduces
the
entire
process
and
provides
interactive
examples
assignments
andor
quizzes
where
students
can
perform
each
task
themselves
first
off
let’s
define
deep
learning
here
is
a
succinct
description:
as
would
be
expected
portions
of
some
of
the
machine
learning
courses
contain
deep
learning
content
i
chose
not
to
include
deep
learning-only
courses
however
if
you
are
interested
in
deep
learning
specifically
we’ve
got
you
covered
with
the
following
article:
my
top
three
recommendations
from
that
list
would
be:
several
courses
listed
below
ask
students
to
have
prior
programming
calculus
linear
algebra
and
statistics
experience
these
prerequisites
are
understandable
given
that
machine
learning
is
an
advanced
discipline
missing
a
few
subjects?
good
news!
some
of
this
experience
can
be
acquired
through
our
recommendations
in
the
first
two
articles
programming
statistics
of
this
data
science
career
guide
several
top-ranked
courses
below
also
provide
gentle
calculus
and
linear
algebra
refreshers
and
highlight
the
aspects
most
relevant
to
machine
learning
for
those
less
familiar
stanford
university’s
machine
learning
on
coursera
is
the
clear
current
winner
in
terms
of
ratings
reviews
and
syllabus
fit
taught
by
the
famous
andrew
ng
google
brain
founder
and
former
chief
scientist
at
baidu
this
was
the
class
that
sparked
the
founding
of
coursera
it
has
a
47-star
weighted
average
rating
over
422
reviews
released
in
2011
it
covers
all
aspects
of
the
machine
learning
workflow
though
it
has
a
smaller
scope
than
the
original
stanford
class
upon
which
it
is
based
it
still
manages
to
cover
a
large
number
of
techniques
and
algorithms
the
estimated
timeline
is
eleven
weeks
with
two
weeks
dedicated
to
neural
networks
and
deep
learning
free
and
paid
options
are
available
ng
is
a
dynamic
yet
gentle
instructor
with
a
palpable
experience
he
inspires
confidence
especially
when
sharing
practical
implementation
tips
and
warnings
about
common
pitfalls
a
linear
algebra
refresher
is
provided
and
ng
highlights
the
aspects
of
calculus
most
relevant
to
machine
learning
evaluation
is
automatic
and
is
done
via
multiple
choice
quizzes
that
follow
each
lesson
and
programming
assignments
the
assignments
there
are
eight
of
them
can
be
completed
in
matlab
or
octave
which
is
an
open-source
version
of
matlab
ng
explains
his
language
choice:
though
python
and
r
are
likely
more
compelling
choices
in
2017
with
the
increased
popularity
of
those
languages
reviewers
note
that
that
shouldn’t
stop
you
from
taking
the
course
a
few
prominent
reviewers
noted
the
following:
columbia
university’s
machine
learning
is
a
relatively
new
offering
that
is
part
of
their
artificial
intelligence
micromasters
on
edx
though
it
is
newer
and
doesn’t
have
a
large
number
of
reviews
the
ones
that
it
does
have
are
exceptionally
strong
professor
john
paisley
is
noted
as
brilliant
clear
and
clever
it
has
a
48-star
weighted
average
rating
over
10
reviews
the
course
also
covers
all
aspects
of
the
machine
learning
workflow
and
more
algorithms
than
the
above
stanford
offering
columbia’s
is
a
more
advanced
introduction
with
reviewers
noting
that
students
should
be
comfortable
with
the
recommended
prerequisites
calculus
linear
algebra
statistics
probability
and
coding
quizzes
11
programming
assignments
4
and
a
final
exam
are
the
modes
of
evaluation
students
can
use
either
python
octave
or
matlab
to
complete
the
assignments
the
course’s
total
estimated
timeline
is
eight
to
ten
hours
per
week
over
twelve
weeks
it
is
free
with
a
verified
certificate
available
for
purchase
below
are
a
few
of
the
aforementioned
sparkling
reviews:
machine
learning
a-ztm
on
udemy
is
an
impressively
detailed
offering
that
provides
instruction
in
both
python
and
r
which
is
rare
and
can’t
be
said
for
any
of
the
other
top
courses
it
has
a
45-star
weighted
average
rating
over
8119
reviews
which
makes
it
the
most
reviewed
course
of
the
ones
considered
it
covers
the
entire
machine
learning
workflow
and
an
almost
ridiculous
in
a
good
way
number
of
algorithms
through
405
hours
of
on-demand
video
the
course
takes
a
more
applied
approach
and
is
lighter
math-wise
than
the
above
two
courses
each
section
starts
with
an
intuition
video
from
eremenko
that
summarizes
the
underlying
theory
of
the
concept
being
taught
de
ponteves
then
walks
through
implementation
with
separate
videos
for
both
python
and
r
as
a
bonus
the
course
includes
python
and
r
code
templates
for
students
to
download
and
use
on
their
own
projects
there
are
quizzes
and
homework
challenges
though
these
aren’t
the
strong
points
of
the
course
eremenko
and
the
superdatascience
team
are
revered
for
their
ability
to
make
the
complex
simple
also
the
prerequisites
listed
are
just
some
high
school
mathematics
so
this
course
might
be
a
better
option
for
those
daunted
by
the
stanford
and
columbia
offerings
a
few
prominent
reviewers
noted
the
following:
our
#1
pick
had
a
weighted
average
rating
of
47
out
of
5
stars
over
422
reviews
let’s
look
at
the
other
alternatives
sorted
by
descending
rating
a
reminder
that
deep
learning-only
courses
are
not
included
in
this
guide
—
you
can
find
those
here
the
analytics
edge
massachusetts
institute
of
technologyedx:
more
focused
on
analytics
in
general
though
it
does
cover
several
machine
learning
topics
uses
r
strong
narrative
that
leverages
familiar
real-world
examples
challenging
ten
to
fifteen
hours
per
week
over
twelve
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
49-star
weighted
average
rating
over
214
reviews
python
for
data
science
and
machine
learning
bootcamp
jose
portillaudemy:
has
large
chunks
of
machine
learning
content
but
covers
the
whole
data
science
process
more
of
a
very
detailed
intro
to
python
amazing
course
though
not
ideal
for
the
scope
of
this
guide
215
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
46-star
weighted
average
rating
over
3316
reviews
data
science
and
machine
learning
bootcamp
with
r
jose
portillaudemy:
the
comments
for
portilla’s
above
course
apply
here
as
well
except
for
r
175
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
46-star
weighted
average
rating
over
1317
reviews
machine
learning
series
lazy
programmer
incudemy:
taught
by
a
data
scientistbig
data
engineerfull
stack
software
engineer
with
an
impressive
resume
lazy
programmer
currently
has
a
series
of
16
machine
learning-focused
courses
on
udemy
in
total
the
courses
have
5000
ratings
and
almost
all
of
them
have
46
stars
a
useful
course
ordering
is
provided
in
each
individual
course’s
description
uses
python
cost
varies
depending
on
udemy
discounts
which
are
frequent
machine
learning
georgia
techudacity:
a
compilation
of
what
was
three
separate
courses:
supervised
unsupervised
and
reinforcement
learning
part
of
udacity’s
machine
learning
engineer
nanodegree
and
georgia
tech’s
online
master’s
degree
oms
bite-sized
videos
as
is
udacity’s
style
friendly
professors
estimated
timeline
of
four
months
free
it
has
a
456-star
weighted
average
rating
over
9
reviews
implementing
predictive
analytics
with
spark
in
azure
hdinsight
microsoftedx:
introduces
the
core
concepts
of
machine
learning
and
a
variety
of
algorithms
leverages
several
big
data-friendly
tools
including
apache
spark
scala
and
hadoop
uses
both
python
and
r
four
hours
per
week
over
six
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
45-star
weighted
average
rating
over
6
reviews
data
science
and
machine
learning
with
python
—
hands
on!
frank
kaneudemy:
uses
python
kane
has
nine
years
of
experience
at
amazon
and
imdb
nine
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
45-star
weighted
average
rating
over
4139
reviews
scala
and
spark
for
big
data
and
machine
learning
jose
portillaudemy:
big
data
focus
specifically
on
implementation
in
scala
and
spark
ten
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
45-star
weighted
average
rating
over
607
reviews
machine
learning
engineer
nanodegree
udacity:
udacity’s
flagship
machine
learning
program
which
features
a
best-in-class
project
review
system
and
career
support
the
program
is
a
compilation
of
several
individual
udacity
courses
which
are
free
co-created
by
kaggle
estimated
timeline
of
six
months
currently
costs
$199
usd
per
month
with
a
50%
tuition
refund
available
for
those
who
graduate
within
12
months
it
has
a
45-star
weighted
average
rating
over
2
reviews
learning
from
data
introductory
machine
learning
california
institute
of
technologyedx:
enrollment
is
currently
closed
on
edx
but
is
also
available
via
caltech’s
independent
platform
see
below
it
has
a
449-star
weighted
average
rating
over
42
reviews
learning
from
data
introductory
machine
learning
yaser
abu-mostafacalifornia
institute
of
technology:
a
real
caltech
course
not
a
watered-down
version
reviews
note
it
is
excellent
for
understanding
machine
learning
theory
the
professor
yaser
abu-mostafa
is
popular
among
students
and
also
wrote
the
textbook
upon
which
this
course
is
based
videos
are
taped
lectures
with
lectures
slides
picture-in-picture
uploaded
to
youtube
homework
assignments
are
pdf
files
the
course
experience
for
online
students
isn’t
as
polished
as
the
top
three
recommendations
it
has
a
443-star
weighted
average
rating
over
7
reviews
mining
massive
datasets
stanford
university:
machine
learning
with
a
focus
on
big
data
introduces
modern
distributed
file
systems
and
mapreduce
ten
hours
per
week
over
seven
weeks
free
it
has
a
44-star
weighted
average
rating
over
30
reviews
aws
machine
learning:
a
complete
guide
with
python
chandra
lingamudemy:
a
unique
focus
on
cloud-based
machine
learning
and
specifically
amazon
web
services
uses
python
nine
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
44-star
weighted
average
rating
over
62
reviews
introduction
to
machine
learning
""
face
detection
in
python
holczer
balazsudemy:
uses
python
eight
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
44-star
weighted
average
rating
over
162
reviews
statlearning:
statistical
learning
stanford
university:
based
on
the
excellent
textbook
an
introduction
to
statistical
learning
with
applications
in
r
and
taught
by
the
professors
who
wrote
it
reviewers
note
that
the
mooc
isn’t
as
good
as
the
book
citing
thin
exercises
and
mediocre
videos
five
hours
per
week
over
nine
weeks
free
it
has
a
435-star
weighted
average
rating
over
84
reviews
machine
learning
specialization
university
of
washingtoncoursera:
great
courses
but
last
two
classes
including
the
capstone
project
were
canceled
reviewers
note
that
this
series
is
more
digestable
read:
easier
for
those
without
strong
technical
backgrounds
than
other
top
machine
learning
courses
eg
stanford’s
or
caltech’s
be
aware
that
the
series
is
incomplete
with
recommender
systems
deep
learning
and
a
summary
missing
free
and
paid
options
available
it
has
a
431-star
weighted
average
rating
over
80
reviews
from
0
to
1:
machine
learning
nlp
""
python-cut
to
the
chase
loony
cornudemy:
a
down-to-earth
shy
but
confident
take
on
machine
learning
techniques
taught
by
four-person
team
with
decades
of
industry
experience
together
uses
python
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
42-star
weighted
average
rating
over
494
reviews
principles
of
machine
learning
microsoftedx:
uses
r
python
and
microsoft
azure
machine
learning
part
of
the
microsoft
professional
program
certificate
in
data
science
three
to
four
hours
per
week
over
six
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
409-star
weighted
average
rating
over
11
reviews
big
data:
statistical
inference
and
machine
learning
queensland
university
of
technologyfuturelearn:
a
nice
brief
exploratory
machine
learning
course
with
a
focus
on
big
data
covers
a
few
tools
like
r
h2o
flow
and
weka
only
three
weeks
in
duration
at
a
recommended
two
hours
per
week
but
one
reviewer
noted
that
six
hours
per
week
would
be
more
appropriate
free
and
paid
options
available
it
has
a
4-star
weighted
average
rating
over
4
reviews
genomic
data
science
and
clustering
bioinformatics
v
university
of
california
san
diegocoursera:
for
those
interested
in
the
intersection
of
computer
science
and
biology
and
how
it
represents
an
important
frontier
in
modern
science
focuses
on
clustering
and
dimensionality
reduction
part
of
ucsd’s
bioinformatics
specialization
free
and
paid
options
available
it
has
a
4-star
weighted
average
rating
over
3
reviews
intro
to
machine
learning
udacity:
prioritizes
topic
breadth
and
practical
tools
in
python
over
depth
and
theory
the
instructors
sebastian
thrun
and
katie
malone
make
this
class
so
fun
consists
of
bite-sized
videos
and
quizzes
followed
by
a
mini-project
for
each
lesson
currently
part
of
udacity’s
data
analyst
nanodegree
estimated
timeline
of
ten
weeks
free
it
has
a
395-star
weighted
average
rating
over
19
reviews
machine
learning
for
data
analysis
wesleyan
universitycoursera:
a
brief
intro
machine
learning
and
a
few
select
algorithms
covers
decision
trees
random
forests
lasso
regression
and
k-means
clustering
part
of
wesleyan’s
data
analysis
and
interpretation
specialization
estimated
timeline
of
four
weeks
free
and
paid
options
available
it
has
a
36-star
weighted
average
rating
over
5
reviews
programming
with
python
for
data
science
microsoftedx:
produced
by
microsoft
in
partnership
with
coding
dojo
uses
python
eight
hours
per
week
over
six
weeks
free
and
paid
options
available
it
has
a
346-star
weighted
average
rating
over
37
reviews
machine
learning
for
trading
georgia
techudacity:
focuses
on
applying
probabilistic
machine
learning
approaches
to
trading
decisions
uses
python
part
of
udacity’s
machine
learning
engineer
nanodegree
and
georgia
tech’s
online
master’s
degree
oms
estimated
timeline
of
four
months
free
it
has
a
329-star
weighted
average
rating
over
14
reviews
practical
machine
learning
johns
hopkins
universitycoursera:
a
brief
practical
introduction
to
a
number
of
machine
learning
algorithms
several
onetwo-star
reviews
expressing
a
variety
of
concerns
part
of
jhu’s
data
science
specialization
four
to
nine
hours
per
week
over
four
weeks
free
and
paid
options
available
it
has
a
311-star
weighted
average
rating
over
37
reviews
machine
learning
for
data
science
and
analytics
columbia
universityedx:
introduces
a
wide
range
of
machine
learning
topics
some
passionate
negative
reviews
with
concerns
including
content
choices
a
lack
of
programming
assignments
and
uninspiring
presentation
seven
to
ten
hours
per
week
over
five
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
274-star
weighted
average
rating
over
36
reviews
recommender
systems
specialization
university
of
minnesotacoursera:
strong
focus
one
specific
type
of
machine
learning
—
recommender
systems
a
four
course
specialization
plus
a
capstone
project
which
is
a
case
study
taught
using
lenskit
an
open-source
toolkit
for
recommender
systems
free
and
paid
options
available
it
has
a
2-star
weighted
average
rating
over
2
reviews
machine
learning
with
big
data
university
of
california
san
diegocoursera:
terrible
reviews
that
highlight
poor
instruction
and
evaluation
some
noted
it
took
them
mere
hours
to
complete
the
whole
course
part
of
ucsd’s
big
data
specialization
free
and
paid
options
available
it
has
a
186-star
weighted
average
rating
over
14
reviews
practical
predictive
analytics:
models
and
methods
university
of
washingtoncoursera:
a
brief
intro
to
core
machine
learning
concepts
one
reviewer
noted
that
there
was
a
lack
of
quizzes
and
that
the
assignments
were
not
challenging
part
of
uw’s
data
science
at
scale
specialization
six
to
eight
hours
per
week
over
four
weeks
free
and
paid
options
available
it
has
a
175-star
weighted
average
rating
over
4
reviews
the
following
courses
had
one
or
no
reviews
as
of
may
2017
machine
learning
for
musicians
and
artists
goldsmiths
university
of
londonkadenze:
unique
students
learn
algorithms
software
tools
and
machine
learning
best
practices
to
make
sense
of
human
gesture
musical
audio
and
other
real-time
data
seven
sessions
in
length
audit
free
and
premium
$10
usd
per
month
options
available
it
has
one
5-star
review
applied
machine
learning
in
python
university
of
michigancoursera:
taught
using
python
and
the
scikit
learn
toolkit
part
of
the
applied
data
science
with
python
specialization
scheduled
to
start
may
29th
free
and
paid
options
available
applied
machine
learning
microsoftedx:
taught
using
various
tools
including
python
r
and
microsoft
azure
machine
learning
note:
microsoft
produces
the
course
includes
hands-on
labs
to
reinforce
the
lecture
content
three
to
four
hours
per
week
over
six
weeks
free
with
a
verified
certificate
available
for
purchase
machine
learning
with
python
big
data
university:
taught
using
python
targeted
towards
beginners
estimated
completion
time
of
four
hours
big
data
university
is
affiliated
with
ibm
free
machine
learning
with
apache
systemml
big
data
university:
taught
using
apache
systemml
which
is
a
declarative
style
language
designed
for
large-scale
machine
learning
estimated
completion
time
of
eight
hours
big
data
university
is
affiliated
with
ibm
free
machine
learning
for
data
science
university
of
california
san
diegoedx:
doesn’t
launch
until
january
2018
programming
examples
and
assignments
are
in
python
using
jupyter
notebooks
eight
hours
per
week
over
ten
weeks
free
with
a
verified
certificate
available
for
purchase
introduction
to
analytics
modeling
georgia
techedx:
the
course
advertises
r
as
its
primary
programming
tool
five
to
ten
hours
per
week
over
ten
weeks
free
with
a
verified
certificate
available
for
purchase
predictive
analytics:
gaining
insights
from
big
data
queensland
university
of
technologyfuturelearn:
brief
overview
of
a
few
algorithms
uses
hewlett
packard
enterprise’s
vertica
analytics
platform
as
an
applied
tool
start
date
to
be
announced
two
hours
per
week
over
four
weeks
free
with
a
certificate
of
achievement
available
for
purchase
introducción
al
machine
learning
universitas
telefónicamiríada
x:
taught
in
spanish
an
introduction
to
machine
learning
that
covers
supervised
and
unsupervised
learning
a
total
of
twenty
estimated
hours
over
four
weeks
machine
learning
path
step
dataquest:
taught
in
python
using
dataquest’s
interactive
in-browser
platform
multiple
guided
projects
and
a
plus
project
where
you
build
your
own
machine
learning
system
using
your
own
data
subscription
required
the
following
six
courses
are
offered
by
datacamp
datacamp’s
hybrid
teaching
style
leverages
video
and
text-based
instruction
with
lots
of
examples
through
an
in-browser
code
editor
a
subscription
is
required
for
full
access
to
each
course
introduction
to
machine
learning
datacamp:
covers
classification
regression
and
clustering
algorithms
uses
r
fifteen
videos
and
81
exercises
with
an
estimated
timeline
of
six
hours
supervised
learning
with
scikit-learn
datacamp:
uses
python
and
scikit-learn
covers
classification
and
regression
algorithms
seventeen
videos
and
54
exercises
with
an
estimated
timeline
of
four
hours
unsupervised
learning
in
r
datacamp:
provides
a
basic
introduction
to
clustering
and
dimensionality
reduction
in
r
sixteen
videos
and
49
exercises
with
an
estimated
timeline
of
four
hours
machine
learning
toolbox
datacamp:
teaches
the
big
ideas
in
machine
learning
uses
r
24
videos
and
88
exercises
with
an
estimated
timeline
of
four
hours
machine
learning
with
the
experts:
school
budgets
datacamp:
a
case
study
from
a
machine
learning
competition
on
drivendata
involves
building
a
model
to
automatically
classify
items
in
a
school’s
budget
datacamp’s
supervised
learning
with
scikit-learn
is
a
prerequisite
fifteen
videos
and
51
exercises
with
an
estimated
timeline
of
four
hours
unsupervised
learning
in
python
datacamp:
covers
a
variety
of
unsupervised
learning
algorithms
using
python
scikit-learn
and
scipy
the
course
ends
with
students
building
a
recommender
system
to
recommend
popular
musical
artists
thirteen
videos
and
52
exercises
with
an
estimated
timeline
of
four
hours
machine
learning
tom
mitchellcarnegie
mellon
university:
carnegie
mellon’s
graduate
introductory
machine
learning
course
a
prerequisite
to
their
second
graduate
level
course
statistical
machine
learning
taped
university
lectures
with
practice
problems
homework
assignments
and
a
midterm
all
with
solutions
posted
online
a
2011
version
of
the
course
also
exists
cmu
is
one
of
the
best
graduate
schools
for
studying
machine
learning
and
has
a
whole
department
dedicated
to
ml
free
statistical
machine
learning
larry
wassermancarnegie
mellon
university:
likely
the
most
advanced
course
in
this
guide
a
follow-up
to
carnegie
mellon’s
machine
learning
course
taped
university
lectures
with
practice
problems
homework
assignments
and
a
midterm
all
with
solutions
posted
online
free
undergraduate
machine
learning
nando
de
freitasuniversity
of
british
columbia:
an
undergraduate
machine
learning
course
lectures
are
filmed
and
put
on
youtube
with
the
slides
posted
on
the
course
website
the
course
assignments
are
posted
as
well
no
solutions
though
de
freitas
is
now
a
full-time
professor
at
the
university
of
oxford
and
receives
praise
for
his
teaching
abilities
in
various
forums
graduate
version
available
see
below
machine
learning
nando
de
freitasuniversity
of
british
columbia:
a
graduate
machine
learning
course
the
comments
in
de
freitas’
undergraduate
course
above
apply
here
as
well
this
is
the
fifth
of
a
six-piece
series
that
covers
the
best
online
courses
for
launching
yourself
into
the
data
science
field
we
covered
programming
in
the
first
article
statistics
and
probability
in
the
second
article
intros
to
data
science
in
the
third
article
and
data
visualization
in
the
fourth
the
final
piece
will
be
a
summary
of
those
articles
plus
the
best
online
courses
for
other
key
topics
such
as
data
wrangling
databases
and
even
software
engineering
if
you’re
looking
for
a
complete
list
of
data
science
online
courses
you
can
find
them
on
class
central’s
data
science
and
big
data
subject
page
if
you
enjoyed
reading
this
check
out
some
of
class
central’s
other
pieces:
if
you
have
suggestions
for
courses
i
missed
let
me
know
in
the
responses!
if
you
found
this
helpful
click
the
💚
so
more
people
will
see
it
here
on
medium
this
is
a
condensed
version
of
my
original
article
published
on
class
central
where
i’ve
included
detailed
course
syllabi
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
curriculum
lead
projects
@
datacamp
i
created
my
own
data
science
master’s
program
our
community
publishes
stories
worth
reading
on
development
design
and
data
science
""
part
1:
why
machine
learning
matters
the
big
picture
of
artificial
intelligence
and
machine
learning
—
past
present
and
future
part
21:
supervised
learning
learning
with
an
answer
key
introducing
linear
regression
loss
functions
overfitting
and
gradient
descent
part
22:
supervised
learning
ii
two
methods
of
classification:
logistic
regression
and
svms
part
23:
supervised
learning
iii
non-parametric
learners:
k-nearest
neighbors
decision
trees
random
forests
introducing
cross-validation
hyperparameter
tuning
and
ensemble
models
part
3:
unsupervised
learning
clustering:
k-means
hierarchical
dimensionality
reduction:
principal
components
analysis
pca
singular
value
decomposition
svd
part
4:
neural
networks
""
deep
learning
why
where
and
how
deep
learning
works
drawing
inspiration
from
the
brain
convolutional
neural
networks
cnns
recurrent
neural
networks
rnns
real-world
applications
part
5:
reinforcement
learning
exploration
and
exploitation
markov
decision
processes
q-learning
policy
learning
and
deep
reinforcement
learning
the
value
learning
problem
appendix:
the
best
machine
learning
resources
a
curated
list
of
resources
for
creating
your
machine
learning
curriculum
this
guide
is
intended
to
be
accessible
to
anyone
basic
concepts
in
probability
statistics
programming
linear
algebra
and
calculus
will
be
discussed
but
it
isn’t
necessary
to
have
prior
knowledge
of
them
to
gain
value
from
this
series
artificial
intelligence
will
shape
our
future
more
powerfully
than
any
other
innovation
this
century
anyone
who
does
not
understand
it
will
soon
find
themselves
feeling
left
behind
waking
up
in
a
world
full
of
technology
that
feels
more
and
more
like
magic
the
rate
of
acceleration
is
already
astounding
after
a
couple
of
ai
winters
and
periods
of
false
hope
over
the
past
four
decades
rapid
advances
in
data
storage
and
computer
processing
power
have
dramatically
changed
the
game
in
recent
years
in
2015
google
trained
a
conversational
agent
ai
that
could
not
only
convincingly
interact
with
humans
as
a
tech
support
helpdesk
but
also
discuss
morality
express
opinions
and
answer
general
facts-based
questions
the
same
year
deepmind
developed
an
agent
that
surpassed
human-level
performance
at
49
atari
games
receiving
only
the
pixels
and
game
score
as
inputs
soon
after
in
2016
deepmind
obsoleted
their
own
achievement
by
releasing
a
new
state-of-the-art
gameplay
method
called
a3c
meanwhile
alphago
defeated
one
of
the
best
human
players
at
go
—
an
extraordinary
achievement
in
a
game
dominated
by
humans
for
two
decades
after
machines
first
conquered
chess
many
masters
could
not
fathom
how
it
would
be
possible
for
a
machine
to
grasp
the
full
nuance
and
complexity
of
this
ancient
chinese
war
strategy
game
with
its
10170
possible
board
positions
there
are
only
1080atoms
in
the
universe
in
march
2017
openai
created
agents
that
invented
their
own
language
to
cooperate
and
more
effectively
achieve
their
goal
soon
after
facebook
reportedly
successfully
training
agents
to
negotiate
and
even
lie
just
a
few
days
ago
as
of
this
writing
on
august
11
2017
openai
reached
yet
another
incredible
milestone
by
defeating
the
world’s
top
professionals
in
1v1
matches
of
the
online
multiplayer
game
dota
2
much
of
our
day-to-day
technology
is
powered
by
artificial
intelligence
point
your
camera
at
the
menu
during
your
next
trip
to
taiwan
and
the
restaurant’s
selections
will
magically
appear
in
english
via
the
google
translate
app
today
ai
is
used
to
design
evidence-based
treatment
plans
for
cancer
patients
instantly
analyze
results
from
medical
tests
to
escalate
to
the
appropriate
specialist
immediately
and
conduct
scientific
research
for
drug
discovery
in
everyday
life
it’s
increasingly
commonplace
to
discover
machines
in
roles
traditionally
occupied
by
humans
really
don’t
be
surprised
if
a
little
housekeeping
delivery
bot
shows
up
instead
of
a
human
next
time
you
call
the
hotel
desk
to
send
up
some
toothpaste
in
this
series
we’ll
explore
the
core
machine
learning
concepts
behind
these
technologies
by
the
end
you
should
be
able
to
describe
how
they
work
at
a
conceptual
level
and
be
equipped
with
the
tools
to
start
building
similar
applications
yourself
artificial
intelligence
is
the
study
of
agents
that
perceive
the
world
around
them
form
plans
and
make
decisions
to
achieve
their
goals
its
foundations
include
mathematics
logic
philosophy
probability
linguistics
neuroscience
and
decision
theory
many
fields
fall
under
the
umbrella
of
ai
such
as
computer
vision
robotics
machine
learning
and
natural
language
processing
machine
learning
is
a
subfield
of
artificial
intelligence
its
goal
is
to
enable
computers
to
learn
on
their
own
a
machine’s
learning
algorithm
enables
it
to
identify
patterns
in
observed
data
build
models
that
explain
the
world
and
predict
things
without
having
explicit
pre-programmed
rules
and
models
the
technologies
discussed
above
are
examples
of
artificial
narrow
intelligence
ani
which
can
effectively
perform
a
narrowly
defined
task
meanwhile
we’re
continuing
to
make
foundational
advances
towards
human-level
artificial
general
intelligence
agi
also
known
as
strong
ai
the
definition
of
an
agi
is
an
artificial
intelligence
that
can
successfully
perform
any
intellectual
task
that
a
human
being
can
including
learning
planning
and
decision-making
under
uncertainty
communicating
in
natural
language
making
jokes
manipulating
people
trading
stocks
or
reprogramming
itself
and
this
last
one
is
a
big
deal
once
we
create
an
ai
that
can
improve
itself
it
will
unlock
a
cycle
of
recursive
self-improvement
that
could
lead
to
an
intelligence
explosion
over
some
unknown
time
period
ranging
from
many
decades
to
a
single
day
you
may
have
heard
this
point
referred
to
as
the
singularity
the
term
is
borrowed
from
the
gravitational
singularity
that
occurs
at
the
center
of
a
black
hole
an
infinitely
dense
one-dimensional
point
where
the
laws
of
physics
as
we
understand
them
start
to
break
down
a
recent
report
by
the
future
of
humanity
institute
surveyed
a
panel
of
ai
researchers
on
timelines
for
agi
and
found
that
researchers
believe
there
is
a
50%
chance
of
ai
outperforming
humans
in
all
tasks
in
45
years
grace
et
al
2017
we’ve
personally
spoken
with
a
number
of
sane
and
reasonable
ai
practitioners
who
predict
much
longer
timelines
the
upper
limit
being
never
and
others
whose
timelines
are
alarmingly
short
—
as
little
as
a
few
years
the
advent
of
greater-than-human-level
artificial
superintelligence
asi
could
be
one
of
the
best
or
worst
things
to
happen
to
our
species
it
carries
with
it
the
immense
challenge
of
specifying
what
ais
will
want
in
a
way
that
is
friendly
to
humans
while
it’s
impossible
to
say
what
the
future
holds
one
thing
is
certain:
2017
is
a
good
time
to
start
understanding
how
machines
think
to
go
beyond
the
abstractions
of
a
philosopher
in
an
armchair
and
intelligently
shape
our
roadmaps
and
policies
with
respect
to
ai
we
must
engage
with
the
details
of
how
machines
see
the
world
—
what
they
want
their
potential
biases
and
failure
modes
their
temperamental
quirks
—
just
as
we
study
psychology
and
neuroscience
to
understand
how
humans
learn
decide
act
and
feel
machine
learning
is
at
the
core
of
our
journey
towards
artificial
general
intelligence
and
in
the
meantime
it
will
change
every
industry
and
have
a
massive
impact
on
our
day-to-day
lives
that’s
why
we
believe
it’s
worth
understanding
machine
learning
at
least
at
a
conceptual
level
—
and
we
designed
this
series
to
be
the
best
place
to
start
you
don’t
necessarily
need
to
read
the
series
cover-to-cover
to
get
value
out
of
it
here
are
three
suggestions
on
how
to
approach
it
depending
on
your
interests
and
how
much
time
you
have:
vishal
most
recently
led
growth
at
upstart
a
lending
platform
that
utilizes
machine
learning
to
price
credit
automate
the
borrowing
process
and
acquire
users
he
spends
his
time
thinking
about
startups
applied
cognitive
science
moral
philosophy
and
the
ethics
of
artificial
intelligence
samer
is
a
master’s
student
in
computer
science
and
engineering
at
ucsd
and
co-founder
of
conigo
labs
prior
to
grad
school
he
founded
tablescribe
a
business
intelligence
tool
for
smbs
and
spent
two
years
advising
fortune
100
companies
at
mckinsey
samer
previously
studied
computer
science
and
ethics
politics
and
economics
at
yale
most
of
this
series
was
written
during
a
10-day
trip
to
the
united
kingdom
in
a
frantic
blur
of
trains
planes
cafes
pubs
and
wherever
else
we
could
find
a
dry
place
to
sit
our
aim
was
to
solidify
our
own
understanding
of
artificial
intelligence
machine
learning
and
how
the
methods
therein
fit
together
—
and
hopefully
create
something
worth
sharing
in
the
process
and
now
without
further
ado
let’s
dive
into
machine
learning
with
part
21:
supervised
learning!
more
from
machine
learning
for
humans
🤖👶
a
special
thanks
to
jonathan
eng
edoardo
conti
grant
schneider
sunny
kumar
stephanie
he
tarun
wadhwa
and
sachin
maini
series
editor
for
their
significant
contributions
and
feedback
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
research
comms
@deepmindai
previously
@upstart
@yale
@trueventurestec
demystifying
artificial
intelligence
""
machine
learning
discussions
on
safe
and
intentional
application
of
ai
for
positive
social
impact
""
the
hbo
show
silicon
valley
released
a
real
ai
app
that
identifies
hotdogs
—
and
not
hotdogs
—
like
the
one
shown
on
season
4’s
4th
episode
the
app
is
now
available
on
android
as
well
as
ios!
to
achieve
this
we
designed
a
bespoke
neural
architecture
that
runs
directly
on
your
phone
and
trained
it
with
tensorflow
keras
""
nvidia
gpus
while
the
use-case
is
farcical
the
app
is
an
approachable
example
of
both
deep
learning
and
edge
computing
all
ai
work
is
powered
100%
by
the
user’s
device
and
images
are
processed
without
ever
leaving
their
phone
this
provides
users
with
a
snappier
experience
no
round
trip
to
the
cloud
offline
availability
and
better
privacy
this
also
allows
us
to
run
the
app
at
a
cost
of
$0
even
under
the
load
of
a
million
users
providing
significant
savings
compared
to
traditional
cloud-based
ai
approaches
the
app
was
developed
in-house
by
the
show
by
a
single
developer
running
on
a
single
laptop
""
attached
gpu
using
hand-curated
data
in
that
respect
it
may
provide
a
sense
of
what
can
be
achieved
today
with
a
limited
amount
of
time
""
resources
by
non-technical
companies
individual
developers
and
hobbyists
alike
in
that
spirit
this
article
attempts
to
give
a
detailed
overview
of
steps
involved
to
help
others
build
their
own
apps
if
you
haven’t
seen
the
show
or
tried
the
app
you
should!
the
app
lets
you
snap
a
picture
and
then
tells
you
whether
it
thinks
that
image
is
of
a
hotdog
or
not
it’s
a
straightforward
use-case
that
pays
homage
to
recent
ai
research
and
applications
in
particular
imagenet
while
we’ve
probably
dedicated
more
engineering
resources
to
recognizing
hotdogs
than
anyone
else
the
app
still
fails
in
horrible
andor
subtle
ways
conversely
it’s
also
sometimes
able
to
recognize
hotdogs
in
complex
situations
according
to
engadget
it’s
incredible
i’ve
had
more
success
identifying
food
with
the
app
in
20
minutes
than
i
have
had
tagging
and
identifying
songs
with
shazam
in
the
past
two
years
have
you
ever
found
yourself
reading
hacker
news
thinking
they
raised
a
10m
series
a
for
that?
i
could
build
it
in
one
weekend!
this
app
probably
feels
a
lot
like
that
and
the
initial
prototype
was
indeed
built
in
a
single
weekend
using
google
cloud
platform’s
vision
api
and
react
native
but
the
final
app
we
ended
up
releasing
on
the
app
store
required
months
of
additional
part-time
work
to
deliver
meaningful
improvements
that
would
be
difficult
for
an
outsider
to
appreciate
we
spent
weeks
optimizing
overall
accuracy
training
time
inference
time
iterating
on
our
setup
""
tooling
so
we
could
have
a
faster
development
iterations
and
spent
a
whole
weekend
optimizing
the
user
experience
around
ios
""
android
permissions
don’t
even
get
me
started
on
that
one
all
too
often
technical
blog
posts
or
academic
papers
skip
over
this
part
preferring
to
present
the
final
chosen
solution
in
the
interest
of
helping
others
learn
from
our
mistake
""
choices
we
will
present
an
abridged
view
of
the
approaches
that
didn’t
work
for
us
before
we
describe
the
final
architecture
we
ended
up
shipping
in
the
next
section
we
chose
react
native
to
build
the
prototype
as
it
would
give
us
an
easy
sandbox
to
experiment
with
and
would
help
us
quickly
support
many
devices
the
experience
ended
up
being
a
good
one
and
we
kept
react
native
for
the
remainder
of
the
project:
it
didn’t
always
make
things
easy
and
the
design
for
the
app
was
purposefully
limited
but
in
the
end
react
native
got
the
job
done
the
other
main
component
we
used
for
the
prototype
—
google
cloud’s
vision
api
was
quickly
abandoned
there
were
3
main
factors:
for
these
reasons
we
started
experimenting
with
what’s
trendily
called
edge
computing
which
for
our
purposes
meant
that
after
training
our
neural
network
on
our
laptop
we
would
export
it
and
embed
it
directly
into
our
mobile
app
so
that
the
neural
network
execution
phase
or
inference
would
run
directly
inside
the
user’s
phone
through
a
chance
encounter
with
pete
warden
of
the
tensorflow
team
we
had
become
aware
of
its
ability
to
run
tensorflow
directly
embedded
on
an
ios
device
and
started
exploring
that
path
after
react
native
tensorflow
became
the
second
fixed
part
of
our
stack
it
only
took
a
day
of
work
to
integrate
tensorflow’s
objective-c
camera
example
in
our
react
native
shell
it
took
slightly
longer
to
use
their
transfer
learning
script
which
helps
you
retrain
the
inception
architecture
to
deal
with
a
more
specific
image
problem
inception
is
the
name
of
a
family
of
neural
architectures
built
by
google
to
deal
with
image
recognition
problems
inception
is
available
pre-trained
which
means
the
training
phase
has
been
completed
and
the
weights
are
set
most
often
for
image
recognition
networks
they
have
been
trained
on
imagenet
a
dataset
containing
over
20000
different
types
of
objects
hotdogs
are
one
of
them
however
much
like
google
cloud’s
vision
api
imagenet
training
rewards
breadth
as
much
as
depth
here
and
out-of-the-box
accuracy
on
a
single
one
of
the
20000
categories
can
be
lacking
as
such
retraining
also
called
transfer
learning
aims
to
take
a
full-trained
neural
net
and
retrain
it
to
perform
better
on
the
specific
problem
you’d
like
to
handle
this
usually
involves
some
degree
of
forgetting
either
by
excising
entire
layers
from
the
stack
or
by
slowly
erasing
the
network’s
ability
to
distinguish
a
type
of
object
eg
chairs
in
favor
of
better
accuracy
at
recognizing
the
one
you
care
about
ie
hotdogs
while
the
network
inception
in
this
case
may
have
been
trained
on
the
14m
images
contained
in
imagenet
we
were
able
to
retrain
it
on
a
just
a
few
thousand
hotdog
images
to
get
drastically
enhanced
hotdog
recognition
the
big
advantage
of
transfer
learning
are
you
will
get
better
results
much
faster
and
with
less
data
than
if
you
train
from
scratch
a
full
training
might
take
months
on
multiple
gpus
and
require
millions
of
images
while
retraining
can
conceivably
be
done
in
hours
on
a
laptop
with
a
couple
thousand
images
one
of
the
biggest
challenges
we
encountered
was
understanding
exactly
what
should
count
as
a
hotdog
and
what
should
not
defining
what
a
hotdog
is
ends
up
being
surprisingly
difficult
do
cut
up
sausages
count
and
if
so
which
kinds?
and
subject
to
cultural
interpretation
similarly
the
open
world
nature
of
our
problem
meant
we
had
to
deal
with
an
almost
infinite
number
of
inputs
while
certain
computer-vision
problems
have
relatively
limited
inputs
say
x-rays
of
bolts
with
or
without
a
mechanical
default
we
had
to
prepare
the
app
to
be
fed
selfies
nature
shots
and
any
number
of
foods
suffice
to
say
this
approach
was
promising
and
did
lead
to
some
improved
results
however
it
had
to
be
abandoned
for
a
couple
of
reasons
first
the
nature
of
our
problem
meant
a
strong
imbalance
in
training
data:
there
are
many
more
examples
of
things
that
are
not
hotdogs
than
things
that
are
hotdogs
in
practice
this
means
that
if
you
train
your
algorithm
on
3
hotdog
images
and
97
non-hotdog
images
and
it
recognizes
0%
of
the
former
but
100%
of
the
latter
it
will
still
score
97%
accuracy
by
default!
this
was
not
straightforward
to
solve
out
of
the
box
using
tensorflow’s
retrain
tool
and
basically
necessitated
setting
up
a
deep
learning
model
from
scratch
import
weights
and
train
in
a
more
controlled
manner
at
this
point
we
decided
to
bite
the
bullet
and
get
something
started
with
keras
a
deep
learning
library
that
provides
nicer
easier-to-use
abstractions
on
top
of
tensorflow
including
pretty
awesome
training
tools
and
a
class_weights
option
which
is
ideal
to
deal
with
this
sort
of
dataset
imbalance
we
were
dealing
with
we
used
that
opportunity
to
try
other
popular
neural
architectures
like
vgg
but
one
problem
remained
none
of
them
could
comfortably
fit
on
an
iphone
they
consumed
too
much
memory
which
led
to
app
crashes
and
would
sometime
takes
up
to
10
seconds
to
compute
which
was
not
ideal
from
a
ux
standpoint
many
things
were
attempted
to
mitigate
that
but
in
the
end
it
these
architectures
were
just
too
big
to
run
efficiently
on
mobile
to
give
you
a
context
out
of
time
this
was
roughly
the
mid-way
point
of
the
project
by
that
time
the
ui
was
90%
done
and
very
little
of
it
was
going
to
change
but
in
hindsight
the
neural
net
was
at
best
20%
done
we
had
a
good
sense
of
challenges
""
a
good
dataset
but
0
lines
of
the
final
neural
architecture
had
been
written
none
of
our
neural
code
could
reliably
run
on
mobile
and
even
our
accuracy
was
going
to
improve
drastically
in
the
weeks
to
come
the
problem
directly
ahead
of
us
was
simple:
if
inception
and
vgg
were
too
big
was
there
a
simpler
pre-trained
neural
network
we
could
retrain?
at
the
suggestion
of
the
always
excellent
jeremy
p
howard
where
has
that
guy
been
all
our
life?
we
explored
xception
enet
and
squeezenet
we
quickly
settled
on
squeezenet
due
to
its
explicit
positioning
as
a
solution
for
embedded
deep
learning
and
the
availability
of
a
pre-trained
keras
model
on
github
yay
open-source
so
how
big
of
a
difference
does
this
make?
an
architecture
like
vgg
uses
about
138
million
parameters
essentially
the
number
of
numbers
necessary
to
model
the
neurons
and
values
between
them
inception
is
already
a
massive
improvement
requiring
only
23
million
parameters
squeezenet
in
comparison
only
requires
125
million
this
has
two
advantages:
there
are
tradeoffs
of
course:
during
this
phase
we
started
experimenting
with
tuning
the
neural
network
architecture
in
particular
we
started
using
batch
normalization
and
trying
different
activation
functions
after
adding
batch
normalization
and
elu
to
squeezenet
we
were
able
to
train
neural
network
that
achieve
90%
accuracy
when
training
from
scratch
however
they
were
relatively
brittle
meaning
the
same
network
would
overfit
in
some
cases
or
underfit
in
others
when
confronted
to
real-life
testing
even
adding
more
examples
to
the
dataset
and
playing
with
data
augmentation
failed
to
deliver
a
network
that
met
expectations
so
while
this
phase
was
promising
and
for
the
first
time
gave
us
a
functioning
app
that
could
work
entirely
on
an
iphone
in
less
than
a
second
we
eventually
moved
to
our
4th
""
final
architecture
our
final
architecture
was
spurred
in
large
part
by
the
publication
on
april
17
of
google’s
mobilenets
paper
promising
a
new
neural
architecture
with
inception-like
accuracy
on
simple
problems
like
ours
with
only
4m
or
so
parameters
this
meant
it
sat
in
an
interesting
sweet
spot
between
a
squeezenet
that
had
maybe
been
overly
simplistic
for
our
purposes
and
the
possibly
overwrought
elephant-trying-to-squeeze-in-a-tutu
of
using
inception
or
vgg
on
mobile
the
paper
introduced
some
capacity
to
tune
the
size
""
complexity
of
network
specifically
to
trade
memorycpu
consumption
against
accuracy
which
was
very
much
top
of
mind
for
us
at
the
time
with
less
than
a
month
to
go
before
the
app
had
to
launch
we
endeavored
to
reproduce
the
paper’s
results
this
was
entirely
anticlimactic
as
within
a
day
of
the
paper
being
published
a
keras
implementation
was
already
offered
publicly
on
github
by
refik
can
malli
a
student
at
istanbul
technical
university
whose
work
we
had
already
benefitted
from
when
we
took
inspiration
from
his
excellent
keras
squeezenet
implementation
the
depth
""
openness
of
the
deep
learning
community
and
the
presence
of
talented
minds
like
rc
is
what
makes
deep
learning
viable
for
applications
today
—
but
they
also
make
working
in
this
field
more
thrilling
than
any
tech
trend
we’ve
been
involved
with
our
final
architecture
ended
up
making
significant
departures
from
the
mobilenets
architecture
or
from
convention
in
particular:
so
how
does
this
stack
work
exactly?
deep
learning
often
gets
a
bad
rap
for
being
a
black
box
and
while
it’s
true
many
components
of
it
can
be
mysterious
the
networks
we
use
often
leak
information
about
how
some
of
their
magic
work
we
can
look
at
the
layers
of
this
stack
and
how
they
activate
on
specific
input
images
giving
us
a
sense
of
each
layer’s
ability
to
recognize
sausage
buns
or
other
particularly
salient
hotdog
features
data
quality
was
of
the
utmost
importance
a
neural
network
can
only
be
as
good
as
the
data
that
trained
it
and
improving
training
set
quality
was
probably
one
of
the
top
3
things
we
spent
time
on
during
this
project
the
key
things
we
did
to
improve
this
were:
the
final
composition
of
our
dataset
was
150k
images
of
which
only
3k
were
hotdogs:
there
are
only
so
many
hotdogs
you
can
look
at
but
there
are
many
not
hotdogs
to
look
at
the
49:1
imbalance
was
dealt
with
by
saying
a
keras
class
weight
of
49:1
in
favor
of
hotdogs
of
the
remaining
147k
images
most
were
of
food
with
just
3k
photos
of
non-food
items
to
help
the
network
generalize
a
bit
more
and
not
get
tricked
into
seeing
a
hotdog
if
presented
with
an
image
of
a
human
in
a
red
outfit
our
data
augmentation
rules
were
as
follows:
these
numbers
were
derived
intuitively
based
on
experiments
and
our
understanding
of
the
real-life
usage
of
our
app
as
opposed
to
careful
experimentation
the
final
key
to
our
data
pipeline
was
using
patrick
rodriguez’s
multiprocess
image
data
generator
for
keras
while
keras
does
have
a
built-in
multi-threaded
and
multiprocess
implementation
we
found
patrick’s
library
to
be
consistently
faster
in
our
experiments
for
reasons
we
did
not
have
time
to
investigate
this
library
cut
our
training
time
to
a
third
of
what
it
used
to
be
the
network
was
trained
using
a
2015
macbook
pro
and
attached
external
gpu
egpu
specifically
an
nvidia
gtx
980
ti
we’d
probably
buy
a
1080
ti
if
we
were
starting
today
we
were
able
to
train
the
network
on
batches
of
128
images
at
a
time
the
network
was
trained
for
a
total
of
240
epochs
meaning
we
ran
all
150k
images
through
the
network
240
times
this
took
about
80
hours
we
trained
the
network
in
3
phases:
while
learning
rates
were
identified
by
running
the
linear
experiment
recommended
by
the
clr
paper
they
seem
to
intuitively
make
sense
in
that
the
max
for
each
phase
is
within
a
factor
of
2
of
the
previous
minimum
which
is
aligned
with
the
industry
standard
recommendation
of
halving
your
learning
rate
if
your
accuracy
plateaus
during
training
in
the
interest
of
time
we
performed
some
training
runs
on
a
paperspace
p5000
instance
running
ubuntu
in
those
cases
we
were
able
to
double
the
batch
size
and
found
that
optimal
learning
rates
for
each
phase
were
roughly
double
as
well
even
having
designed
a
relatively
compact
neural
architecture
and
having
trained
it
to
handle
situations
it
may
find
in
a
mobile
context
we
had
a
lot
of
work
left
to
make
it
run
properly
trying
to
run
a
top-of-the-line
neural
net
architecture
out
of
the
box
can
quickly
burns
hundreds
megabytes
of
ram
which
few
mobile
devices
can
spare
today
beyond
network
optimizations
it
turns
out
the
way
you
handle
images
or
even
load
tensorflow
itself
can
have
a
huge
impact
on
how
quickly
your
network
runs
how
little
ram
it
uses
and
how
crash-free
the
experience
will
be
for
your
users
this
was
maybe
the
most
mysterious
part
of
this
project
relatively
little
information
can
be
found
about
it
possibly
due
to
the
dearth
of
production
deep
learning
applications
running
on
mobile
devices
as
of
today
however
we
must
commend
the
tensorflow
team
and
particularly
pete
warden
andrew
harp
and
chad
whipkey
for
the
existing
documentation
and
their
kindness
in
answering
our
inquiries
instead
of
using
tensorflow
on
ios
we
looked
at
using
apple’s
built-in
deep
learning
libraries
instead
bnns
mpscnn
and
later
on
coreml
we
would
have
designed
the
network
in
keras
trained
it
with
tensorflow
exported
all
the
weight
values
re-implemented
the
network
with
bnns
or
mpscnn
or
imported
it
via
coreml
and
loaded
the
parameters
into
that
new
implementation
however
the
biggest
obstacle
was
that
these
new
apple
libraries
are
only
available
on
ios
10
and
we
wanted
to
support
older
versions
of
ios
as
ios
10
adoption
and
these
frameworks
continue
to
improve
there
may
not
be
a
case
for
using
tensorflow
on
device
in
the
near
future
if
you
think
injecting
javascript
into
your
app
on
the
fly
is
cool
try
injecting
neural
nets
into
your
app!
the
last
production
trick
we
used
was
to
leverage
codepush
and
apple’s
relatively
permissive
terms
of
service
to
live-inject
new
versions
of
our
neural
networks
after
submission
to
the
app
store
while
this
was
mostly
done
to
help
us
quickly
deliver
accuracy
improvements
to
our
users
after
release
you
could
conceivably
use
this
approach
to
drastically
expand
or
alter
the
feature
set
of
your
app
without
going
through
an
app
store
review
again
there
are
a
lot
of
things
that
didn’t
work
or
we
didn’t
have
time
to
do
and
these
are
the
ideas
we’d
investigate
in
the
future:
finally
we’d
be
remiss
not
to
mention
the
obvious
and
important
influence
of
user
experience
developer
experience
and
built-in
biases
in
developing
an
ai
app
each
probably
deserve
their
own
post
or
their
own
book
but
here
are
the
very
concrete
impacts
of
these
3
things
in
our
experience
ux
user
experience
is
arguably
more
critical
at
every
stage
of
the
development
of
an
ai
app
than
for
a
traditional
application
there
are
no
deep
learning
algorithms
that
will
give
you
perfect
results
right
now
but
there
are
many
situations
where
the
right
mix
of
deep
learning
""
ux
will
lead
to
results
that
are
indistinguishable
from
perfect
proper
ux
expectations
are
irreplaceable
when
it
comes
to
setting
developers
on
the
right
path
to
design
their
neural
networks
setting
the
proper
expectations
for
users
when
they
use
the
app
and
gracefully
handling
the
inevitable
ai
failures
building
ai
apps
without
a
ux-first
mindset
is
like
training
a
neural
net
without
stochastic
gradient
descent:
you
will
end
up
stuck
in
the
local
minima
of
the
uncanny
valley
on
your
way
to
building
the
perfect
ai
use-case
dx
developer
experience
is
extremely
important
as
well
because
deep
learning
training
time
is
the
new
horsing
around
while
waiting
for
your
program
to
compile
we
suggest
you
heavily
favor
dx
first
hence
keras
as
it’s
always
possible
to
optimize
runtime
for
later
runs
manual
gpu
parallelization
multi-process
data
augmentation
tensorflow
pipeline
even
re-implementing
for
caffe2
""
pytorch
even
projects
with
relatively
obtuse
apis
""
documentation
like
tensorflow
greatly
improve
dx
by
providing
a
highly-tested
highly-used
well-maintained
environment
for
training
""
running
neural
networks
for
the
same
reason
it’s
hard
to
beat
both
the
cost
as
well
as
the
flexibility
of
having
your
own
local
gpu
for
development
being
able
to
look
at
""
edit
images
locally
edit
code
with
your
preferred
tool
without
delays
greatly
improves
the
development
quality
""
speed
of
building
ai
projects
most
ai
apps
will
hit
more
critical
cultural
biases
than
ours
but
as
an
example
even
our
straightforward
use-case
caught
us
flat-footed
with
built-in
biases
in
our
initial
dataset
that
made
the
app
unable
to
recognize
french-style
hotdogs
asian
hotdogs
and
more
oddities
we
did
not
have
immediate
personal
experience
with
it’s
critical
to
remember
that
ai
do
not
make
better
decisions
than
humans
—
they
are
infected
by
the
same
human
biases
we
fall
prey
to
via
the
training
sets
humans
provide
thanks
to:
mike
judge
alec
berg
clay
tarver
todd
silverstein
jonathan
dotan
lisa
schomas
amy
solomon
dorothy
street
""
rich
toyon
and
all
the
writers
of
the
show
—
the
app
would
simply
not
exist
without
themmeaghan
dana
david
jay
and
everyone
at
hbo
scale
venture
partners
""
gitlab
rachel
thomas
and
jeremy
howard
""
fast
ai
for
all
that
they
have
taught
me
and
for
kindly
reviewing
a
draft
of
this
post
check
out
their
free
online
deep
learning
course
it’s
awesome!
jp
simard
for
his
help
on
ios
and
finally
the
tensorflow
team
""
rmachinelearning
for
their
help
""
inspiration
""
and
thanks
to
everyone
who
used
""
shared
the
app!
it
made
staring
at
pictures
of
hotdogs
for
months
on
end
totally
worth
it
😅
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ai
startups
""
hbo’s
silicon
valley
get
in
touch:
timanglade@gmailcom
""
member
feature
story
a
software
engineer
explains
the
science
behind
personalized
music
recommendations
photo
by
studioeastgetty
images
photo
by
studioeastgetty
images
this
monday
—
just
like
every
monday
before
it
—
over
100
million
spotify
users
found
a
fresh
new
playlist
waiting
for
them
called
discover
weekly
it’s
a
custom
mixtape
of
30
songs
they’ve
never
listened
to
before
but
will
probably
love
and
it’s
pretty
much
magic
i’m
a
huge
fan
of
spotify
and
particularly
discover
weekly
why?
it
makes
me
feel
seen
it
knows
my
musical
tastes
better
than
any
person
in
my
entire
life
ever
has
and
i’m
consistently
delighted
by
how
satisfyingly
just
right
it
is
every
week
with
tracks
i
probably
would
never
have
found
myself
or
known
i
would
like
for
those
of
you
who
live
under
a
soundproof
rock
let
me
introduce
you
to
my
virtual
best
friend:
as
it
turns
out
i’m
not
alone
in
my
obsession
with
discover
weekly
the
user
base
goes
crazy
for
it
which
has
driven
spotify
to
rethink
its
focus
and
invest
more
resources
into
algorithm-based
playlists
ever
since
discover
weekly
debuted
in
2015
i’ve
been
dying
to
know
how
it
works
what’s
more
i’m
a
spotify
fangirl
so
i
sometimes
like
to
pretend
that
i
work
there
and
research
their
products
after
three
weeks
of
mad
googling
i
feel
like
i’ve
finally
gotten
a
glimpse
behind
the
curtain
so
how
does
spotify
do
such
an
amazing
job
of
choosing
those
30
songs
for
each
person
each
week?
let’s
zoom
out
for
a
second
to
look
at
how
other
music
services
have
tackled
music
recommendations
and
how
spotify’s
doing
it
better
back
in
the
2000s
songza
kicked
off
the
online
music
curation
scene
using
manual
curation
to
create
playlists
for
users
this
meant
that
a
team
of
music
experts
or
other
human
curators
would
put
together
playlists
that
they
just
thought
sounded
good
and
then
users
would
listen
to
those
playlists
later
beats
music
would
employ
this
same
strategy
manual
curation
worked
alright
but
it
was
based
on
that
specific
curator’s
choices
and
therefore
couldn’t
take
into
account
each
listener’s
individual
music
taste
like
songza
pandora
was
also
one
of
the
original
players
in
digital
music
curation
it
employed
a
slightly
more
advanced
approach
instead
manually
tagging
attributes
of
songs
this
meant
a
group
of
people
listened
to
music
chose
a
bunch
of
descriptive
words
for
each
track
and
tagged
the
tracks
accordingly
then
pandora’s
code
could
simply
filter
for
certain
tags
to
make
playlists
of
similar-sounding
music
around
that
same
time
a
music
intelligence
agency
from
the
mit
media
lab
called
the
echo
nest
was
born
which
took
a
radical
cutting-edge
approach
to
personalized
music
the
echo
nest
used
algorithms
to
analyze
the
audio
and
textual
content
of
music
allowing
it
to
perform
music
identification
personalized
recommendation
playlist
creation
and
analysis
finally
taking
another
approach
is
lastfm
which
still
exists
today
and
uses
a
process
called
collaborative
filtering
to
identify
music
its
users
might
like
but
more
on
that
in
a
moment
so
if
that’s
how
other
music
curation
services
have
handled
recommendations
how
does
spotify’s
magic
engine
run?
how
does
it
seem
to
nail
individual
users’
tastes
so
much
more
accurately
than
any
of
the
other
services?
spotify
doesn’t
actually
use
a
single
revolutionary
recommendation
model
instead
they
mix
together
some
of
the
best
strategies
used
by
other
services
to
create
their
own
uniquely
powerful
discovery
engine
to
create
discover
weekly
there
are
three
main
types
of
recommendation
models
that
spotify
employs:
let’s
dive
into
how
each
of
these
recommendation
models
work!
first
some
background:
when
people
hear
the
words
collaborative
filtering
they
generally
think
of
netflix
as
it
was
one
of
the
first
companies
to
use
this
method
to
power
a
recommendation
model
taking
users’
star-based
movie
ratings
to
inform
its
understanding
of
which
movies
to
recommend
to
other
similar
users
after
netflix
was
successful
the
use
of
collaborative
filtering
spread
quickly
and
is
now
often
the
starting
point
for
anyone
trying
to
make
a
recommendation
model
unlike
netflix
spotify
doesn’t
have
a
star-based
system
with
which
users
rate
their
music
instead
spotify’s
data
is
implicit
feedback
—
specifically
the
stream
counts
of
the
tracks
and
additional
streaming
data
such
as
whether
a
user
saved
the
track
to
their
own
playlist
or
visited
the
artist’s
page
after
listening
to
a
song
but
what
is
collaborative
filtering
truly
and
how
does
it
work?
here’s
a
high-level
rundown
explained
in
a
quick
conversation:
what’s
going
on
here?
each
of
these
individuals
has
track
preferences:
the
one
on
the
left
likes
tracks
p
q
r
and
s
while
the
one
on
the
right
likes
tracks
q
r
s
and
t
collaborative
filtering
then
uses
that
data
to
say:
hmmm
you
both
like
three
of
the
same
tracks
—
q
r
and
s
—
so
you
are
probably
similar
users
therefore
you’re
each
likely
to
enjoy
other
tracks
that
the
other
person
has
listened
to
that
you
haven’t
heard
yet
therefore
it
suggests
that
the
one
on
the
right
check
out
track
p
—
the
only
track
not
mentioned
but
that
his
similar
counterpart
enjoyed
—
and
the
one
on
the
left
check
out
track
t
for
the
same
reasoning
simple
right?
but
how
does
spotify
actually
use
that
concept
in
practice
to
calculate
millions
of
users’
suggested
tracks
based
on
millions
of
other
users’
preferences?
with
matrix
math
done
with
python
libraries!
in
actuality
this
matrix
you
see
here
is
gigantic
each
row
represents
one
of
spotify’s
140
million
users
—
if
you
use
spotify
you
yourself
are
a
row
in
this
matrix
—
and
each
column
represents
one
of
the
30
million
songs
in
spotify’s
database
then
the
python
library
runs
this
long
complicated
matrix
factorization
formula:
when
it
finishes
we
end
up
with
two
types
of
vectors
represented
here
by
x
and
y
x
is
a
user
vector
representing
one
single
user’s
taste
and
y
is
a
song
vector
representing
one
single
song’s
profile
now
we
have
140
million
user
vectors
and
30
million
song
vectors
the
actual
content
of
these
vectors
is
just
a
bunch
of
numbers
that
are
essentially
meaningless
on
their
own
but
are
hugely
useful
when
compared
to
find
out
which
users’
musical
tastes
are
most
similar
to
mine
collaborative
filtering
compares
my
vector
with
all
of
the
other
users’
vectors
ultimately
spitting
out
which
users
are
the
closest
matches
the
same
goes
for
the
y
vector
songs:
you
can
compare
a
single
song’s
vector
with
all
the
others
and
find
out
which
songs
are
most
similar
to
the
one
in
question
collaborative
filtering
does
a
pretty
good
job
but
spotify
knew
they
could
do
even
better
by
adding
another
engine
enter
nlp
the
second
type
of
recommendation
models
that
spotify
employs
are
natural
language
processing
nlp
models
the
source
data
for
these
models
as
the
name
suggests
are
regular
ol’
words:
track
metadata
news
articles
blogs
and
other
text
around
the
internet
natural
language
processing
which
is
the
ability
of
a
computer
to
understand
human
speech
as
it
is
spoken
is
a
vast
field
unto
itself
often
harnessed
through
sentiment
analysis
apis
the
exact
mechanisms
behind
nlp
are
beyond
the
scope
of
this
article
but
here’s
what
happens
on
a
very
high
level:
spotify
crawls
the
web
constantly
looking
for
blog
posts
and
other
written
text
about
music
to
figure
out
what
people
are
saying
about
specific
artists
and
songs
—
which
adjectives
and
what
particular
language
is
frequently
used
in
reference
to
those
artists
and
songs
and
which
other
artists
and
songs
are
also
being
discussed
alongside
them
while
i
don’t
know
the
specifics
of
how
spotify
chooses
to
then
process
this
scraped
data
i
can
offer
some
insight
based
on
how
the
echo
nest
used
to
work
with
them
they
would
bucket
spotify’s
data
up
into
what
they
call
cultural
vectors
or
top
terms
each
artist
and
song
had
thousands
of
top
terms
that
changed
on
the
daily
each
term
had
an
associated
weight
which
correlated
to
its
relative
importance
—
roughly
the
probability
that
someone
will
describe
the
music
or
artist
with
that
term
then
much
like
in
collaborative
filtering
the
nlp
model
uses
these
terms
and
weights
to
create
a
vector
representation
of
the
song
that
can
be
used
to
determine
if
two
pieces
of
music
are
similar
cool
right?
first
a
question
you
might
be
thinking:
first
of
all
adding
a
third
model
further
improves
the
accuracy
of
the
music
recommendation
service
but
this
model
also
serves
a
secondary
purpose:
unlike
the
first
two
types
raw
audio
models
take
new
songs
into
account
take
for
example
a
song
your
singer-songwriter
friend
has
put
up
on
spotify
maybe
it
only
has
50
listens
so
there
are
few
other
listeners
to
collaboratively
filter
it
against
it
also
isn’t
mentioned
anywhere
on
the
internet
yet
so
nlp
models
won’t
pick
it
up
luckily
raw
audio
models
don’t
discriminate
between
new
tracks
and
popular
tracks
so
with
their
help
your
friend’s
song
could
end
up
in
a
discover
weekly
playlist
alongside
popular
songs!
but
how
can
we
analyze
raw
audio
data
which
seems
so
abstract?
with
convolutional
neural
networks!
convolutional
neural
networks
are
the
same
technology
used
in
facial
recognition
software
in
spotify’s
case
they’ve
been
modified
for
use
on
audio
data
instead
of
pixels
here’s
an
example
of
a
neural
network
architecture:
this
particular
neural
network
has
four
convolutional
layers
seen
as
the
thick
bars
on
the
left
and
three
dense
layers
seen
as
the
more
narrow
bars
on
the
right
the
inputs
are
time-frequency
representations
of
audio
frames
which
are
then
concatenated
or
linked
together
to
form
the
spectrogram
the
audio
frames
go
through
these
convolutional
layers
and
after
passing
through
the
last
one
you
can
see
a
global
temporal
pooling
layer
which
pools
across
the
entire
time
axis
effectively
computing
statistics
of
the
learned
features
across
the
time
of
the
song
after
processing
the
neural
network
spits
out
an
understanding
of
the
song
including
characteristics
like
estimated
time
signature
key
mode
tempo
and
loudness
below
is
a
plot
of
data
for
a
30-second
snippet
of
around
the
world
by
daft
punk
ultimately
this
reading
of
the
song’s
key
characteristics
allows
spotify
to
understand
fundamental
similarities
between
songs
and
therefore
which
users
might
enjoy
them
based
on
their
own
listening
history
that
covers
the
basics
of
the
three
major
types
of
recommendation
models
feeding
spotify’s
recommendations
pipeline
and
ultimately
powering
the
discover
weekly
playlist!
of
course
these
recommendation
models
are
all
connected
to
spotify’s
larger
ecosystem
which
includes
giant
amounts
of
data
storage
and
uses
lots
of
hadoop
clusters
to
scale
recommendations
and
make
these
engines
work
on
enormous
matrices
endless
online
music
articles
and
huge
numbers
of
audio
files
i
hope
this
was
informative
and
piqued
your
curiosity
like
it
did
mine
for
now
i’ll
be
working
my
way
through
my
own
discover
weekly
finding
my
new
favorite
music
while
appreciating
all
the
machine
learning
that’s
going
on
behind
the
scenes
🎶
thanks
also
to
ladycollective
for
reading
this
article
and
suggesting
edits
software
engineer
writer
and
generally
creative
human
interested
in
art
feminism
mindfulness
and
authenticity
http:sophiacioccacom
welcome
to
a
place
where
words
matter
on
medium
smart
voices
and
original
ideas
take
center
stage
—
with
no
ads
in
sight
watch
follow
all
the
topics
you
care
about
and
we’ll
deliver
the
best
stories
for
you
to
your
homepage
and
inbox
explore
get
unlimited
access
to
the
best
stories
on
medium
—
and
support
writers
while
you’re
at
it
just
$5month
upgrade
""
at
athelas
we
use
convolutional
neural
networkscnns
for
a
lot
more
than
just
classification!
in
this
post
we’ll
see
how
cnns
can
be
used
with
great
results
in
image
instance
segmentation
ever
since
alex
krizhevsky
geoff
hinton
and
ilya
sutskever
won
imagenet
in
2012
convolutional
neural
networkscnns
have
become
the
gold
standard
for
image
classification
in
fact
since
then
cnns
have
improved
to
the
point
where
they
now
outperform
humans
on
the
imagenet
challenge!
while
these
results
are
impressive
image
classification
is
far
simpler
than
the
complexity
and
diversity
of
true
human
visual
understanding
in
classification
there’s
generally
an
image
with
a
single
object
as
the
focus
and
the
task
is
to
say
what
that
image
is
see
above
but
when
we
look
at
the
world
around
us
we
carry
out
far
more
complex
tasks
we
see
complicated
sights
with
multiple
overlapping
objects
and
different
backgrounds
and
we
not
only
classify
these
different
objects
but
also
identify
their
boundaries
differences
and
relations
to
one
another!
can
cnns
help
us
with
such
complex
tasks?
namely
given
a
more
complicated
image
can
we
use
cnns
to
identify
the
different
objects
in
the
image
and
their
boundaries?
as
has
been
shown
by
ross
girshick
and
his
peers
over
the
last
few
years
the
answer
is
conclusively
yes
through
this
post
we’ll
cover
the
intuition
behind
some
of
the
main
techniques
used
in
object
detection
and
segmentation
and
see
how
they’ve
evolved
from
one
implementation
to
the
next
in
particular
we’ll
cover
r-cnn
regional
cnn
the
original
application
of
cnns
to
this
problem
along
with
its
descendants
fast
r-cnn
and
faster
r-cnn
finally
we’ll
cover
mask
r-cnn
a
paper
released
recently
by
facebook
research
that
extends
such
object
detection
techniques
to
provide
pixel
level
segmentation
here
are
the
papers
referenced
in
this
post:
inspired
by
the
research
of
hinton’s
lab
at
the
university
of
toronto
a
small
team
at
uc
berkeley
led
by
professor
jitendra
malik
asked
themselves
what
today
seems
like
an
inevitable
question:
object
detection
is
the
task
of
finding
the
different
objects
in
an
image
and
classifying
them
as
seen
in
the
image
above
the
team
comprised
of
ross
girshick
a
name
we’ll
see
again
jeff
donahue
and
trevor
darrel
found
that
this
problem
can
be
solved
with
krizhevsky’s
results
by
testing
on
the
pascal
voc
challenge
a
popular
object
detection
challenge
akin
to
imagenet
they
write
let’s
now
take
a
moment
to
understand
how
their
architecture
regions
with
cnns
r-cnn
works
understanding
r-cnn
the
goal
of
r-cnn
is
to
take
in
an
image
and
correctly
identify
where
the
main
objects
via
a
bounding
box
in
the
image
but
how
do
we
find
out
where
these
bounding
boxes
are?
r-cnn
does
what
we
might
intuitively
do
as
well
-
propose
a
bunch
of
boxes
in
the
image
and
see
if
any
of
them
actually
correspond
to
an
object
r-cnn
creates
these
bounding
boxes
or
region
proposals
using
a
process
called
selective
search
which
you
can
read
about
here
at
a
high
level
selective
search
shown
in
the
image
above
looks
at
the
image
through
windows
of
different
sizes
and
for
each
size
tries
to
group
together
adjacent
pixels
by
texture
color
or
intensity
to
identify
objects
once
the
proposals
are
created
r-cnn
warps
the
region
to
a
standard
square
size
and
passes
it
through
to
a
modified
version
of
alexnet
the
winning
submission
to
imagenet
2012
that
inspired
r-cnn
as
shown
above
on
the
final
layer
of
the
cnn
r-cnn
adds
a
support
vector
machine
svm
that
simply
classifies
whether
this
is
an
object
and
if
so
what
object
this
is
step
4
in
the
image
above
improving
the
bounding
boxes
now
having
found
the
object
in
the
box
can
we
tighten
the
box
to
fit
the
true
dimensions
of
the
object?
we
can
and
this
is
the
final
step
of
r-cnn
r-cnn
runs
a
simple
linear
regression
on
the
region
proposal
to
generate
tighter
bounding
box
coordinates
to
get
our
final
result
here
are
the
inputs
and
outputs
of
this
regression
model:
so
to
summarize
r-cnn
is
just
the
following
steps:
r-cnn
works
really
well
but
is
really
quite
slow
for
a
few
simple
reasons:
in
2015
ross
girshick
the
first
author
of
r-cnn
solved
both
these
problems
leading
to
the
second
algorithm
in
our
short
history
-
fast
r-cnn
let’s
now
go
over
its
main
insights
fast
r-cnn
insight
1:
roi
region
of
interest
pooling
for
the
forward
pass
of
the
cnn
girshick
realized
that
for
each
image
a
lot
of
proposed
regions
for
the
image
invariably
overlapped
causing
us
to
run
the
same
cnn
computation
again
and
again
~2000
times!
his
insight
was
simple
—
why
not
run
the
cnn
just
once
per
image
and
then
find
a
way
to
share
that
computation
across
the
~2000
proposals?
this
is
exactly
what
fast
r-cnn
does
using
a
technique
known
as
roipool
region
of
interest
pooling
at
its
core
roipool
shares
the
forward
pass
of
a
cnn
for
an
image
across
its
subregions
in
the
image
above
notice
how
the
cnn
features
for
each
region
are
obtained
by
selecting
a
corresponding
region
from
the
cnn’s
feature
map
then
the
features
in
each
region
are
pooled
usually
using
max
pooling
so
all
it
takes
us
is
one
pass
of
the
original
image
as
opposed
to
~2000!
fast
r-cnn
insight
2:
combine
all
models
into
one
network
the
second
insight
of
fast
r-cnn
is
to
jointly
train
the
cnn
classifier
and
bounding
box
regressor
in
a
single
model
where
earlier
we
had
different
models
to
extract
image
features
cnn
classify
svm
and
tighten
bounding
boxes
regressor
fast
r-cnn
instead
used
a
single
network
to
compute
all
three
you
can
see
how
this
was
done
in
the
image
above
fast
r-cnn
replaced
the
svm
classifier
with
a
softmax
layer
on
top
of
the
cnn
to
output
a
classification
it
also
added
a
linear
regression
layer
parallel
to
the
softmax
layer
to
output
bounding
box
coordinates
in
this
way
all
the
outputs
needed
came
from
one
single
network!
here
are
the
inputs
and
outputs
to
this
overall
model:
even
with
all
these
advancements
there
was
still
one
remaining
bottleneck
in
the
fast
r-cnn
process
—
the
region
proposer
as
we
saw
the
very
first
step
to
detecting
the
locations
of
objects
is
generating
a
bunch
of
potential
bounding
boxes
or
regions
of
interest
to
test
in
fast
r-cnn
these
proposals
were
created
using
selective
search
a
fairly
slow
process
that
was
found
to
be
the
bottleneck
of
the
overall
process
in
the
middle
2015
a
team
at
microsoft
research
composed
of
shaoqing
ren
kaiming
he
ross
girshick
and
jian
sun
found
a
way
to
make
the
region
proposal
step
almost
cost
free
through
an
architecture
they
creatively
named
faster
r-cnn
the
insight
of
faster
r-cnn
was
that
region
proposals
depended
on
features
of
the
image
that
were
already
calculated
with
the
forward
pass
of
the
cnn
first
step
of
classification
so
why
not
reuse
those
same
cnn
results
for
region
proposals
instead
of
running
a
separate
selective
search
algorithm?
indeed
this
is
just
what
the
faster
r-cnn
team
achieved
in
the
image
above
you
can
see
how
a
single
cnn
is
used
to
both
carry
out
region
proposals
and
classification
this
way
only
one
cnn
needs
to
be
trained
and
we
get
region
proposals
almost
for
free!
the
authors
write:
here
are
the
inputs
and
outputs
of
their
model:
how
the
regions
are
generated
let’s
take
a
moment
to
see
how
faster
r-cnn
generates
these
region
proposals
from
cnn
features
faster
r-cnn
adds
a
fully
convolutional
network
on
top
of
the
features
of
the
cnn
creating
what’s
known
as
the
region
proposal
network
the
region
proposal
network
works
by
passing
a
sliding
window
over
the
cnn
feature
map
and
at
each
window
outputting
k
potential
bounding
boxes
and
scores
for
how
good
each
of
those
boxes
is
expected
to
be
what
do
these
k
boxes
represent?
intuitively
we
know
that
objects
in
an
image
should
fit
certain
common
aspect
ratios
and
sizes
for
instance
we
know
that
we
want
some
rectangular
boxes
that
resemble
the
shapes
of
humans
likewise
we
know
we
won’t
see
many
boxes
that
are
very
very
thin
in
such
a
way
we
create
k
such
common
aspect
ratios
we
call
anchor
boxes
for
each
such
anchor
box
we
output
one
bounding
box
and
score
per
position
in
the
image
with
these
anchor
boxes
in
mind
let’s
take
a
look
at
the
inputs
and
outputs
to
this
region
proposal
network:
we
then
pass
each
such
bounding
box
that
is
likely
to
be
an
object
into
fast
r-cnn
to
generate
a
classification
and
tightened
bounding
boxes
so
far
we’ve
seen
how
we’ve
been
able
to
use
cnn
features
in
many
interesting
ways
to
effectively
locate
different
objects
in
an
image
with
bounding
boxes
can
we
extend
such
techniques
to
go
one
step
further
and
locate
exact
pixels
of
each
object
instead
of
just
bounding
boxes?
this
problem
known
as
image
segmentation
is
what
kaiming
he
and
a
team
of
researchers
including
girshick
explored
at
facebook
ai
using
an
architecture
known
as
mask
r-cnn
much
like
fast
r-cnn
and
faster
r-cnn
mask
r-cnn’s
underlying
intuition
is
straight
forward
given
that
faster
r-cnn
works
so
well
for
object
detection
could
we
extend
it
to
also
carry
out
pixel
level
segmentation?
mask
r-cnn
does
this
by
adding
a
branch
to
faster
r-cnn
that
outputs
a
binary
mask
that
says
whether
or
not
a
given
pixel
is
part
of
an
object
the
branch
in
white
in
the
above
image
as
before
is
just
a
fully
convolutional
network
on
top
of
a
cnn
based
feature
map
here
are
its
inputs
and
outputs:
but
the
mask
r-cnn
authors
had
to
make
one
small
adjustment
to
make
this
pipeline
work
as
expected
roialign
-
realigning
roipool
to
be
more
accurate
when
run
without
modifications
on
the
original
faster
r-cnn
architecture
the
mask
r-cnn
authors
realized
that
the
regions
of
the
feature
map
selected
by
roipool
were
slightly
misaligned
from
the
regions
of
the
original
image
since
image
segmentation
requires
pixel
level
specificity
unlike
bounding
boxes
this
naturally
led
to
inaccuracies
the
authors
were
able
to
solve
this
problem
by
cleverly
adjusting
roipool
to
be
more
precisely
aligned
using
a
method
known
as
roialign
imagine
we
have
an
image
of
size
128x128
and
a
feature
map
of
size
25x25
let’s
imagine
we
want
features
the
region
corresponding
to
the
top-left
15x15
pixels
in
the
original
image
see
above
how
might
we
select
these
pixels
from
the
feature
map?
we
know
each
pixel
in
the
original
image
corresponds
to
~
25128
pixels
in
the
feature
map
to
select
15
pixels
from
the
original
image
we
just
select
15
*
25128
~=
293
pixels
in
roipool
we
would
round
this
down
and
select
2
pixels
causing
a
slight
misalignment
however
in
roialign
we
avoid
such
rounding
instead
we
use
bilinear
interpolation
to
get
a
precise
idea
of
what
would
be
at
pixel
293
this
at
a
high
level
is
what
allows
us
to
avoid
the
misalignments
caused
by
roipool
once
these
masks
are
generated
mask
r-cnn
combines
them
with
the
classifications
and
bounding
boxes
from
faster
r-cnn
to
generate
such
wonderfully
precise
segmentations:
if
you’re
interested
in
trying
out
these
algorithms
yourselves
here
are
relevant
repositories:
faster
r-cnn
mask
r-cnn
in
just
3
years
we’ve
seen
how
the
research
community
has
progressed
from
krizhevsky
et
al’s
original
result
to
r-cnn
and
finally
all
the
way
to
such
powerful
results
as
mask
r-cnn
seen
in
isolation
results
like
mask
r-cnn
seem
like
incredible
leaps
of
genius
that
would
be
unapproachable
yet
through
this
post
i
hope
you’ve
seen
how
such
advancements
are
really
the
sum
of
intuitive
incremental
improvements
through
years
of
hard
work
and
collaboration
each
of
the
ideas
proposed
by
r-cnn
fast
r-cnn
faster
r-cnn
and
finally
mask
r-cnn
were
not
necessarily
quantum
leaps
yet
their
sum
products
have
led
to
really
remarkable
results
that
bring
us
closer
to
a
human
level
understanding
of
sight
what
particularly
excites
me
is
that
the
time
between
r-cnn
and
mask
r-cnn
was
just
three
years!
with
continued
funding
focus
and
support
how
much
further
can
computer
vision
improve
over
the
next
three
years?
if
you
see
any
errors
or
issues
in
this
post
please
contact
me
at
dhruv@getathelascom
and
ill
immediately
correct
them!
if
you’re
interested
in
applying
such
techniques
come
join
us
at
athelas
where
we
apply
computer
vision
to
blood
diagnostics
daily:
other
posts
we’ve
written:
thanks
to
bharath
ramsundar
pranav
ramkrishnan
tanay
tandon
and
oliver
cameron
for
help
with
this
post!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
@dhruvp
vp
eng
@athelas
mit
math
and
cs
undergrad
’13
mit
cs
masters
’14
previously:
director
of
ai
programs
@
udacity
blood
diagnostics
through
deep
learning
http:athelascom
""
i
sometimes
see
people
refer
to
neural
networks
as
just
another
tool
in
your
machine
learning
toolbox
they
have
some
pros
and
cons
they
work
here
or
there
and
sometimes
you
can
use
them
to
win
kaggle
competitions
unfortunately
this
interpretation
completely
misses
the
forest
for
the
trees
neural
networks
are
not
just
another
classifier
they
represent
the
beginning
of
a
fundamental
shift
in
how
we
write
software
they
are
software
20
the
classical
stack
of
software
10
is
what
we’re
all
familiar
with
—
it
is
written
in
languages
such
as
python
c
etc
it
consists
of
explicit
instructions
to
the
computer
written
by
a
programmer
by
writing
each
line
of
code
the
programmer
identifies
a
specific
point
in
program
space
with
some
desirable
behavior
in
contrast
software
20
can
be
written
in
much
more
abstract
human
unfriendly
language
such
as
the
weights
of
a
neural
network
no
human
is
involved
in
writing
this
code
because
there
are
a
lot
of
weights
typical
networks
might
have
millions
and
coding
directly
in
weights
is
kind
of
hard
i
tried
instead
our
approach
is
to
specify
some
goal
on
the
behavior
of
a
desirable
program
eg
satisfy
a
dataset
of
input
output
pairs
of
examples
or
win
a
game
of
go
write
a
rough
skeleton
of
the
code
eg
a
neural
net
architecture
that
identifies
a
subset
of
program
space
to
search
and
use
the
computational
resources
at
our
disposal
to
search
this
space
for
a
program
that
works
in
the
specific
case
of
neural
networks
we
restrict
the
search
to
a
continuous
subset
of
the
program
space
where
the
search
process
can
be
made
somewhat
surprisingly
efficient
with
backpropagation
and
stochastic
gradient
descent
it
turns
out
that
a
large
portion
of
real-world
problems
have
the
property
that
it
is
significantly
easier
to
collect
the
data
or
more
generally
identify
a
desirable
behavior
than
to
explicitly
write
the
program
in
these
cases
the
programmers
will
often
split
into
two
the
20
programmers
manually
curate
maintain
massage
clean
and
label
datasets
each
labeled
example
literally
programs
the
final
system
because
the
dataset
gets
compiled
into
software
20
code
via
the
optimization
meanwhile
the
10
programmers
maintain
the
surrounding
tools
analytics
visualizations
labeling
interfaces
infrastructure
and
the
training
code
let’s
briefly
examine
some
concrete
examples
of
this
ongoing
transition
in
each
of
these
areas
we’ve
seen
improvements
over
the
last
few
years
when
we
give
up
on
trying
to
address
a
complex
problem
by
writing
explicit
code
and
instead
transition
the
code
into
the
20
stack
visual
recognition
used
to
consist
of
engineered
features
with
a
bit
of
machine
learning
sprinkled
on
top
at
the
end
eg
an
svm
since
then
we
discovered
much
more
powerful
visual
features
by
obtaining
large
datasets
eg
imagenet
and
searching
in
the
space
of
convolutional
neural
network
architectures
more
recently
we
don’t
even
trust
ourselves
to
hand-code
the
architectures
and
we’ve
begun
searching
over
those
as
well
speech
recognition
used
to
involve
a
lot
of
preprocessing
gaussian
mixture
models
and
hidden
markov
models
but
today
consist
almost
entirely
of
neural
net
stuff
a
very
related
often
cited
humorous
quote
attributed
to
fred
jelinek
from
1985
reads
every
time
i
fire
a
linguist
the
performance
of
our
speech
recognition
system
goes
up
speech
synthesis
has
historically
been
approached
with
various
stitching
mechanisms
but
today
the
state
of
the
art
models
are
large
convnets
eg
wavenet
that
produce
raw
audio
signal
outputs
machine
translation
has
usually
been
approaches
with
phrase-based
statistical
techniques
but
neural
networks
are
quickly
becoming
dominant
my
favorite
architectures
are
trained
in
the
multilingual
setting
where
a
single
model
translates
from
any
source
language
to
any
target
language
and
in
weakly
supervised
or
entirely
unsupervised
settings
games
explicitly
hand-coded
go
playing
programs
have
been
developed
for
a
long
while
but
alphago
zero
a
convnet
that
looks
at
the
raw
state
of
the
board
and
plays
a
move
has
now
become
by
far
the
strongest
player
of
the
game
i
expect
we’re
going
to
see
very
similar
results
in
other
areas
eg
dota
2
or
starcraft
databases
more
traditional
systems
outside
of
artificial
intelligence
are
also
seeing
early
hints
of
a
transition
for
instance
the
case
for
learned
index
structures
replaces
core
components
of
a
data
management
system
with
a
neural
network
outperforming
cache-optimized
b-trees
by
up
to
70%
in
speed
while
saving
an
order-of-magnitude
in
memory
you’ll
notice
that
many
of
my
links
above
involve
work
done
at
google
this
is
because
google
is
currently
at
the
forefront
of
re-writing
large
chunks
of
itself
into
software
20
code
one
model
to
rule
them
all
provides
an
early
sketch
of
what
this
might
look
like
where
the
statistical
strength
of
the
individual
domains
is
amalgamated
into
one
consistent
understanding
of
the
world
why
should
we
prefer
to
port
complex
programs
into
software
20?
clearly
one
easy
answer
is
that
they
work
better
in
practice
however
there
are
a
lot
of
other
convenient
reasons
to
prefer
this
stack
let’s
take
a
look
at
some
of
the
benefits
of
software
20
think:
a
convnet
compared
to
software
10
think:
a
production-level
c
code
base
software
20
is:
computationally
homogeneous
a
typical
neural
network
is
to
the
first
order
made
up
of
a
sandwich
of
only
two
operations:
matrix
multiplication
and
thresholding
at
zero
relu
compare
that
with
the
instruction
set
of
classical
software
which
is
significantly
more
heterogenous
and
complex
because
you
only
have
to
provide
software
10
implementation
for
a
small
number
of
the
core
computational
primitives
eg
matrix
multiply
it
is
much
easier
to
make
various
correctnessperformance
guarantees
simple
to
bake
into
silicon
as
a
corollary
since
the
instruction
set
of
a
neural
network
is
relatively
small
it
is
significantly
easier
to
implement
these
networks
much
closer
to
silicon
eg
with
custom
asics
neuromorphic
chips
and
so
on
the
world
will
change
when
low-powered
intelligence
becomes
pervasive
around
us
eg
small
inexpensive
chips
could
come
with
a
pretrained
convnet
a
speech
recognizer
and
a
wavenet
speech
synthesis
network
all
integrated
in
a
small
protobrain
that
you
can
attach
to
stuff
constant
running
time
every
iteration
of
a
typical
neural
net
forward
pass
takes
exactly
the
same
amount
of
flops
there
is
zero
variability
based
on
the
different
execution
paths
your
code
could
take
through
some
sprawling
c
code
base
of
course
you
could
have
dynamic
compute
graphs
but
the
execution
flow
is
normally
still
significantly
constrained
this
way
we
are
also
almost
guaranteed
to
never
find
ourselves
in
unintended
infinite
loops
constant
memory
use
related
to
the
above
there
is
no
dynamically
allocated
memory
anywhere
so
there
is
also
little
possibility
of
swapping
to
disk
or
memory
leaks
that
you
have
to
hunt
down
in
your
code
it
is
highly
portable
a
sequence
of
matrix
multiplies
is
significantly
easier
to
run
on
arbitrary
computational
configurations
compared
to
classical
binaries
or
scripts
it
is
very
agile
if
you
had
a
c
code
and
someone
wanted
you
to
make
it
twice
as
fast
at
cost
of
performance
if
needed
it
would
be
highly
non-trivial
to
tune
the
system
for
the
new
spec
however
in
software
20
we
can
take
our
network
remove
half
of
the
channels
retrain
and
there
—
it
runs
exactly
at
twice
the
speed
and
works
a
bit
worse
it’s
magic
conversely
if
you
happen
to
get
more
datacompute
you
can
immediately
make
your
program
work
better
just
by
adding
more
channels
and
retraining
modules
can
meld
into
an
optimal
whole
our
software
is
often
decomposed
into
modules
that
communicate
through
public
functions
apis
or
endpoints
however
if
two
software
20
modules
that
were
originally
trained
separately
interact
we
can
easily
backpropagate
through
the
whole
think
about
how
amazing
it
could
be
if
your
web
browser
could
automatically
re-design
the
low-level
system
instructions
10
stacks
down
to
achieve
a
higher
efficiency
in
loading
web
pages
with
20
this
is
the
default
behavior
it
is
better
than
you
finally
and
most
importantly
a
neural
network
is
a
better
piece
of
code
than
anything
you
or
i
can
come
up
with
in
a
large
fraction
of
valuable
verticals
which
currently
at
the
very
least
involve
anything
to
do
with
imagesvideo
and
soundspeech
the
20
stack
also
has
some
of
its
own
disadvantages
at
the
end
of
the
optimization
we’re
left
with
large
networks
that
work
well
but
it’s
very
hard
to
tell
how
across
many
applications
areas
we’ll
be
left
with
a
choice
of
using
a
90%
accurate
model
we
understand
or
99%
accurate
model
we
don’t
the
20
stack
can
fail
in
unintuitive
and
embarrassing
ways
or
worse
they
can
silently
fail
eg
by
silently
adopting
biases
in
their
training
data
which
are
very
difficult
to
properly
analyze
and
examine
when
their
sizes
are
easily
in
the
millions
in
most
cases
finally
we’re
still
discovering
some
of
the
peculiar
properties
of
this
stack
for
instance
the
existence
of
adversarial
examples
and
attacks
highlights
the
unintuitive
nature
of
this
stack
software
10
is
code
we
write
software
20
is
code
we
do
not
write
but
seems
to
work
well
it
is
likely
that
any
setting
where
the
program
is
not
obvious
but
one
can
repeatedly
evaluate
the
performance
of
it
eg
—
did
you
classify
some
images
correctly?
do
you
win
games
of
go?
will
be
subject
to
this
transition
because
the
optimization
can
find
much
better
code
than
what
we
can
write
if
you
think
of
neural
networks
as
a
new
software
stack
and
not
just
a
pretty
good
classifier
it
quickly
becomes
apparent
there
is
a
lot
of
work
to
do
for
example
from
a
systems
perspective
in
the
10
stack
llvm
ir
forms
a
middle
layer
between
a
number
of
front
ends
languages
and
back
ends
architectures
and
provides
an
opportunity
for
optimization
with
neural
networks
we’re
already
seeing
an
explosion
of
front
ends
for
specifying
program
subsets
to
search
over
pytorch
tf
chainer
mxnet
etc
and
back
ends
to
run
the
training
compilation
and
inference
cpu
gpu
tpu?
ipu?
""
but
what
is
a
fitting
ir
and
how
we
can
optimize
it
halide-like?
as
another
example
we’ve
built
up
a
vast
amount
of
tooling
that
assists
humans
in
writing
10
code
like
powerful
ides
with
features
like
syntax
highlighting
debuggers
profilers
go
to
def
git
integration
etc
who
is
going
to
develop
the
first
powerful
software
20
ides
which
help
with
all
of
the
workflows
in
accumulating
visualizing
cleaning
labeling
and
sourcing
datasets?
there
is
a
lot
of
room
for
a
layer
of
intelligence
assisting
the
20
programmers
eg
perhaps
the
ide
bubbles
up
images
that
the
network
suspects
are
mislabeled
or
assists
in
labeling
or
finds
examples
where
the
network
is
currently
uncertain
finally
in
the
long
term
the
future
of
software
20
is
bright
because
it
is
increasingly
clear
to
many
that
when
we
develop
agi
it
will
certainly
be
written
in
software
20
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
director
of
ai
at
tesla
previously
research
scientist
at
openai
and
phd
student
at
stanford
i
like
to
train
deep
neural
nets
on
large
datasets
""
for
a
recent
hackathon
that
we
did
at
statworx
some
of
our
team
members
scraped
minutely
sp
500
data
from
the
google
finance
api
the
data
consisted
of
index
as
well
as
stock
prices
of
the
sp’s
500
constituents
having
this
data
at
hand
the
idea
of
developing
a
deep
learning
model
for
predicting
the
sp
500
index
based
on
the
500
constituents
prices
one
minute
ago
came
immediately
on
my
mind
playing
around
with
the
data
and
building
the
deep
learning
model
with
tensorflow
was
fun
and
so
i
decided
to
write
my
first
mediumcom
story:
a
little
tensorflow
tutorial
on
predicting
sp
500
stock
prices
what
you
will
read
is
not
an
in-depth
tutorial
but
more
a
high-level
introduction
to
the
important
building
blocks
and
concepts
of
tensorflow
models
the
python
code
i’ve
created
is
not
optimized
for
efficiency
but
understandability
the
dataset
i’ve
used
can
be
downloaded
from
here
40mb
our
team
exported
the
scraped
stock
data
from
our
scraping
server
as
a
csv
file
the
dataset
contains
n
=
41266
minutes
of
data
ranging
from
april
to
august
2017
on
500
stocks
as
well
as
the
total
sp
500
index
price
index
and
stocks
are
arranged
in
wide
format
the
data
was
already
cleaned
and
prepared
meaning
missing
stock
and
index
prices
were
locf’ed
last
observation
carried
forward
so
that
the
file
did
not
contain
any
missing
values
a
quick
look
at
the
sp
time
series
using
pyplotplotdata['sp500']:
note:
this
is
actually
the
lead
of
the
sp
500
index
meaning
its
value
is
shifted
1
minute
into
the
future
this
operation
is
necessary
since
we
want
to
predict
the
next
minute
of
the
index
and
not
the
current
minute
the
dataset
was
split
into
training
and
test
data
the
training
data
contained
80%
of
the
total
dataset
the
data
was
not
shuffled
but
sequentially
sliced
the
training
data
ranges
from
april
to
approx
end
of
july
2017
the
test
data
ends
end
of
august
2017
there
are
a
lot
of
different
approaches
to
time
series
cross
validation
such
as
rolling
forecasts
with
and
without
refitting
or
more
elaborate
concepts
such
as
time
series
bootstrap
resampling
the
latter
involves
repeated
samples
from
the
remainder
of
the
seasonal
decomposition
of
the
time
series
in
order
to
simulate
samples
that
follow
the
same
seasonal
pattern
as
the
original
time
series
but
are
not
exact
copies
of
its
values
most
neural
network
architectures
benefit
from
scaling
the
inputs
sometimes
also
the
output
why?
because
most
common
activation
functions
of
the
network’s
neurons
such
as
tanh
or
sigmoid
are
defined
on
the
[-1
1]
or
[0
1]
interval
respectively
nowadays
rectified
linear
unit
relu
activations
are
commonly
used
activations
which
are
unbounded
on
the
axis
of
possible
activation
values
however
we
will
scale
both
the
inputs
and
targets
anyway
scaling
can
be
easily
accomplished
in
python
using
sklearn’s
minmaxscaler
remark:
caution
must
be
undertaken
regarding
what
part
of
the
data
is
scaled
and
when
a
common
mistake
is
to
scale
the
whole
dataset
before
training
and
test
split
are
being
applied
why
is
this
a
mistake?
because
scaling
invokes
the
calculation
of
statistics
eg
the
minmax
of
a
variable
when
performing
time
series
forecasting
in
real
life
you
do
not
have
information
from
future
observations
at
the
time
of
forecasting
therefore
calculation
of
scaling
statistics
has
to
be
conducted
on
training
data
and
must
then
be
applied
to
the
test
data
otherwise
you
use
future
information
at
the
time
of
forecasting
which
commonly
biases
forecasting
metrics
in
a
positive
direction
tensorflow
is
a
great
piece
of
software
and
currently
the
leading
deep
learning
and
neural
network
computation
framework
it
is
based
on
a
c
low
level
backend
but
is
usually
controlled
via
python
there
is
also
a
neat
tensorflow
library
for
r
maintained
by
rstudio
tensorflow
operates
on
a
graph
representation
of
the
underlying
computational
task
this
approach
allows
the
user
to
specify
mathematical
operations
as
elements
in
a
graph
of
data
variables
and
operators
since
neural
networks
are
actually
graphs
of
data
and
mathematical
operations
tensorflow
is
just
perfect
for
neural
networks
and
deep
learning
check
out
this
simple
example
stolen
from
our
deep
learning
introduction
from
our
blog:
in
the
figure
above
two
numbers
are
supposed
to
be
added
those
numbers
are
stored
in
two
variables
a
and
b
the
two
values
are
flowing
through
the
graph
and
arrive
at
the
square
node
where
they
are
being
added
the
result
of
the
addition
is
stored
into
another
variable
c
actually
a
b
and
c
can
be
considered
as
placeholders
any
numbers
that
are
fed
into
a
and
b
get
added
and
are
stored
into
c
this
is
exactly
how
tensorflow
works
the
user
defines
an
abstract
representation
of
the
model
neural
network
through
placeholders
and
variables
afterwards
the
placeholders
get
filled
with
real
data
and
the
actual
computations
take
place
the
following
code
implements
the
toy
example
from
above
in
tensorflow:
after
having
imported
the
tensorflow
library
two
placeholders
are
defined
using
tfplaceholder
they
correspond
to
the
two
blue
circles
on
the
left
of
the
image
above
afterwards
the
mathematical
addition
is
defined
via
tfadd
the
result
of
the
computation
is
c
=
9
with
placeholders
set
up
the
graph
can
be
executed
with
any
integer
value
for
a
and
b
of
course
the
former
problem
is
just
a
toy
example
the
required
graphs
and
computations
in
a
neural
network
are
much
more
complex
as
mentioned
before
it
all
starts
with
placeholders
we
need
two
placeholders
in
order
to
fit
our
model:
x
contains
the
network's
inputs
the
stock
prices
of
all
sp
500
constituents
at
time
t
=
t
and
y
the
network's
outputs
the
index
value
of
the
sp
500
at
time
t
=
t
""
1
the
shape
of
the
placeholders
correspond
to
[none
n_stocks]
with
[none]
meaning
that
the
inputs
are
a
2-dimensional
matrix
and
the
outputs
are
a
1-dimensional
vector
it
is
crucial
to
understand
which
input
and
output
dimensions
the
neural
net
needs
in
order
to
design
it
properly
the
none
argument
indicates
that
at
this
point
we
do
not
yet
know
the
number
of
observations
that
flow
through
the
neural
net
graph
in
each
batch
so
we
keep
if
flexible
we
will
later
define
the
variable
batch_size
that
controls
the
number
of
observations
per
training
batch
besides
placeholders
variables
are
another
cornerstone
of
the
tensorflow
universe
while
placeholders
are
used
to
store
input
and
target
data
in
the
graph
variables
are
used
as
flexible
containers
within
the
graph
that
are
allowed
to
change
during
graph
execution
weights
and
biases
are
represented
as
variables
in
order
to
adapt
during
training
variables
need
to
be
initialized
prior
to
model
training
we
will
get
into
that
a
litte
later
in
more
detail
the
model
consists
of
four
hidden
layers
the
first
layer
contains
1024
neurons
slightly
more
than
double
the
size
of
the
inputs
subsequent
hidden
layers
are
always
half
the
size
of
the
previous
layer
which
means
512
256
and
finally
128
neurons
a
reduction
of
the
number
of
neurons
for
each
subsequent
layer
compresses
the
information
the
network
identifies
in
the
previous
layers
of
course
other
network
architectures
and
neuron
configurations
are
possible
but
are
out
of
scope
for
this
introduction
level
article
it
is
important
to
understand
the
required
variable
dimensions
between
input
hidden
and
output
layers
as
a
rule
of
thumb
in
multilayer
perceptrons
mlps
the
type
of
networks
used
here
the
second
dimension
of
the
previous
layer
is
the
first
dimension
in
the
current
layer
for
weight
matrices
this
might
sound
complicated
but
is
essentially
just
each
layer
passing
its
output
as
input
to
the
next
layer
the
biases
dimension
equals
the
second
dimension
of
the
current
layer’s
weight
matrix
which
corresponds
the
number
of
neurons
in
this
layer
after
definition
of
the
required
weight
and
bias
variables
the
network
topology
the
architecture
of
the
network
needs
to
be
specified
hereby
placeholders
data
and
variables
weighs
and
biases
need
to
be
combined
into
a
system
of
sequential
matrix
multiplications
furthermore
the
hidden
layers
of
the
network
are
transformed
by
activation
functions
activation
functions
are
important
elements
of
the
network
architecture
since
they
introduce
non-linearity
to
the
system
there
are
dozens
of
possible
activation
functions
out
there
one
of
the
most
common
is
the
rectified
linear
unit
relu
which
will
also
be
used
in
this
model
the
image
below
illustrates
the
network
architecture
the
model
consists
of
three
major
building
blocks
the
input
layer
the
hidden
layers
and
the
output
layer
this
architecture
is
called
a
feedforward
network
feedforward
indicates
that
the
batch
of
data
solely
flows
from
left
to
right
other
network
architectures
such
as
recurrent
neural
networks
also
allow
data
flowing
backwards
in
the
network
the
cost
function
of
the
network
is
used
to
generate
a
measure
of
deviation
between
the
network’s
predictions
and
the
actual
observed
training
targets
for
regression
problems
the
mean
squared
error
mse
function
is
commonly
used
mse
computes
the
average
squared
deviation
between
predictions
and
targets
basically
any
differentiable
function
can
be
implemented
in
order
to
compute
a
deviation
measure
between
predictions
and
targets
however
the
mse
exhibits
certain
properties
that
are
advantageous
for
the
general
optimization
problem
to
be
solved
the
optimizer
takes
care
of
the
necessary
computations
that
are
used
to
adapt
the
network’s
weight
and
bias
variables
during
training
those
computations
invoke
the
calculation
of
so
called
gradients
that
indicate
the
direction
in
which
the
weights
and
biases
have
to
be
changed
during
training
in
order
to
minimize
the
network’s
cost
function
the
development
of
stable
and
speedy
optimizers
is
a
major
field
in
neural
network
an
deep
learning
research
here
the
adam
optimizer
is
used
which
is
one
of
the
current
default
optimizers
in
deep
learning
development
adam
stands
for
adaptive
moment
estimation
and
can
be
considered
as
a
combination
between
two
other
popular
optimizers
adagrad
and
rmsprop
initializers
are
used
to
initialize
the
network’s
variables
before
training
since
neural
networks
are
trained
using
numerical
optimization
techniques
the
starting
point
of
the
optimization
problem
is
one
the
key
factors
to
find
good
solutions
to
the
underlying
problem
there
are
different
initializers
available
in
tensorflow
each
with
different
initialization
approaches
here
i
use
the
tfvariance_scaling_initializer
which
is
one
of
the
default
initialization
strategies
note
that
with
tensorflow
it
is
possible
to
define
multiple
initialization
functions
for
different
variables
within
the
graph
however
in
most
cases
a
unified
initialization
is
sufficient
after
having
defined
the
placeholders
variables
initializers
cost
functions
and
optimizers
of
the
network
the
model
needs
to
be
trained
usually
this
is
done
by
minibatch
training
during
minibatch
training
random
data
samples
of
n
=
batch_size
are
drawn
from
the
training
data
and
fed
into
the
network
the
training
dataset
gets
divided
into
n
""
batch_size
batches
that
are
sequentially
fed
into
the
network
at
this
point
the
placeholders
x
and
y
come
into
play
they
store
the
input
and
target
data
and
present
them
to
the
network
as
inputs
and
targets
a
sampled
data
batch
of
x
flows
through
the
network
until
it
reaches
the
output
layer
there
tensorflow
compares
the
models
predictions
against
the
actual
observed
targets
y
in
the
current
batch
afterwards
tensorflow
conducts
an
optimization
step
and
updates
the
networks
parameters
corresponding
to
the
selected
learning
scheme
after
having
updated
the
weights
and
biases
the
next
batch
is
sampled
and
the
process
repeats
itself
the
procedure
continues
until
all
batches
have
been
presented
to
the
network
one
full
sweep
over
all
batches
is
called
an
epoch
the
training
of
the
network
stops
once
the
maximum
number
of
epochs
is
reached
or
another
stopping
criterion
defined
by
the
user
applies
during
the
training
we
evaluate
the
networks
predictions
on
the
test
set
—
the
data
which
is
not
learned
but
set
aside
—
for
every
5th
batch
and
visualize
it
additionally
the
images
are
exported
to
disk
and
later
combined
into
a
video
animation
of
the
training
process
see
below
the
model
quickly
learns
the
shape
und
location
of
the
time
series
in
the
test
data
and
is
able
to
produce
an
accurate
prediction
after
some
epochs
nice!
one
can
see
that
the
networks
rapidly
adapts
to
the
basic
shape
of
the
time
series
and
continues
to
learn
finer
patterns
of
the
data
this
also
corresponds
to
the
adam
learning
scheme
that
lowers
the
learning
rate
during
model
training
in
order
not
to
overshoot
the
optimization
minimum
after
10
epochs
we
have
a
pretty
close
fit
to
the
test
data!
the
final
test
mse
equals
000078
it
is
very
low
because
the
target
is
scaled
the
mean
absolute
percentage
error
of
the
forecast
on
the
test
set
is
equal
to
531%
which
is
pretty
good
note
that
this
is
just
a
fit
to
the
test
data
no
actual
out
of
sample
metrics
in
a
real
world
scenario
please
note
that
there
are
tons
of
ways
of
further
improving
this
result:
design
of
layers
and
neurons
choosing
different
initialization
and
activation
schemes
introduction
of
dropout
layers
of
neurons
early
stopping
and
so
on
furthermore
different
types
of
deep
learning
models
such
as
recurrent
neural
networks
might
achieve
better
performance
on
this
task
however
this
is
not
the
scope
of
this
introductory
post
the
release
of
tensorflow
was
a
landmark
event
in
deep
learning
research
its
flexibility
and
performance
allows
researchers
to
develop
all
kinds
of
sophisticated
neural
network
architectures
as
well
as
other
ml
algorithms
however
flexibility
comes
at
the
cost
of
longer
time-to-model
cycles
compared
to
higher
level
apis
such
as
keras
or
mxnet
nonetheless
i
am
sure
that
tensorflow
will
make
its
way
to
the
de-facto
standard
in
neural
network
and
deep
learning
development
in
research
and
practical
applications
many
of
our
customers
are
already
using
tensorflow
or
start
developing
projects
that
employ
tensorflow
models
also
our
data
science
consultants
at
statworx
are
heavily
using
tensorflow
for
deep
learning
and
neural
net
research
and
development
let’s
see
what
google
has
planned
for
the
future
of
tensorflow
one
thing
that
is
missing
at
least
in
my
opinion
is
a
neat
graphical
user
interface
for
designing
and
developing
neural
net
architectures
with
tensorflow
backend
maybe
this
is
something
google
is
already
working
on
""
if
you
have
any
comments
or
questions
on
my
first
medium
story
feel
free
to
comment
below!
i
will
try
to
answer
them
also
feel
free
to
use
my
code
or
share
this
story
with
your
peers
on
social
platforms
of
your
choice
update:
i’ve
added
both
the
python
script
as
well
as
a
zipped
dataset
to
a
github
repository
feel
free
to
clone
and
fork
lastly
follow
me
on:
twitter
|
linkedin
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ceo
@
statworx
doing
data
science
stats
and
ml
for
over
a
decade
food
wine
and
cocktail
enthusiast
check
our
website:
https:wwwstatworxcom
highlights
from
machine
learning
research
projects
and
learning
materials
from
and
for
ml
scientists
engineers
an
enthusiasts
""
by
ashok
chandrashekar
fernando
amat
justin
basilico
and
tony
jebara
for
many
years
the
main
goal
of
the
netflix
personalized
recommendation
system
has
been
to
get
the
right
titles
in
front
each
of
our
members
at
the
right
time
with
a
catalog
spanning
thousands
of
titles
and
a
diverse
member
base
spanning
over
a
hundred
million
accounts
recommending
the
titles
that
are
just
right
for
each
member
is
crucial
but
the
job
of
recommendation
does
not
end
there
why
should
you
care
about
any
particular
title
we
recommend?
what
can
we
say
about
a
new
and
unfamiliar
title
that
will
pique
your
interest?
how
do
we
convince
you
that
a
title
is
worth
watching?
answering
these
questions
is
critical
in
helping
our
members
discover
great
content
especially
for
unfamiliar
titles
one
avenue
to
address
this
challenge
is
to
consider
the
artwork
or
imagery
we
use
to
portray
the
titles
if
the
artwork
representing
a
title
captures
something
compelling
to
you
then
it
acts
as
a
gateway
into
that
title
and
gives
you
some
visual
evidence
for
why
the
title
might
be
good
for
you
the
artwork
may
highlight
an
actor
that
you
recognize
capture
an
exciting
moment
like
a
car
chase
or
contain
a
dramatic
scene
that
conveys
the
essence
of
a
movie
or
tv
show
if
we
present
that
perfect
image
on
your
homepage
and
as
they
say:
an
image
is
worth
a
thousand
words
then
maybe
just
maybe
you
will
give
it
a
try
this
is
yet
another
way
netflix
differs
from
traditional
media
offerings:
we
don’t
have
one
product
but
over
a
100
million
different
products
with
one
for
each
of
our
members
with
personalized
recommendations
and
personalized
visuals
in
previous
work
we
discussed
an
effort
to
find
the
single
perfect
artwork
for
each
title
across
all
our
members
through
multi-armed
bandit
algorithms
we
hunted
for
the
best
artwork
for
a
title
say
stranger
things
that
would
earn
the
most
plays
from
the
largest
fraction
of
our
members
however
given
the
enormous
diversity
in
taste
and
preferences
wouldn’t
it
be
better
if
we
could
find
the
best
artwork
for
each
of
our
members
to
highlight
the
aspects
of
a
title
that
are
specifically
relevant
to
them?
as
inspiration
let
us
explore
scenarios
where
personalization
of
artwork
would
be
meaningful
consider
the
following
examples
where
different
members
have
different
viewing
histories
on
the
left
are
three
titles
a
member
watched
in
the
past
to
the
right
of
the
arrow
is
the
artwork
that
a
member
would
get
for
a
particular
movie
that
we
recommend
for
them
let
us
consider
trying
to
personalize
the
image
we
use
to
depict
the
movie
good
will
hunting
here
we
might
personalize
this
decision
based
on
how
much
a
member
prefers
different
genres
and
themes
someone
who
has
watched
many
romantic
movies
may
be
interested
in
good
will
hunting
if
we
show
the
artwork
containing
matt
damon
and
minnie
driver
whereas
a
member
who
has
watched
many
comedies
might
be
drawn
to
the
movie
if
we
use
the
artwork
containing
robin
williams
a
well-known
comedian
in
another
scenario
let’s
imagine
how
the
different
preferences
for
cast
members
might
influence
the
personalization
of
the
artwork
for
the
movie
pulp
fiction
a
member
who
watches
many
movies
featuring
uma
thurman
would
likely
respond
positively
to
the
artwork
for
pulp
fiction
that
contains
uma
meanwhile
a
fan
of
john
travolta
may
be
more
interested
in
watching
pulp
fiction
if
the
artwork
features
john
of
course
not
all
the
scenarios
for
personalizing
artwork
are
this
clear
and
obvious
so
we
don’t
enumerate
such
hand-derived
rules
but
instead
rely
on
the
data
to
tell
us
what
signals
to
use
overall
by
personalizing
artwork
we
help
each
title
put
its
best
foot
forward
for
every
member
and
thus
improve
our
member
experience
at
netflix
we
embrace
personalization
and
algorithmically
adapt
many
aspects
of
our
member
experience
including
the
rows
we
select
for
the
homepage
the
titles
we
select
for
those
rows
the
galleries
we
display
the
messages
we
send
and
so
forth
each
new
aspect
that
we
personalize
has
unique
challenges
personalizing
the
artwork
we
display
is
no
exception
and
presents
different
personalization
challenges
one
challenge
of
image
personalization
is
that
we
can
only
select
a
single
piece
of
artwork
to
represent
each
title
in
each
place
we
present
it
in
contrast
typical
recommendation
settings
let
us
present
multiple
selections
to
a
member
where
we
can
subsequently
learn
about
their
preferences
from
the
item
a
member
selects
this
means
that
image
selection
is
a
chicken-and-egg
problem
operating
in
a
closed
loop:
if
a
member
plays
a
title
it
can
only
come
from
the
image
that
we
decided
to
present
to
that
member
what
we
seek
to
understand
is
when
presenting
a
specific
piece
of
artwork
for
a
title
influenced
a
member
to
play
or
not
to
play
a
title
and
when
a
member
would
have
played
a
title
or
not
regardless
of
which
image
we
presented
therefore
artwork
personalization
sits
on
top
of
the
traditional
recommendation
problem
and
the
algorithms
need
to
work
in
conjunction
with
each
other
of
course
to
properly
learn
how
to
personalize
artwork
we
need
to
collect
a
lot
of
data
to
find
signals
that
indicate
when
one
piece
of
artwork
is
significantly
better
for
a
member
another
challenge
is
to
understand
the
impact
of
changing
artwork
that
we
show
a
member
for
a
title
between
sessions
does
changing
artwork
reduce
recognizability
of
the
title
and
make
it
difficult
to
visually
locate
the
title
again
for
example
if
the
member
thought
was
interested
before
but
had
not
yet
watched
it?
or
does
changing
the
artwork
itself
lead
the
member
to
reconsider
it
due
to
an
improved
selection?
clearly
if
we
find
better
artwork
to
present
to
a
member
we
should
probably
use
it
but
continuous
changes
can
also
confuse
people
changing
images
also
introduces
an
attribution
problem
as
it
becomes
unclear
which
image
led
a
member
to
be
interested
in
a
title
next
there
is
the
challenge
of
understanding
how
artwork
performs
in
relation
to
other
artwork
we
select
in
the
same
page
or
session
maybe
a
bold
close-up
of
the
main
character
works
for
a
title
on
a
page
because
it
stands
out
compared
to
the
other
artwork
but
if
every
title
had
a
similar
image
then
the
page
as
a
whole
may
not
seem
as
compelling
looking
at
each
piece
of
artwork
in
isolation
may
not
be
enough
and
we
need
to
think
about
how
to
select
a
diverse
set
of
images
across
titles
on
a
page
and
across
a
session
beyond
the
artwork
for
other
titles
the
effectiveness
of
the
artwork
for
a
title
may
depend
on
what
other
types
of
evidence
and
assets
eg
synopses
trailers
etc
we
also
display
for
that
title
thus
we
may
need
a
diverse
selection
where
each
can
highlight
complementary
aspects
of
a
title
that
may
be
compelling
to
a
member
to
achieve
effective
personalization
we
also
need
a
good
pool
of
artwork
for
each
title
this
means
that
we
need
several
assets
where
each
is
engaging
informative
and
representative
of
a
title
to
avoid
clickbait
the
set
of
images
for
a
title
also
needs
to
be
diverse
enough
to
cover
a
wide
potential
audience
interested
in
different
aspects
of
the
content
after
all
how
engaging
and
informative
a
piece
of
artwork
is
truly
depends
on
the
individual
seeing
it
therefore
we
need
to
have
artwork
that
highlights
not
only
different
themes
in
a
title
but
also
different
aesthetics
our
teams
of
artists
and
designers
strive
to
create
images
that
are
diverse
across
many
dimensions
they
also
take
into
consideration
the
personalization
algorithms
which
will
select
the
images
during
their
creative
process
for
generating
artwork
finally
there
are
engineering
challenges
to
personalize
artwork
at
scale
one
challenge
is
that
our
member
experience
is
very
visual
and
thus
contains
a
lot
of
imagery
so
using
personalized
selection
for
each
asset
means
handling
a
peak
of
over
20
million
requests
per
second
with
low
latency
such
a
system
must
be
robust:
failing
to
properly
render
the
artwork
in
our
ui
brings
a
significantly
degrades
the
experience
our
personalization
algorithm
also
needs
to
respond
quickly
when
a
title
launches
which
means
rapidly
learning
to
personalize
in
a
cold-start
situation
then
after
launch
the
algorithm
must
continuously
adapt
as
the
effectiveness
of
artwork
may
change
over
time
as
both
the
title
evolves
through
its
life
cycle
and
member
tastes
evolve
much
of
the
netflix
recommendation
engine
is
powered
by
machine
learning
algorithms
traditionally
we
collect
a
batch
of
data
on
how
our
members
use
the
service
then
we
run
a
new
machine
learning
algorithm
on
this
batch
of
data
next
we
test
this
new
algorithm
against
the
current
production
system
through
an
ab
test
an
ab
test
helps
us
see
if
the
new
algorithm
is
better
than
our
current
production
system
by
trying
it
out
on
a
random
subset
of
members
members
in
group
a
get
the
current
production
experience
while
members
in
group
b
get
the
new
algorithm
if
members
in
group
b
have
higher
engagement
with
netflix
then
we
roll-out
the
new
algorithm
to
the
entire
member
population
unfortunately
this
batch
approach
incurs
regret:
many
members
over
a
long
period
of
time
did
not
benefit
from
the
better
experience
this
is
illustrated
in
the
figure
below
to
reduce
this
regret
we
move
away
from
batch
machine
learning
and
consider
online
machine
learning
for
artwork
personalization
the
specific
online
learning
framework
we
use
is
contextual
bandits
rather
than
waiting
to
collect
a
full
batch
of
data
waiting
to
learn
a
model
and
then
waiting
for
an
ab
test
to
conclude
contextual
bandits
rapidly
figure
out
the
optimal
personalized
artwork
selection
for
a
title
for
each
member
and
context
briefly
contextual
bandits
are
a
class
of
online
learning
algorithms
that
trade
off
the
cost
of
gathering
training
data
required
for
learning
an
unbiased
model
on
an
ongoing
basis
with
the
benefits
of
applying
the
learned
model
to
each
member
context
in
our
previous
unpersonalized
image
selection
work
we
used
non-contextual
bandits
where
we
found
the
winning
image
regardless
of
the
context
for
personalization
the
member
is
the
context
as
we
expect
different
members
to
respond
differently
to
the
images
a
key
property
of
contextual
bandits
is
that
they
are
designed
to
minimize
regret
at
a
high
level
the
training
data
for
a
contextual
bandit
is
obtained
through
the
injection
of
controlled
randomization
in
the
learned
model’s
predictions
the
randomization
schemes
can
vary
in
complexity
from
simple
epsilon-greedy
formulations
with
uniform
randomness
to
closed
loop
schemes
that
adaptively
vary
the
degree
of
randomization
as
a
function
of
model
uncertainty
we
broadly
refer
to
this
process
as
data
exploration
the
number
of
candidate
artworks
that
are
available
for
a
title
along
with
the
size
of
the
overall
population
for
which
the
system
will
be
deployed
informs
the
choice
of
the
data
exploration
strategy
with
such
exploration
we
need
to
log
information
about
the
randomization
for
each
artwork
selection
this
logging
allows
us
to
correct
for
skewed
selection
propensities
and
thereby
perform
offline
model
evaluation
in
an
unbiased
fashion
as
described
later
exploration
in
contextual
bandits
typically
has
a
cost
or
regret
due
to
the
fact
that
our
artwork
selection
in
a
member
session
may
not
use
the
predicted
best
image
for
that
session
what
impact
does
this
randomization
have
on
the
member
experience
and
consequently
on
our
metrics?
with
over
a
hundred
millions
members
the
regret
incurred
by
exploration
is
typically
very
small
and
is
amortized
across
our
large
member
base
with
each
member
implicitly
helping
provide
feedback
on
artwork
for
a
small
portion
of
the
catalog
this
makes
the
cost
of
exploration
per
member
negligible
which
is
an
important
consideration
when
choosing
contextual
bandits
to
drive
a
key
aspect
of
our
member
experience
randomization
and
exploration
with
contextual
bandits
would
be
less
suitable
if
the
cost
of
exploration
were
high
under
our
online
exploration
scheme
we
obtain
a
training
dataset
that
records
for
each
member
title
image
tuple
whether
that
selection
resulted
in
a
play
of
the
title
or
not
furthermore
we
can
control
the
exploration
such
that
artwork
selections
do
not
change
too
often
this
gives
a
cleaner
attribution
of
the
member’s
engagement
to
specific
artwork
we
also
carefully
determine
the
label
for
each
observation
by
looking
at
the
quality
of
engagement
to
avoid
learning
a
model
that
recommends
clickbait
images:
ones
that
entice
a
member
to
start
playing
but
ultimately
result
in
low-quality
engagement
in
this
online
learning
setting
we
train
our
contextual
bandit
model
to
select
the
best
artwork
for
each
member
based
on
their
context
we
typically
have
up
to
a
few
dozen
candidate
artwork
images
per
title
to
learn
the
selection
model
we
can
consider
a
simplification
of
the
problem
by
ranking
images
for
a
member
independently
across
titles
even
with
this
simplification
we
can
still
learn
member
image
preferences
across
titles
because
for
every
image
candidate
we
have
some
members
who
were
presented
with
it
and
engaged
with
the
title
and
some
members
who
were
presented
with
it
and
did
not
engage
these
preferences
can
be
modeled
to
predict
for
each
member
title
image
tuple
the
probability
that
the
member
will
enjoy
a
quality
engagement
these
can
be
supervised
learning
models
or
contextual
bandit
counterparts
with
thompson
sampling
linucb
or
bayesian
methods
that
intelligently
balance
making
the
best
prediction
with
data
exploration
in
contextual
bandits
the
context
is
usually
represented
as
an
feature
vector
provided
as
input
to
the
model
there
are
many
signals
we
can
use
as
features
for
this
problem
in
particular
we
can
consider
many
attributes
of
the
member:
the
titles
they’ve
played
the
genre
of
the
titles
interactions
of
the
member
with
the
specific
title
their
country
their
language
preferences
the
device
that
the
member
is
using
the
time
of
day
and
the
day
of
week
since
our
algorithm
selects
images
in
conjunction
with
our
personalized
recommendation
engine
we
can
also
use
signals
regarding
what
our
various
recommendation
algorithms
think
of
the
title
irrespective
of
what
image
is
used
to
represent
it
an
important
consideration
is
that
some
images
are
naturally
better
than
others
in
the
candidate
pool
we
observe
the
overall
take
rates
for
all
the
images
in
our
data
exploration
which
is
simply
the
number
of
quality
plays
divided
by
the
number
of
impressions
our
previous
work
on
unpersonalized
artwork
selection
used
overall
differences
in
take
rates
to
determine
the
single
best
image
to
select
for
a
whole
population
in
our
new
contextual
personalized
model
the
overall
take
rates
are
still
important
and
personalization
still
recovers
selections
that
agree
on
average
with
the
unpersonalized
model’s
ranking
the
optimal
assignment
of
image
artwork
to
a
member
is
a
selection
problem
to
find
the
best
candidate
image
from
a
title’s
pool
of
available
images
once
the
model
is
trained
as
above
we
use
it
to
rank
the
images
for
each
context
the
model
predicts
the
probability
of
play
for
a
given
image
in
a
given
a
member
context
we
sort
a
candidate
set
of
images
by
these
probabilities
and
pick
the
one
with
the
highest
probability
that
is
the
image
we
present
to
that
particular
member
to
evaluate
our
contextual
bandit
algorithms
prior
to
deploying
them
online
on
real
members
we
can
use
an
offline
technique
known
as
replay
[1]
this
method
allows
us
to
answer
counterfactual
questions
based
on
the
logged
exploration
data
figure
1
in
other
words
we
can
compare
offline
what
would
have
happened
in
historical
sessions
under
different
scenarios
if
we
had
used
different
algorithms
in
an
unbiased
way
replay
allows
us
to
see
how
members
would
have
engaged
with
our
titles
if
we
had
hypothetically
presented
images
that
were
selected
through
a
new
algorithm
rather
than
the
algorithm
used
in
production
for
images
we
are
interested
in
several
metrics
particularly
the
take
fraction
as
described
above
figure
2
shows
how
contextual
bandit
approach
helps
increase
the
average
take
fraction
across
the
catalog
compared
to
random
selection
or
non-contextual
bandits
after
experimenting
with
many
different
models
offline
and
finding
ones
that
had
a
substantial
increase
in
replay
we
ultimately
ran
an
ab
test
to
compare
the
most
promising
personalized
contextual
bandits
against
unpersonalized
bandits
as
we
suspected
the
personalization
worked
and
generated
a
significant
lift
in
our
core
metrics
we
also
saw
a
reasonable
correlation
between
what
we
measured
offline
in
replay
and
what
we
saw
online
with
the
models
the
online
results
also
produced
some
interesting
insights
for
example
the
improvement
of
personalization
was
larger
in
cases
where
the
member
had
no
prior
interaction
with
the
title
this
makes
sense
because
we
would
expect
that
the
artwork
would
be
more
important
to
someone
when
a
title
is
less
familiar
with
this
approach
we’ve
taken
our
first
steps
in
personalizing
the
selection
of
artwork
for
our
recommendations
and
across
our
service
this
has
resulted
in
a
meaningful
improvement
in
how
our
members
discover
new
content
so
we’ve
rolled
it
out
to
everyone!
this
project
is
the
first
instance
of
personalizing
not
just
what
we
recommend
but
also
how
we
recommend
to
our
members
but
there
are
many
opportunities
to
expand
and
improve
this
initial
approach
these
opportunities
include
developing
algorithms
to
handle
cold-start
by
personalizing
new
images
and
new
titles
as
quickly
as
possible
for
example
by
using
techniques
from
computer
vision
another
opportunity
is
extending
this
personalization
approach
across
other
types
of
artwork
we
use
and
other
evidence
that
describe
our
titles
such
as
synopses
metadata
and
trailers
there
is
also
an
even
broader
problem:
helping
artists
and
designers
figure
out
what
new
imagery
we
should
add
to
the
set
to
make
a
title
even
more
compelling
and
personalizable
if
these
types
of
challenges
interest
you
please
let
us
know!
we
are
always
looking
for
great
people
to
join
our
team
and
for
these
types
of
projects
we
are
especially
excited
by
candidates
with
machine
learning
andor
computer
vision
expertise
[1]
l
li
w
chu
j
langford
and
x
wang
unbiased
offline
evaluation
of
contextual-bandit-based
news
article
recommendation
algorithms
in
proceedings
of
the
fourth
acm
international
conference
on
web
search
and
data
mining
new
york
ny
usa
2011
pp
297–306
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
learn
more
about
how
netflix
designs
builds
and
operates
our
systems
and
engineering
organizations
learn
about
netflix’s
world
class
engineering
efforts
company
culture
product
developments
and
more
""
artificial
intelligence
ai
is
the
mantra
of
the
current
era
the
phrase
is
intoned
by
technologists
academicians
journalists
and
venture
capitalists
alike
as
with
many
phrases
that
cross
over
from
technical
academic
fields
into
general
circulation
there
is
significant
misunderstanding
accompanying
the
use
of
the
phrase
but
this
is
not
the
classical
case
of
the
public
not
understanding
the
scientists
—
here
the
scientists
are
often
as
befuddled
as
the
public
the
idea
that
our
era
is
somehow
seeing
the
emergence
of
an
intelligence
in
silicon
that
rivals
our
own
entertains
all
of
us
—
enthralling
us
and
frightening
us
in
equal
measure
and
unfortunately
it
distracts
us
there
is
a
different
narrative
that
one
can
tell
about
the
current
era
consider
the
following
story
which
involves
humans
computers
data
and
life-or-death
decisions
but
where
the
focus
is
something
other
than
intelligence-in-silicon
fantasies
when
my
spouse
was
pregnant
14
years
ago
we
had
an
ultrasound
there
was
a
geneticist
in
the
room
and
she
pointed
out
some
white
spots
around
the
heart
of
the
fetus
those
are
markers
for
down
syndrome
she
noted
and
your
risk
has
now
gone
up
to
1
in
20
she
further
let
us
know
that
we
could
learn
whether
the
fetus
in
fact
had
the
genetic
modification
underlying
down
syndrome
via
an
amniocentesis
but
amniocentesis
was
risky
—
the
risk
of
killing
the
fetus
during
the
procedure
was
roughly
1
in
300
being
a
statistician
i
determined
to
find
out
where
these
numbers
were
coming
from
to
cut
a
long
story
short
i
discovered
that
a
statistical
analysis
had
been
done
a
decade
previously
in
the
uk
where
these
white
spots
which
reflect
calcium
buildup
were
indeed
established
as
a
predictor
of
down
syndrome
but
i
also
noticed
that
the
imaging
machine
used
in
our
test
had
a
few
hundred
more
pixels
per
square
inch
than
the
machine
used
in
the
uk
study
i
went
back
to
tell
the
geneticist
that
i
believed
that
the
white
spots
were
likely
false
positives
—
that
they
were
literally
white
noise
she
said
ah
that
explains
why
we
started
seeing
an
uptick
in
down
syndrome
diagnoses
a
few
years
ago
it’s
when
the
new
machine
arrived
we
didn’t
do
the
amniocentesis
and
a
healthy
girl
was
born
a
few
months
later
but
the
episode
troubled
me
particularly
after
a
back-of-the-envelope
calculation
convinced
me
that
many
thousands
of
people
had
gotten
that
diagnosis
that
same
day
worldwide
that
many
of
them
had
opted
for
amniocentesis
and
that
a
number
of
babies
had
died
needlessly
and
this
happened
day
after
day
until
it
somehow
got
fixed
the
problem
that
this
episode
revealed
wasn’t
about
my
individual
medical
care
it
was
about
a
medical
system
that
measured
variables
and
outcomes
in
various
places
and
times
conducted
statistical
analyses
and
made
use
of
the
results
in
other
places
and
times
the
problem
had
to
do
not
just
with
data
analysis
per
se
but
with
what
database
researchers
call
provenance
—
broadly
where
did
data
arise
what
inferences
were
drawn
from
the
data
and
how
relevant
are
those
inferences
to
the
present
situation?
while
a
trained
human
might
be
able
to
work
all
of
this
out
on
a
case-by-case
basis
the
issue
was
that
of
designing
a
planetary-scale
medical
system
that
could
do
this
without
the
need
for
such
detailed
human
oversight
i’m
also
a
computer
scientist
and
it
occurred
to
me
that
the
principles
needed
to
build
planetary-scale
inference-and-decision-making
systems
of
this
kind
blending
computer
science
with
statistics
and
taking
into
account
human
utilities
were
nowhere
to
be
found
in
my
education
and
it
occurred
to
me
that
the
development
of
such
principles
—
which
will
be
needed
not
only
in
the
medical
domain
but
also
in
domains
such
as
commerce
transportation
and
education
—
were
at
least
as
important
as
those
of
building
ai
systems
that
can
dazzle
us
with
their
game-playing
or
sensorimotor
skills
whether
or
not
we
come
to
understand
intelligence
any
time
soon
we
do
have
a
major
challenge
on
our
hands
in
bringing
together
computers
and
humans
in
ways
that
enhance
human
life
while
this
challenge
is
viewed
by
some
as
subservient
to
the
creation
of
artificial
intelligence
it
can
also
be
viewed
more
prosaically
—
but
with
no
less
reverence
—
as
the
creation
of
a
new
branch
of
engineering
much
like
civil
engineering
and
chemical
engineering
in
decades
past
this
new
discipline
aims
to
corral
the
power
of
a
few
key
ideas
bringing
new
resources
and
capabilities
to
people
and
doing
so
safely
whereas
civil
engineering
and
chemical
engineering
were
built
on
physics
and
chemistry
this
new
engineering
discipline
will
be
built
on
ideas
that
the
preceding
century
gave
substance
to
—
ideas
such
as
information
algorithm
data
uncertainty
computing
inference
and
optimization
moreover
since
much
of
the
focus
of
the
new
discipline
will
be
on
data
from
and
about
humans
its
development
will
require
perspectives
from
the
social
sciences
and
humanities
while
the
building
blocks
have
begun
to
emerge
the
principles
for
putting
these
blocks
together
have
not
yet
emerged
and
so
the
blocks
are
currently
being
put
together
in
ad-hoc
ways
thus
just
as
humans
built
buildings
and
bridges
before
there
was
civil
engineering
humans
are
proceeding
with
the
building
of
societal-scale
inference-and-decision-making
systems
that
involve
machines
humans
and
the
environment
just
as
early
buildings
and
bridges
sometimes
fell
to
the
ground
—
in
unforeseen
ways
and
with
tragic
consequences
—
many
of
our
early
societal-scale
inference-and-decision-making
systems
are
already
exposing
serious
conceptual
flaws
and
unfortunately
we
are
not
very
good
at
anticipating
what
the
next
emerging
serious
flaw
will
be
what
we’re
missing
is
an
engineering
discipline
with
its
principles
of
analysis
and
design
the
current
public
dialog
about
these
issues
too
often
uses
ai
as
an
intellectual
wildcard
one
that
makes
it
difficult
to
reason
about
the
scope
and
consequences
of
emerging
technology
let
us
begin
by
considering
more
carefully
what
ai
has
been
used
to
refer
to
both
recently
and
historically
most
of
what
is
being
called
ai
today
particularly
in
the
public
sphere
is
what
has
been
called
machine
learning
ml
for
the
past
several
decades
ml
is
an
algorithmic
field
that
blends
ideas
from
statistics
computer
science
and
many
other
disciplines
see
below
to
design
algorithms
that
process
data
make
predictions
and
help
make
decisions
in
terms
of
impact
on
the
real
world
ml
is
the
real
thing
and
not
just
recently
indeed
that
ml
would
grow
into
massive
industrial
relevance
was
already
clear
in
the
early
1990s
and
by
the
turn
of
the
century
forward-looking
companies
such
as
amazon
were
already
using
ml
throughout
their
business
solving
mission-critical
back-end
problems
in
fraud
detection
and
supply-chain
prediction
and
building
innovative
consumer-facing
services
such
as
recommendation
systems
as
datasets
and
computing
resources
grew
rapidly
over
the
ensuing
two
decades
it
became
clear
that
ml
would
soon
power
not
only
amazon
but
essentially
any
company
in
which
decisions
could
be
tied
to
large-scale
data
new
business
models
would
emerge
the
phrase
data
science
began
to
be
used
to
refer
to
this
phenomenon
reflecting
the
need
of
ml
algorithms
experts
to
partner
with
database
and
distributed-systems
experts
to
build
scalable
robust
ml
systems
and
reflecting
the
larger
social
and
environmental
scope
of
the
resulting
systems
this
confluence
of
ideas
and
technology
trends
has
been
rebranded
as
ai
over
the
past
few
years
this
rebranding
is
worthy
of
some
scrutiny
historically
the
phrase
ai
was
coined
in
the
late
1950’s
to
refer
to
the
heady
aspiration
of
realizing
in
software
and
hardware
an
entity
possessing
human-level
intelligence
we
will
use
the
phrase
human-imitative
ai
to
refer
to
this
aspiration
emphasizing
the
notion
that
the
artificially
intelligent
entity
should
seem
to
be
one
of
us
if
not
physically
at
least
mentally
whatever
that
might
mean
this
was
largely
an
academic
enterprise
while
related
academic
fields
such
as
operations
research
statistics
pattern
recognition
information
theory
and
control
theory
already
existed
and
were
often
inspired
by
human
intelligence
and
animal
intelligence
these
fields
were
arguably
focused
on
low-level
signals
and
decisions
the
ability
of
say
a
squirrel
to
perceive
the
three-dimensional
structure
of
the
forest
it
lives
in
and
to
leap
among
its
branches
was
inspirational
to
these
fields
ai
was
meant
to
focus
on
something
different
—
the
high-level
or
cognitive
capability
of
humans
to
reason
and
to
think
sixty
years
later
however
high-level
reasoning
and
thought
remain
elusive
the
developments
which
are
now
being
called
ai
arose
mostly
in
the
engineering
fields
associated
with
low-level
pattern
recognition
and
movement
control
and
in
the
field
of
statistics
—
the
discipline
focused
on
finding
patterns
in
data
and
on
making
well-founded
predictions
tests
of
hypotheses
and
decisions
indeed
the
famous
backpropagation
algorithm
that
was
rediscovered
by
david
rumelhart
in
the
early
1980s
and
which
is
now
viewed
as
being
at
the
core
of
the
so-called
ai
revolution
first
arose
in
the
field
of
control
theory
in
the
1950s
and
1960s
one
of
its
early
applications
was
to
optimize
the
thrusts
of
the
apollo
spaceships
as
they
headed
towards
the
moon
since
the
1960s
much
progress
has
been
made
but
it
has
arguably
not
come
about
from
the
pursuit
of
human-imitative
ai
rather
as
in
the
case
of
the
apollo
spaceships
these
ideas
have
often
been
hidden
behind
the
scenes
and
have
been
the
handiwork
of
researchers
focused
on
specific
engineering
challenges
although
not
visible
to
the
general
public
research
and
systems-building
in
areas
such
as
document
retrieval
text
classification
fraud
detection
recommendation
systems
personalized
search
social
network
analysis
planning
diagnostics
and
ab
testing
have
been
a
major
success
—
these
are
the
advances
that
have
powered
companies
such
as
google
netflix
facebook
and
amazon
one
could
simply
agree
to
refer
to
all
of
this
as
ai
and
indeed
that
is
what
appears
to
have
happened
such
labeling
may
come
as
a
surprise
to
optimization
or
statistics
researchers
who
wake
up
to
find
themselves
suddenly
referred
to
as
ai
researchers
but
labeling
of
researchers
aside
the
bigger
problem
is
that
the
use
of
this
single
ill-defined
acronym
prevents
a
clear
understanding
of
the
range
of
intellectual
and
commercial
issues
at
play
the
past
two
decades
have
seen
major
progress
—
in
industry
and
academia
—
in
a
complementary
aspiration
to
human-imitative
ai
that
is
often
referred
to
as
intelligence
augmentation
ia
here
computation
and
data
are
used
to
create
services
that
augment
human
intelligence
and
creativity
a
search
engine
can
be
viewed
as
an
example
of
ia
it
augments
human
memory
and
factual
knowledge
as
can
natural
language
translation
it
augments
the
ability
of
a
human
to
communicate
computing-based
generation
of
sounds
and
images
serves
as
a
palette
and
creativity
enhancer
for
artists
while
services
of
this
kind
could
conceivably
involve
high-level
reasoning
and
thought
currently
they
don’t
—
they
mostly
perform
various
kinds
of
string-matching
and
numerical
operations
that
capture
patterns
that
humans
can
make
use
of
hoping
that
the
reader
will
tolerate
one
last
acronym
let
us
conceive
broadly
of
a
discipline
of
intelligent
infrastructure
ii
whereby
a
web
of
computation
data
and
physical
entities
exists
that
makes
human
environments
more
supportive
interesting
and
safe
such
infrastructure
is
beginning
to
make
its
appearance
in
domains
such
as
transportation
medicine
commerce
and
finance
with
vast
implications
for
individual
humans
and
societies
this
emergence
sometimes
arises
in
conversations
about
an
internet
of
things
but
that
effort
generally
refers
to
the
mere
problem
of
getting
things
onto
the
internet
—
not
to
the
far
grander
set
of
challenges
associated
with
these
things
capable
of
analyzing
those
data
streams
to
discover
facts
about
the
world
and
interacting
with
humans
and
other
things
at
a
far
higher
level
of
abstraction
than
mere
bits
for
example
returning
to
my
personal
anecdote
we
might
imagine
living
our
lives
in
a
societal-scale
medical
system
that
sets
up
data
flows
and
data-analysis
flows
between
doctors
and
devices
positioned
in
and
around
human
bodies
thereby
able
to
aid
human
intelligence
in
making
diagnoses
and
providing
care
the
system
would
incorporate
information
from
cells
in
the
body
dna
blood
tests
environment
population
genetics
and
the
vast
scientific
literature
on
drugs
and
treatments
it
would
not
just
focus
on
a
single
patient
and
a
doctor
but
on
relationships
among
all
humans
—
just
as
current
medical
testing
allows
experiments
done
on
one
set
of
humans
or
animals
to
be
brought
to
bear
in
the
care
of
other
humans
it
would
help
maintain
notions
of
relevance
provenance
and
reliability
in
the
way
that
the
current
banking
system
focuses
on
such
challenges
in
the
domain
of
finance
and
payment
and
while
one
can
foresee
many
problems
arising
in
such
a
system
—
involving
privacy
issues
liability
issues
security
issues
etc
—
these
problems
should
properly
be
viewed
as
challenges
not
show-stoppers
we
now
come
to
a
critical
issue:
is
working
on
classical
human-imitative
ai
the
best
or
only
way
to
focus
on
these
larger
challenges?
some
of
the
most
heralded
recent
success
stories
of
ml
have
in
fact
been
in
areas
associated
with
human-imitative
ai
—
areas
such
as
computer
vision
speech
recognition
game-playing
and
robotics
so
perhaps
we
should
simply
await
further
progress
in
domains
such
as
these
there
are
two
points
to
make
here
first
although
one
would
not
know
it
from
reading
the
newspapers
success
in
human-imitative
ai
has
in
fact
been
limited
—
we
are
very
far
from
realizing
human-imitative
ai
aspirations
unfortunately
the
thrill
and
fear
of
making
even
limited
progress
on
human-imitative
ai
gives
rise
to
levels
of
over-exuberance
and
media
attention
that
is
not
present
in
other
areas
of
engineering
second
and
more
importantly
success
in
these
domains
is
neither
sufficient
nor
necessary
to
solve
important
ia
and
ii
problems
on
the
sufficiency
side
consider
self-driving
cars
for
such
technology
to
be
realized
a
range
of
engineering
problems
will
need
to
be
solved
that
may
have
little
relationship
to
human
competencies
or
human
lack-of-competencies
the
overall
transportation
system
an
ii
system
will
likely
more
closely
resemble
the
current
air-traffic
control
system
than
the
current
collection
of
loosely-coupled
forward-facing
inattentive
human
drivers
it
will
be
vastly
more
complex
than
the
current
air-traffic
control
system
specifically
in
its
use
of
massive
amounts
of
data
and
adaptive
statistical
modeling
to
inform
fine-grained
decisions
it
is
those
challenges
that
need
to
be
in
the
forefront
and
in
such
an
effort
a
focus
on
human-imitative
ai
may
be
a
distraction
as
for
the
necessity
argument
it
is
sometimes
argued
that
the
human-imitative
ai
aspiration
subsumes
ia
and
ii
aspirations
because
a
human-imitative
ai
system
would
not
only
be
able
to
solve
the
classical
problems
of
ai
as
embodied
eg
in
the
turing
test
but
it
would
also
be
our
best
bet
for
solving
ia
and
ii
problems
such
an
argument
has
little
historical
precedent
did
civil
engineering
develop
by
envisaging
the
creation
of
an
artificial
carpenter
or
bricklayer?
should
chemical
engineering
have
been
framed
in
terms
of
creating
an
artificial
chemist?
even
more
polemically:
if
our
goal
was
to
build
chemical
factories
should
we
have
first
created
an
artificial
chemist
who
would
have
then
worked
out
how
to
build
a
chemical
factory?
a
related
argument
is
that
human
intelligence
is
the
only
kind
of
intelligence
that
we
know
and
that
we
should
aim
to
mimic
it
as
a
first
step
but
humans
are
in
fact
not
very
good
at
some
kinds
of
reasoning
—
we
have
our
lapses
biases
and
limitations
moreover
critically
we
did
not
evolve
to
perform
the
kinds
of
large-scale
decision-making
that
modern
ii
systems
must
face
nor
to
cope
with
the
kinds
of
uncertainty
that
arise
in
ii
contexts
one
could
argue
that
an
ai
system
would
not
only
imitate
human
intelligence
but
also
correct
it
and
would
also
scale
to
arbitrarily
large
problems
but
we
are
now
in
the
realm
of
science
fiction
—
such
speculative
arguments
while
entertaining
in
the
setting
of
fiction
should
not
be
our
principal
strategy
going
forward
in
the
face
of
the
critical
ia
and
ii
problems
that
are
beginning
to
emerge
we
need
to
solve
ia
and
ii
problems
on
their
own
merits
not
as
a
mere
corollary
to
a
human-imitative
ai
agenda
it
is
not
hard
to
pinpoint
algorithmic
and
infrastructure
challenges
in
ii
systems
that
are
not
central
themes
in
human-imitative
ai
research
ii
systems
require
the
ability
to
manage
distributed
repositories
of
knowledge
that
are
rapidly
changing
and
are
likely
to
be
globally
incoherent
such
systems
must
cope
with
cloud-edge
interactions
in
making
timely
distributed
decisions
and
they
must
deal
with
long-tail
phenomena
whereby
there
is
lots
of
data
on
some
individuals
and
little
data
on
most
individuals
they
must
address
the
difficulties
of
sharing
data
across
administrative
and
competitive
boundaries
finally
and
of
particular
importance
ii
systems
must
bring
economic
ideas
such
as
incentives
and
pricing
into
the
realm
of
the
statistical
and
computational
infrastructures
that
link
humans
to
each
other
and
to
valued
goods
such
ii
systems
can
be
viewed
as
not
merely
providing
a
service
but
as
creating
markets
there
are
domains
such
as
music
literature
and
journalism
that
are
crying
out
for
the
emergence
of
such
markets
where
data
analysis
links
producers
and
consumers
and
this
must
all
be
done
within
the
context
of
evolving
societal
ethical
and
legal
norms
of
course
classical
human-imitative
ai
problems
remain
of
great
interest
as
well
however
the
current
focus
on
doing
ai
research
via
the
gathering
of
data
the
deployment
of
deep
learning
infrastructure
and
the
demonstration
of
systems
that
mimic
certain
narrowly-defined
human
skills
—
with
little
in
the
way
of
emerging
explanatory
principles
—
tends
to
deflect
attention
from
major
open
problems
in
classical
ai
these
problems
include
the
need
to
bring
meaning
and
reasoning
into
systems
that
perform
natural
language
processing
the
need
to
infer
and
represent
causality
the
need
to
develop
computationally-tractable
representations
of
uncertainty
and
the
need
to
develop
systems
that
formulate
and
pursue
long-term
goals
these
are
classical
goals
in
human-imitative
ai
but
in
the
current
hubbub
over
the
ai
revolution
it
is
easy
to
forget
that
they
are
not
yet
solved
ia
will
also
remain
quite
essential
because
for
the
foreseeable
future
computers
will
not
be
able
to
match
humans
in
their
ability
to
reason
abstractly
about
real-world
situations
we
will
need
well-thought-out
interactions
of
humans
and
computers
to
solve
our
most
pressing
problems
and
we
will
want
computers
to
trigger
new
levels
of
human
creativity
not
replace
human
creativity
whatever
that
might
mean
it
was
john
mccarthy
while
a
professor
at
dartmouth
and
soon
to
take
a
position
at
mit
who
coined
the
term
ai
apparently
to
distinguish
his
budding
research
agenda
from
that
of
norbert
wiener
then
an
older
professor
at
mit
wiener
had
coined
cybernetics
to
refer
to
his
own
vision
of
intelligent
systems
—
a
vision
that
was
closely
tied
to
operations
research
statistics
pattern
recognition
information
theory
and
control
theory
mccarthy
on
the
other
hand
emphasized
the
ties
to
logic
in
an
interesting
reversal
it
is
wiener’s
intellectual
agenda
that
has
come
to
dominate
in
the
current
era
under
the
banner
of
mccarthy’s
terminology
this
state
of
affairs
is
surely
however
only
temporary
the
pendulum
swings
more
in
ai
than
in
most
fields
but
we
need
to
move
beyond
the
particular
historical
perspectives
of
mccarthy
and
wiener
we
need
to
realize
that
the
current
public
dialog
on
ai
—
which
focuses
on
a
narrow
subset
of
industry
and
a
narrow
subset
of
academia
—
risks
blinding
us
to
the
challenges
and
opportunities
that
are
presented
by
the
full
scope
of
ai
ia
and
ii
this
scope
is
less
about
the
realization
of
science-fiction
dreams
or
nightmares
of
super-human
machines
and
more
about
the
need
for
humans
to
understand
and
shape
technology
as
it
becomes
ever
more
present
and
influential
in
their
daily
lives
moreover
in
this
understanding
and
shaping
there
is
a
need
for
a
diverse
set
of
voices
from
all
walks
of
life
not
merely
a
dialog
among
the
technologically
attuned
focusing
narrowly
on
human-imitative
ai
prevents
an
appropriately
wide
range
of
voices
from
being
heard
while
industry
will
continue
to
drive
many
developments
academia
will
also
continue
to
play
an
essential
role
not
only
in
providing
some
of
the
most
innovative
technical
ideas
but
also
in
bringing
researchers
from
the
computational
and
statistical
disciplines
together
with
researchers
from
other
disciplines
whose
contributions
and
perspectives
are
sorely
needed
—
notably
the
social
sciences
the
cognitive
sciences
and
the
humanities
on
the
other
hand
while
the
humanities
and
the
sciences
are
essential
as
we
go
forward
we
should
also
not
pretend
that
we
are
talking
about
something
other
than
an
engineering
effort
of
unprecedented
scale
and
scope
—
society
is
aiming
to
build
new
kinds
of
artifacts
these
artifacts
should
be
built
to
work
as
claimed
we
do
not
want
to
build
systems
that
help
us
with
medical
treatments
transportation
options
and
commercial
opportunities
to
find
out
after
the
fact
that
these
systems
don’t
really
work
—
that
they
make
errors
that
take
their
toll
in
terms
of
human
lives
and
happiness
in
this
regard
as
i
have
emphasized
there
is
an
engineering
discipline
yet
to
emerge
for
the
data-focused
and
learning-focused
fields
as
exciting
as
these
latter
fields
appear
to
be
they
cannot
yet
be
viewed
as
constituting
an
engineering
discipline
moreover
we
should
embrace
the
fact
that
what
we
are
witnessing
is
the
creation
of
a
new
branch
of
engineering
the
term
engineering
is
often
invoked
in
a
narrow
sense
—
in
academia
and
beyond
—
with
overtones
of
cold
affectless
machinery
and
negative
connotations
of
loss
of
control
by
humans
but
an
engineering
discipline
can
be
what
we
want
it
to
be
in
the
current
era
we
have
a
real
opportunity
to
conceive
of
something
historically
new
—
a
human-centric
engineering
discipline
i
will
resist
giving
this
emerging
discipline
a
name
but
if
the
acronym
ai
continues
to
be
used
as
placeholder
nomenclature
going
forward
let’s
be
aware
of
the
very
real
limitations
of
this
placeholder
let’s
broaden
our
scope
tone
down
the
hype
and
recognize
the
serious
challenges
ahead
michael
i
jordan
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
michael
i
jordan
is
a
professor
in
the
department
of
electrical
engineering
and
computer
sciences
and
the
department
of
statistics
at
uc
berkeley
""
by
blaise
agüera
y
arcas
alexander
todorov
and
margaret
mitchell
a
study
claiming
that
artificial
intelligence
can
infer
sexual
orientation
from
facial
images
caused
a
media
uproar
in
the
fall
of
2017
the
economist
featured
this
work
on
the
cover
of
their
september
9th
magazine
on
the
other
hand
two
major
lgbtq
organizations
the
human
rights
campaign
and
glaad
immediately
labeled
it
junk
science
michal
kosinski
who
co-authored
the
study
with
fellow
researcher
yilun
wang
initially
expressed
surprise
calling
the
critiques
knee-jerk
reactions
however
he
then
proceeded
to
make
even
bolder
claims:
that
such
ai
algorithms
will
soon
be
able
to
measure
the
intelligence
political
orientation
and
criminal
inclinations
of
people
from
their
facial
images
alone
kosinski’s
controversial
claims
are
nothing
new
last
year
two
computer
scientists
from
china
posted
a
non-peer-reviewed
paper
online
in
which
they
argued
that
their
ai
algorithm
correctly
categorizes
criminals
with
nearly
90%
accuracy
from
a
government
id
photo
alone
technology
startups
had
also
begun
to
crop
up
claiming
that
they
can
profile
people’s
character
from
their
facial
images
these
developments
had
prompted
the
three
of
us
to
collaborate
earlier
in
the
year
on
a
medium
essay
physiognomy’s
new
clothes
to
confront
claims
that
ai
face
recognition
reveals
deep
character
traits
we
described
how
the
junk
science
of
physiognomy
has
roots
going
back
into
antiquity
with
practitioners
in
every
era
resurrecting
beliefs
based
on
prejudice
using
the
new
methodology
of
the
age
in
the
19th
century
this
included
anthropology
and
psychology
in
the
20th
genetics
and
statistical
analysis
and
in
the
21st
artificial
intelligence
in
late
2016
the
paper
motivating
our
physiognomy
essay
seemed
well
outside
the
mainstream
in
tech
and
academia
but
as
in
other
areas
of
discourse
what
recently
felt
like
a
fringe
position
must
now
be
addressed
head
on
kosinski
is
a
faculty
member
of
stanford’s
graduate
school
of
business
and
this
new
study
has
been
accepted
for
publication
in
the
respected
journal
of
personality
and
social
psychology
much
of
the
ensuing
scrutiny
has
focused
on
ethics
implicitly
assuming
that
the
science
is
valid
we
will
focus
on
the
science
the
authors
trained
and
tested
their
sexual
orientation
detector
using
35326
images
from
public
profiles
on
a
us
dating
website
composite
images
of
the
lesbian
gay
and
straight
men
and
women
in
the
sample
reveal
a
great
deal
about
the
information
available
to
the
algorithm:
clearly
there
are
differences
between
these
four
composite
faces
wang
and
kosinski
assert
that
the
key
differences
are
in
physiognomy
meaning
that
a
sexual
orientation
tends
to
go
along
with
a
characteristic
facial
structure
however
we
can
immediately
see
that
some
of
these
differences
are
more
superficial
for
example
the
average
straight
woman
appears
to
wear
eyeshadow
while
the
average
lesbian
does
not
glasses
are
clearly
visible
on
the
gay
man
and
to
a
lesser
extent
on
the
lesbian
while
they
seem
absent
in
the
heterosexual
composites
might
it
be
the
case
that
the
algorithm’s
ability
to
detect
orientation
has
little
to
do
with
facial
structure
but
is
due
rather
to
patterns
in
grooming
presentation
and
lifestyle?
we
conducted
a
survey
of
8000
americans
using
amazon’s
mechanical
turk
crowdsourcing
platform
to
see
if
we
could
independently
confirm
these
patterns
asking
77
yesno
questions
such
as
do
you
wear
eyeshadow?
do
you
wear
glasses?
and
do
you
have
a
beard?
as
well
as
questions
about
gender
and
sexual
orientation
the
results
show
that
lesbians
indeed
use
eyeshadow
much
less
than
straight
women
do
gay
men
and
women
do
both
wear
glasses
more
and
young
opposite-sex-attracted
men
are
considerably
more
likely
to
have
prominent
facial
hair
than
their
gay
or
same-sex-attracted
peers
breaking
down
the
answers
by
the
age
of
the
respondent
can
provide
a
richer
and
clearer
view
of
the
data
than
any
single
statistic
in
the
following
figures
we
show
the
proportion
of
women
who
answer
yes
to
do
you
ever
use
makeup?
top
and
do
you
wear
eyeshadow?
bottom
averaged
over
6-year
age
intervals:
the
blue
curves
represent
strictly
opposite-sex
attracted
women
a
nearly
identical
set
to
those
who
answered
yes
to
are
you
heterosexual
or
straight?
the
cyan
curve
represents
women
who
answer
yes
to
either
or
both
of
are
you
sexually
attracted
to
women?
and
are
you
romantically
attracted
to
women?
and
the
red
curve
represents
women
who
answer
yes
to
are
you
homosexual
gay
or
lesbian?
[1]
the
shaded
regions
around
each
curve
show
68%
confidence
intervals
[2]
the
patterns
revealed
here
are
intuitive
it
won’t
be
breaking
news
to
most
that
straight
women
tend
to
wear
more
makeup
and
eyeshadow
than
same-sex
attracted
and
even
more
so
lesbian-identifying
women
on
the
other
hand
these
curves
also
show
us
how
often
these
stereotypes
are
violated
that
same-sex
attracted
men
of
most
ages
wear
glasses
significantly
more
than
exclusively
opposite-sex
attracted
men
do
might
be
a
bit
less
obvious
but
this
trend
is
equally
clear:
[3]
a
proponent
of
physiognomy
might
be
tempted
to
guess
that
this
is
somehow
related
to
differences
in
visual
acuity
between
these
populations
of
men
however
asking
the
question
do
you
like
how
you
look
in
glasses?
reveals
that
this
is
likely
more
of
a
stylistic
choice:
same-sex
attracted
women
also
report
wearing
glasses
more
as
well
as
liking
how
they
look
in
glasses
more
across
a
range
of
ages:
one
can
also
see
how
opposite-sex
attracted
women
under
the
age
of
40
wear
contact
lenses
significantly
more
than
same-sex
attracted
women
despite
reporting
that
they
have
a
vision
defect
at
roughly
the
same
rate
further
illustrating
how
the
difference
is
driven
by
an
aesthetic
preference:
[4]
similar
analysis
shows
that
young
same-sex
attracted
men
are
much
less
likely
to
have
hairy
faces
than
opposite-sex
attracted
men
serious
facial
hair
in
our
plots
is
defined
as
answering
yes
to
having
a
goatee
beard
or
moustache
but
no
to
stubble
overall
opposite-sex
attracted
men
in
our
sample
are
35%
more
likely
to
have
serious
facial
hair
than
same-sex
attracted
men
and
for
men
under
the
age
of
31
who
are
overrepresented
on
dating
websites
this
rises
to
75%
wang
and
kosinski
speculate
in
their
paper
that
the
faintness
of
the
beard
and
moustache
in
their
gay
male
composite
might
be
connected
with
prenatal
underexposure
to
androgens
male
hormones
resulting
in
a
feminizing
effect
hence
sparser
facial
hair
the
fact
that
we
see
a
cohort
of
same-sex
attracted
men
in
their
40s
who
have
just
as
much
facial
hair
as
opposite-sex
attracted
men
suggests
a
different
story
in
which
fashion
trends
and
cultural
norms
play
the
dominant
role
in
choices
about
facial
hair
among
men
not
differing
exposure
to
hormones
early
in
development
the
authors
of
the
paper
additionally
note
that
the
heterosexual
male
composite
appears
to
have
darker
skin
than
the
other
three
composites
our
survey
confirms
that
opposite-sex
attracted
men
consistently
self-report
having
a
tan
face
yes
to
is
your
face
tan?
slightly
more
often
than
same-sex
attracted
men:
once
again
wang
and
kosinski
reach
for
a
hormonal
explanation
writing:
while
the
brightness
of
the
facial
image
might
be
driven
by
many
factors
previous
research
found
that
testosterone
stimulates
melanocyte
structure
and
function
leading
to
a
darker
skin
however
a
simpler
answer
is
suggested
by
the
responses
to
the
question
do
you
work
outdoors?:
overall
opposite-sex
attracted
men
are
29%
more
likely
to
work
outdoors
and
among
men
under
31
this
rises
to
39%
previous
research
has
found
that
increased
exposure
to
sunlight
leads
to
darker
skin!
[5]
none
of
these
results
prove
that
there
is
no
physiological
basis
for
sexual
orientation
in
fact
ample
evidence
shows
us
that
orientation
runs
much
deeper
than
a
choice
or
a
lifestyle
in
a
critique
aimed
in
part
at
fraudulent
conversion
therapy
programs
united
states
surgeon
general
david
satcher
wrote
in
a
2001
report
sexual
orientation
is
usually
determined
by
adolescence
if
not
earlier
[]
and
there
is
no
valid
scientific
evidence
that
sexual
orientation
can
be
changed
it
follows
that
if
we
dig
deeply
enough
into
human
physiology
and
neuroscience
we
will
eventually
find
reliable
correlates
and
maybe
even
the
origins
of
sexual
orientation
in
our
survey
we
also
find
some
evidence
of
outwardly
visible
correlates
of
orientation
that
are
not
cultural:
perhaps
most
strikingly
very
tall
women
are
overrepresented
among
lesbian-identifying
respondents
[6]
however
while
this
is
interesting
it’s
very
far
from
a
good
predictor
of
women’s
sexual
orientation
makeup
and
eyeshadow
do
much
better
the
way
wang
and
kosinski
measure
the
efficacy
of
their
ai
gaydar
is
equivalent
to
choosing
a
straight
and
a
gay
or
lesbian
face
image
both
from
data
held
out
during
the
training
process
and
asking
how
often
the
algorithm
correctly
guesses
which
is
which
50%
performance
would
be
no
better
than
random
chance
for
women
guessing
that
the
taller
of
the
two
is
the
lesbian
achieves
only
51%
accuracy
—
barely
above
random
chance
this
is
because
despite
the
statistically
meaningful
overrepresentation
of
tall
women
among
the
lesbian
population
the
great
majority
of
lesbians
are
not
unusually
tall
by
contrast
the
performance
measures
in
the
paper
81%
for
gay
men
and
71%
for
lesbian
women
seem
impressive
[7]
consider
however
that
we
can
achieve
comparable
results
with
trivial
models
based
only
on
a
handful
of
yesno
survey
questions
about
presentation
for
example
for
pairs
of
women
one
of
whom
is
lesbian
the
following
not-exactly-superhuman
algorithm
is
on
average
63%
accurate:
if
neither
or
both
women
wear
eyeshadow
flip
a
coin
otherwise
guess
that
the
one
who
wears
eyeshadow
is
straight
and
the
other
lesbian
adding
six
more
yesno
questions
about
presentation
do
you
ever
use
makeup?
do
you
have
long
hair?
do
you
have
short
hair?
do
you
ever
use
colored
lipstick?
do
you
like
how
you
look
in
glasses?
and
do
you
work
outdoors?
as
additional
signals
raises
the
performance
to
70%
[8]
given
how
many
more
details
about
presentation
are
available
in
a
face
image
71%
performance
no
longer
seems
so
impressive
several
studies
including
a
recent
one
in
the
journal
of
sex
research
have
shown
that
human
judges’
gaydar
is
no
more
reliable
than
a
coin
flip
when
the
judgement
is
based
on
pictures
taken
under
well-controlled
conditions
head
pose
lighting
glasses
makeup
etc
it’s
better
than
chance
if
these
variables
are
not
controlled
for
because
a
person’s
presentation
—
especially
if
that
person
is
out
—
involves
social
signaling
we
signal
our
orientation
and
many
other
kinds
of
status
presumably
in
order
to
attract
the
kind
of
attention
we
want
and
to
fit
in
with
people
like
us
[9]
wang
and
kosinski
argue
against
this
interpretation
on
the
grounds
that
their
algorithm
works
on
facebook
selfies
of
openly
gay
men
as
well
as
dating
website
selfies
the
issue
however
is
not
whether
the
images
come
from
a
dating
website
or
facebook
but
whether
they
are
self-posted
or
taken
under
standardized
conditions
most
people
present
themselves
in
ways
that
have
been
calibrated
over
many
years
of
media
consumption
observing
others
looking
in
the
mirror
and
gauging
social
reactions
in
one
of
the
earliest
gaydar
studies
using
social
media
participants
could
categorize
gay
men
with
about
58%
accuracy
but
when
the
researchers
used
facebook
images
of
gay
and
heterosexual
men
posted
by
their
friends
still
far
from
a
perfect
control
the
accuracy
dropped
to
52%
if
subtle
biases
in
image
quality
expression
and
grooming
can
be
picked
up
on
by
humans
these
biases
can
also
be
detected
by
an
ai
algorithm
while
wang
and
kosinski
acknowledge
grooming
and
style
they
believe
that
the
chief
differences
between
their
composite
images
relate
to
face
shape
arguing
that
gay
men’s
faces
are
more
feminine
narrower
jaws
longer
noses
larger
foreheads
while
lesbian
faces
are
more
masculine
larger
jaws
shorter
noses
smaller
foreheads
as
with
less
facial
hair
on
gay
men
and
darker
skin
on
straight
men
they
suggest
that
the
mechanism
is
gender-atypical
hormonal
exposure
during
development
this
echoes
a
widely
discredited
19th
century
model
of
homosexuality
sexual
inversion
more
likely
heterosexual
men
tend
to
take
selfies
from
slightly
below
which
will
have
the
apparent
effect
of
enlarging
the
chin
shortening
the
nose
shrinking
the
forehead
and
attenuating
the
smile
see
our
selfies
below
this
view
emphasizes
dominance
—
or
perhaps
more
benignly
an
expectation
that
the
viewer
will
be
shorter
on
the
other
hand
as
a
wedding
photographer
notes
in
her
blog
when
you
shoot
from
above
your
eyes
look
bigger
which
is
generally
attractive
—
especially
for
women
this
may
be
a
heteronormative
assessment
when
a
face
is
photographed
from
below
the
nostrils
are
prominent
while
higher
shooting
angles
de-emphasize
and
eventually
conceal
them
altogether
looking
again
at
the
composite
images
we
can
see
that
the
heterosexual
male
face
has
more
pronounced
dark
spots
corresponding
to
the
nostrils
than
the
gay
male
while
the
opposite
is
true
for
the
female
faces
this
is
consistent
with
a
pattern
of
heterosexual
men
on
average
shooting
from
below
heterosexual
women
from
above
as
the
wedding
photographer
suggests
and
gay
men
and
lesbian
women
from
directly
in
front
a
similar
pattern
is
evident
in
the
eyebrows:
shooting
from
above
makes
them
look
more
v-shaped
but
their
apparent
shape
becomes
flatter
and
eventually
caret-shaped
^
as
the
camera
is
lowered
shooting
from
below
also
makes
the
outer
corners
of
the
eyes
appear
lower
in
short
the
changes
in
the
average
positions
of
facial
landmarks
are
consistent
with
what
we
would
expect
to
see
from
differing
selfie
angles
the
ambiguity
between
shooting
angle
and
the
real
physical
sizes
of
facial
features
is
hard
to
fully
disentangle
from
a
two-dimensional
image
both
for
a
human
viewer
and
for
an
algorithm
although
the
authors
are
using
face
recognition
technology
designed
to
try
to
cancel
out
all
effects
of
head
pose
lighting
grooming
and
other
variables
not
intrinsic
to
the
face
we
can
confirm
that
this
doesn’t
work
perfectly
that’s
why
multiple
distinct
images
of
a
person
help
when
grouping
photos
by
subject
in
google
photos
and
why
a
person
may
initially
appear
in
more
than
one
group
tom
white
a
researcher
at
victoria
university
in
new
zealand
has
experimented
with
the
same
facial
recognition
engine
kosinski
and
wang
use
vgg
face
and
has
found
that
its
output
varies
systematically
based
on
variables
like
smiling
and
head
pose
when
he
trains
a
classifier
based
on
vgg
face’s
output
to
distinguish
a
happy
expression
from
a
neutral
one
it
gets
the
answer
right
92%
of
the
time
—
which
is
significant
given
that
the
heterosexual
female
composite
has
a
much
more
pronounced
smile
changes
in
head
pose
might
be
even
more
reliably
detectable
for
576
test
images
a
classifier
is
able
to
pick
out
the
ones
facing
to
the
right
with
100%
accuracy
in
summary
we
have
shown
how
the
obvious
differences
between
lesbian
or
gay
and
straight
faces
in
selfies
relate
to
grooming
presentation
and
lifestyle
—
that
is
differences
in
culture
not
in
facial
structure
these
differences
include:
we’ve
demonstrated
that
just
a
handful
of
yesno
questions
about
these
variables
can
do
nearly
as
good
a
job
at
guessing
orientation
as
supposedly
sophisticated
facial
recognition
ai
further
the
current
generation
of
facial
recognition
remains
sensitive
to
head
pose
and
facial
expression
therefore
—
at
least
at
this
point
—
it’s
hard
to
credit
the
notion
that
this
ai
is
in
some
way
superhuman
at
outing
us
based
on
subtle
but
unalterable
details
of
our
facial
structure
this
doesn’t
negate
the
privacy
concerns
the
authors
and
various
commentators
have
raised
but
it
emphasizes
that
such
concerns
relate
less
to
ai
per
se
than
to
mass
surveillance
which
is
troubling
regardless
of
the
technologies
used
even
when
as
in
the
days
of
the
stasi
in
east
germany
these
were
nothing
but
paper
files
and
audiotapes
like
computers
or
the
internal
combustion
engine
ai
is
a
general-purpose
technology
that
can
be
used
to
automate
a
great
many
tasks
including
ones
that
should
not
be
undertaken
in
the
first
place
we
are
hopeful
about
the
confluence
of
new
powerful
ai
technologies
with
social
science
but
not
because
we
believe
in
reviving
the
19th
century
research
program
of
inferring
people’s
inner
character
from
their
outer
appearance
rather
we
believe
ai
is
an
essential
tool
for
understanding
patterns
in
human
culture
and
behavior
it
can
expose
stereotypes
inherent
in
everyday
language
it
can
reveal
uncomfortable
truths
as
in
google’s
work
with
the
geena
davis
institute
where
our
face
gender
classifier
established
that
men
are
seen
and
heard
nearly
twice
as
often
as
women
in
hollywood
movies
yet
female-led
films
outperform
others
at
the
box
office!
making
social
progress
and
holding
ourselves
to
account
is
more
difficult
without
such
hard
evidence
even
when
it
only
confirms
our
suspicions
two
of
us
margaret
mitchell
and
blaise
agüera
y
arcas
are
research
scientists
specializing
in
machine
learning
and
ai
at
google
agüera
y
arcas
leads
a
team
that
includes
deep
learning
applied
to
face
recognition
and
powers
face
grouping
in
google
photos
alex
todorov
is
a
professor
in
the
psychology
department
at
princeton
where
he
directs
the
social
perception
lab
he
is
the
author
of
face
value:
the
irresistible
influence
of
first
impressions
[1]
this
wording
is
based
on
several
large
national
surveys
which
we
were
able
to
use
to
sanity-check
our
numbers
about
6%
of
respondents
identified
as
homosexual
gay
or
lesbian
and
85%
as
heterosexual
about
4%
of
all
genders
were
exclusively
same-sex
attracted
of
the
men
10%
were
either
sexually
or
romantically
same-sex
attracted
and
of
the
women
20%
just
under
1%
of
respondents
were
trans
and
about
2%
identified
with
both
or
neither
of
the
pronouns
she
and
he
these
numbers
are
broadly
consistent
with
other
surveys
especially
when
considered
as
a
function
of
age
the
mechanical
turk
population
skews
somewhat
younger
than
the
overall
population
of
the
us
and
consistent
with
other
studies
our
data
show
that
younger
people
are
far
more
likely
to
identify
non-heteronormatively
[2]
these
are
wider
for
same-sex
attracted
and
lesbian
women
because
they
are
minority
populations
resulting
in
a
larger
sampling
error
the
same
holds
for
older
people
in
our
sample
[3]
for
the
remainder
of
the
plots
we
stick
to
opposite-sex
attracted
and
same-sex
attracted
as
the
counts
are
higher
and
the
error
bars
therefore
smaller
these
categories
are
also
somewhat
less
culturally
freighted
since
they
rely
on
questions
about
attraction
rather
than
identity
as
with
eyeshadow
and
makeup
the
effects
are
similar
and
often
even
larger
when
comparing
heterosexual-identifying
with
lesbian-
or
gay-identifying
people
[4]
although
we
didn’t
test
this
explicitly
slightly
different
rates
of
laser
correction
surgery
seem
a
likely
cause
of
the
small
but
growing
disparity
between
opposite-sex
attracted
and
same-sex
attracted
women
who
answer
yes
to
the
vision
defect
questions
as
they
age
[5]
this
finding
may
prompt
the
further
question
why
do
more
opposite-sex
attracted
men
work
outdoors?
this
is
not
addressed
by
any
of
our
survey
questions
but
hopefully
the
other
evidence
presented
here
will
discourage
an
essentialist
assumption
such
as
straight
men
are
just
more
outdoorsy
without
the
evidence
of
a
controlled
study
that
can
support
the
leap
from
correlation
to
cause
such
explanations
are
a
form
of
logical
fallacy
sometimes
called
a
just-so
story:
an
unverifiable
narrative
explanation
for
a
cultural
practice
[6]
of
the
253
lesbian-identified
women
in
the
sample
5
or
2%
were
over
six
feet
and
25
or
10%
were
over
5’9
out
of
3333
heterosexual
women
women
who
answered
yes
to
are
you
heterosexual
or
straight?
only
16
or
05%
were
over
six
feet
and
152
or
5%
were
over
5’9
[7]
they
note
that
these
figures
rise
to
91%
for
men
and
83%
for
women
if
5
images
are
considered
[8]
these
results
are
based
on
the
simplest
possible
machine
learning
technique
a
linear
classifier
the
classifier
is
trained
on
a
randomly
chosen
70%
of
the
data
with
the
remaining
30%
of
the
data
held
out
for
testing
over
500
repetitions
of
this
procedure
the
error
is
6953%
±
298%
with
the
same
number
of
repetitions
and
holdout
basing
the
decision
on
height
alone
gives
an
error
of
5108%
±
327%
and
basing
it
on
eyeshadow
alone
yields
6296%
±
239%
[9]
a
longstanding
body
of
work
eg
goffman’s
the
presentation
of
self
in
everyday
life
1959
and
jones
and
pittman’s
toward
a
general
theory
of
strategic
self-presentation
1982
delves
more
deeply
into
why
we
present
ourselves
the
way
we
do
both
for
instrumental
reasons
status
power
attraction
and
because
our
presentation
informs
and
is
informed
by
how
we
conceive
of
our
social
selves
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
blaise
aguera
y
arcas
leads
google’s
ai
group
in
seattle
he
founded
seadragon
and
was
one
of
the
creators
of
photosynth
at
microsoft
""
in
machine
learning
there’s
something
called
the
no
free
lunch
theorem
in
a
nutshell
it
states
that
no
one
algorithm
works
best
for
every
problem
and
it’s
especially
relevant
for
supervised
learning
ie
predictive
modeling
for
example
you
can’t
say
that
neural
networks
are
always
better
than
decision
trees
or
vice-versa
there
are
many
factors
at
play
such
as
the
size
and
structure
of
your
dataset
as
a
result
you
should
try
many
different
algorithms
for
your
problem
while
using
a
hold-out
test
set
of
data
to
evaluate
performance
and
select
the
winner
of
course
the
algorithms
you
try
must
be
appropriate
for
your
problem
which
is
where
picking
the
right
machine
learning
task
comes
in
as
an
analogy
if
you
need
to
clean
your
house
you
might
use
a
vacuum
a
broom
or
a
mop
but
you
wouldn’t
bust
out
a
shovel
and
start
digging
however
there
is
a
common
principle
that
underlies
all
supervised
machine
learning
algorithms
for
predictive
modeling
this
is
a
general
learning
task
where
we
would
like
to
make
predictions
in
the
future
y
given
new
examples
of
input
variables
x
we
don’t
know
what
the
function
f
looks
like
or
its
form
if
we
did
we
would
use
it
directly
and
we
would
not
need
to
learn
it
from
data
using
machine
learning
algorithms
the
most
common
type
of
machine
learning
is
to
learn
the
mapping
y
=
fx
to
make
predictions
of
y
for
new
x
this
is
called
predictive
modeling
or
predictive
analytics
and
our
goal
is
to
make
the
most
accurate
predictions
possible
for
machine
learning
newbies
who
are
eager
to
understand
the
basic
of
machine
learning
here
is
a
quick
tour
on
the
top
10
machine
learning
algorithms
used
by
data
scientists
linear
regression
is
perhaps
one
of
the
most
well-known
and
well-understood
algorithms
in
statistics
and
machine
learning
predictive
modeling
is
primarily
concerned
with
minimizing
the
error
of
a
model
or
making
the
most
accurate
predictions
possible
at
the
expense
of
explainability
we
will
borrow
reuse
and
steal
algorithms
from
many
different
fields
including
statistics
and
use
them
towards
these
ends
the
representation
of
linear
regression
is
an
equation
that
describes
a
line
that
best
fits
the
relationship
between
the
input
variables
x
and
the
output
variables
y
by
finding
specific
weightings
for
the
input
variables
called
coefficients
b
for
example:
y
=
b0
""
b1
*
x
we
will
predict
y
given
the
input
x
and
the
goal
of
the
linear
regression
learning
algorithm
is
to
find
the
values
for
the
coefficients
b0
and
b1
different
techniques
can
be
used
to
learn
the
linear
regression
model
from
data
such
as
a
linear
algebra
solution
for
ordinary
least
squares
and
gradient
descent
optimization
linear
regression
has
been
around
for
more
than
200
years
and
has
been
extensively
studied
some
good
rules
of
thumb
when
using
this
technique
are
to
remove
variables
that
are
very
similar
correlated
and
to
remove
noise
from
your
data
if
possible
it
is
a
fast
and
simple
technique
and
good
first
algorithm
to
try
logistic
regression
is
another
technique
borrowed
by
machine
learning
from
the
field
of
statistics
it
is
the
go-to
method
for
binary
classification
problems
problems
with
two
class
values
logistic
regression
is
like
linear
regression
in
that
the
goal
is
to
find
the
values
for
the
coefficients
that
weight
each
input
variable
unlike
linear
regression
the
prediction
for
the
output
is
transformed
using
a
non-linear
function
called
the
logistic
function
the
logistic
function
looks
like
a
big
s
and
will
transform
any
value
into
the
range
0
to
1
this
is
useful
because
we
can
apply
a
rule
to
the
output
of
the
logistic
function
to
snap
values
to
0
and
1
eg
if
less
than
05
then
output
1
and
predict
a
class
value
because
of
the
way
that
the
model
is
learned
the
predictions
made
by
logistic
regression
can
also
be
used
as
the
probability
of
a
given
data
instance
belonging
to
class
0
or
class
1
this
can
be
useful
for
problems
where
you
need
to
give
more
rationale
for
a
prediction
like
linear
regression
logistic
regression
does
work
better
when
you
remove
attributes
that
are
unrelated
to
the
output
variable
as
well
as
attributes
that
are
very
similar
correlated
to
each
other
it’s
a
fast
model
to
learn
and
effective
on
binary
classification
problems
logistic
regression
is
a
classification
algorithm
traditionally
limited
to
only
two-class
classification
problems
if
you
have
more
than
two
classes
then
the
linear
discriminant
analysis
algorithm
is
the
preferred
linear
classification
technique
the
representation
of
lda
is
pretty
straight
forward
it
consists
of
statistical
properties
of
your
data
calculated
for
each
class
for
a
single
input
variable
this
includes:
predictions
are
made
by
calculating
a
discriminate
value
for
each
class
and
making
a
prediction
for
the
class
with
the
largest
value
the
technique
assumes
that
the
data
has
a
gaussian
distribution
bell
curve
so
it
is
a
good
idea
to
remove
outliers
from
your
data
before
hand
it’s
a
simple
and
powerful
method
for
classification
predictive
modeling
problems
decision
trees
are
an
important
type
of
algorithm
for
predictive
modeling
machinelearning
the
representation
of
the
decision
tree
model
is
a
binary
tree
this
is
your
binary
tree
from
algorithms
and
data
structures
nothing
too
fancy
each
node
represents
a
single
input
variable
x
and
a
split
point
on
that
variable
assuming
the
variable
is
numeric
the
leaf
nodes
of
the
tree
contain
an
output
variable
y
which
is
used
to
make
a
prediction
predictions
are
made
by
walking
the
splits
of
the
tree
until
arriving
at
a
leaf
node
and
output
the
class
value
at
that
leaf
node
trees
are
fast
to
learn
and
very
fast
for
making
predictions
they
are
also
often
accurate
for
a
broad
range
of
problems
and
do
not
require
any
special
preparation
for
your
data
naive
bayes
is
a
simple
but
surprisingly
powerful
algorithm
for
predictive
modeling
the
model
is
comprised
of
two
types
of
probabilities
that
can
be
calculated
directly
from
your
training
data:
1
the
probability
of
each
class
and
2
the
conditional
probability
for
each
class
given
each
x
value
once
calculated
the
probability
model
can
be
used
to
make
predictions
for
new
data
using
bayes
theorem
when
your
data
is
real-valued
it
is
common
to
assume
a
gaussian
distribution
bell
curve
so
that
you
can
easily
estimate
these
probabilities
naive
bayes
is
called
naive
because
it
assumes
that
each
input
variable
is
independent
this
is
a
strong
assumption
and
unrealistic
for
real
data
nevertheless
the
technique
is
very
effective
on
a
large
range
of
complex
problems
the
knn
algorithm
is
very
simple
and
very
effective
the
model
representation
for
knn
is
the
entire
training
dataset
simple
right?
predictions
are
made
for
a
new
data
point
by
searching
through
the
entire
training
set
for
the
k
most
similar
instances
the
neighbors
and
summarizing
the
output
variable
for
those
k
instances
for
regression
problems
this
might
be
the
mean
output
variable
for
classification
problems
this
might
be
the
mode
or
most
common
class
value
the
trick
is
in
how
to
determine
the
similarity
between
the
data
instances
the
simplest
technique
if
your
attributes
are
all
of
the
same
scale
all
in
inches
for
example
is
to
use
the
euclidean
distance
a
number
you
can
calculate
directly
based
on
the
differences
between
each
input
variable
knn
can
require
a
lot
of
memory
or
space
to
store
all
of
the
data
but
only
performs
a
calculation
or
learn
when
a
prediction
is
needed
just
in
time
you
can
also
update
and
curate
your
training
instances
over
time
to
keep
predictions
accurate
the
idea
of
distance
or
closeness
can
break
down
in
very
high
dimensions
lots
of
input
variables
which
can
negatively
affect
the
performance
of
the
algorithm
on
your
problem
this
is
called
the
curse
of
dimensionality
it
suggests
you
only
use
those
input
variables
that
are
most
relevant
to
predicting
the
output
variable
a
downside
of
k-nearest
neighbors
is
that
you
need
to
hang
on
to
your
entire
training
dataset
the
learning
vector
quantization
algorithm
or
lvq
for
short
is
an
artificial
neural
network
algorithm
that
allows
you
to
choose
how
many
training
instances
to
hang
onto
and
learns
exactly
what
those
instances
should
look
like
the
representation
for
lvq
is
a
collection
of
codebook
vectors
these
are
selected
randomly
in
the
beginning
and
adapted
to
best
summarize
the
training
dataset
over
a
number
of
iterations
of
the
learning
algorithm
after
learned
the
codebook
vectors
can
be
used
to
make
predictions
just
like
k-nearest
neighbors
the
most
similar
neighbor
best
matching
codebook
vector
is
found
by
calculating
the
distance
between
each
codebook
vector
and
the
new
data
instance
the
class
value
or
real
value
in
the
case
of
regression
for
the
best
matching
unit
is
then
returned
as
the
prediction
best
results
are
achieved
if
you
rescale
your
data
to
have
the
same
range
such
as
between
0
and
1
if
you
discover
that
knn
gives
good
results
on
your
dataset
try
using
lvq
to
reduce
the
memory
requirements
of
storing
the
entire
training
dataset
support
vector
machines
are
perhaps
one
of
the
most
popular
and
talked
about
machine
learning
algorithms
a
hyperplane
is
a
line
that
splits
the
input
variable
space
in
svm
a
hyperplane
is
selected
to
best
separate
the
points
in
the
input
variable
space
by
their
class
either
class
0
or
class
1
in
two-dimensions
you
can
visualize
this
as
a
line
and
let’s
assume
that
all
of
our
input
points
can
be
completely
separated
by
this
line
the
svm
learning
algorithm
finds
the
coefficients
that
results
in
the
best
separation
of
the
classes
by
the
hyperplane
the
distance
between
the
hyperplane
and
the
closest
data
points
is
referred
to
as
the
margin
the
best
or
optimal
hyperplane
that
can
separate
the
two
classes
is
the
line
that
has
the
largest
margin
only
these
points
are
relevant
in
defining
the
hyperplane
and
in
the
construction
of
the
classifier
these
points
are
called
the
support
vectors
they
support
or
define
the
hyperplane
in
practice
an
optimization
algorithm
is
used
to
find
the
values
for
the
coefficients
that
maximizes
the
margin
svm
might
be
one
of
the
most
powerful
out-of-the-box
classifiers
and
worth
trying
on
your
dataset
random
forest
is
one
of
the
most
popular
and
most
powerful
machine
learning
algorithms
it
is
a
type
of
ensemble
machine
learning
algorithm
called
bootstrap
aggregation
or
bagging
the
bootstrap
is
a
powerful
statistical
method
for
estimating
a
quantity
from
a
data
sample
such
as
a
mean
you
take
lots
of
samples
of
your
data
calculate
the
mean
then
average
all
of
your
mean
values
to
give
you
a
better
estimation
of
the
true
mean
value
in
bagging
the
same
approach
is
used
but
instead
for
estimating
entire
statistical
models
most
commonly
decision
trees
multiple
samples
of
your
training
data
are
taken
then
models
are
constructed
for
each
data
sample
when
you
need
to
make
a
prediction
for
new
data
each
model
makes
a
prediction
and
the
predictions
are
averaged
to
give
a
better
estimate
of
the
true
output
value
random
forest
is
a
tweak
on
this
approach
where
decision
trees
are
created
so
that
rather
than
selecting
optimal
split
points
suboptimal
splits
are
made
by
introducing
randomness
the
models
created
for
each
sample
of
the
data
are
therefore
more
different
than
they
otherwise
would
be
but
still
accurate
in
their
unique
and
different
ways
combining
their
predictions
results
in
a
better
estimate
of
the
true
underlying
output
value
if
you
get
good
results
with
an
algorithm
with
high
variance
like
decision
trees
you
can
often
get
better
results
by
bagging
that
algorithm
boosting
is
an
ensemble
technique
that
attempts
to
create
a
strong
classifier
from
a
number
of
weak
classifiers
this
is
done
by
building
a
model
from
the
training
data
then
creating
a
second
model
that
attempts
to
correct
the
errors
from
the
first
model
models
are
added
until
the
training
set
is
predicted
perfectly
or
a
maximum
number
of
models
are
added
adaboost
was
the
first
really
successful
boosting
algorithm
developed
for
binary
classification
it
is
the
best
starting
point
for
understanding
boosting
modern
boosting
methods
build
on
adaboost
most
notably
stochastic
gradient
boosting
machines
adaboost
is
used
with
short
decision
trees
after
the
first
tree
is
created
the
performance
of
the
tree
on
each
training
instance
is
used
to
weight
how
much
attention
the
next
tree
that
is
created
should
pay
attention
to
each
training
instance
training
data
that
is
hard
to
predict
is
given
more
weight
whereas
easy
to
predict
instances
are
given
less
weight
models
are
created
sequentially
one
after
the
other
each
updating
the
weights
on
the
training
instances
that
affect
the
learning
performed
by
the
next
tree
in
the
sequence
after
all
the
trees
are
built
predictions
are
made
for
new
data
and
the
performance
of
each
tree
is
weighted
by
how
accurate
it
was
on
training
data
because
so
much
attention
is
put
on
correcting
mistakes
by
the
algorithm
it
is
important
that
you
have
clean
data
with
outliers
removed
a
typical
question
asked
by
a
beginner
when
facing
a
wide
variety
of
machine
learning
algorithms
is
which
algorithm
should
i
use?
the
answer
to
the
question
varies
depending
on
many
factors
including:
1
the
size
quality
and
nature
of
data
2
the
available
computational
time
3
the
urgency
of
the
task
and
4
what
you
want
to
do
with
the
data
even
an
experienced
data
scientist
cannot
tell
which
algorithm
will
perform
the
best
before
trying
different
algorithms
although
there
are
many
other
machine
learning
algorithms
these
are
the
most
popular
ones
if
you’re
a
newbie
to
machine
learning
these
would
be
a
good
starting
point
to
learn
—
—
if
you
enjoyed
this
piece
i’d
love
it
if
you
hit
the
clap
button
👏
so
others
might
stumble
upon
it
you
can
find
my
own
code
on
github
and
more
of
my
writing
and
projects
at
https:jamesklecom
you
can
also
follow
me
on
twitter
email
me
directly
or
find
me
on
linkedin
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
blue
ocean
thinker
https:jamesklecom
sharing
concepts
ideas
and
codes
""
for
more
content
like
this
follow
insight
and
emmanuel
on
twitter
whether
you
are
an
established
company
or
working
to
launch
a
new
service
you
can
always
leverage
text
data
to
validate
improve
and
expand
the
functionalities
of
your
product
the
science
of
extracting
meaning
and
learning
from
text
data
is
an
active
topic
of
research
called
natural
language
processing
nlp
nlp
produces
new
and
exciting
results
on
a
daily
basis
and
is
a
very
large
field
however
having
worked
with
hundreds
of
companies
the
insight
team
has
seen
a
few
key
practical
applications
come
up
much
more
frequently
than
any
other:
while
many
nlp
papers
and
tutorials
exist
online
we
have
found
it
hard
to
find
guidelines
and
tips
on
how
to
approach
these
problems
efficiently
from
the
ground
up
after
leading
hundreds
of
projects
a
year
and
gaining
advice
from
top
teams
all
over
the
united
states
we
wrote
this
post
to
explain
how
to
build
machine
learning
solutions
to
solve
problems
like
the
ones
mentioned
above
we’ll
begin
with
the
simplest
method
that
could
work
and
then
move
on
to
more
nuanced
solutions
such
as
feature
engineering
word
vectors
and
deep
learning
after
reading
this
article
you’ll
know
how
to:
we
wrote
this
post
as
a
step-by-step
guide
it
can
also
serve
as
a
high
level
overview
of
highly
effective
standard
approaches
this
post
is
accompanied
by
an
interactive
notebook
demonstrating
and
applying
all
these
techniques
feel
free
to
run
the
code
and
follow
along!
every
machine
learning
problem
starts
with
data
such
as
a
list
of
emails
posts
or
tweets
common
sources
of
textual
information
include:
disasters
on
social
media
dataset
for
this
post
we
will
use
a
dataset
generously
provided
by
crowdflower
called
disasters
on
social
media
where:
our
task
will
be
to
detect
which
tweets
are
about
a
disastrous
event
as
opposed
to
an
irrelevant
topic
such
as
a
movie
why?
a
potential
application
would
be
to
exclusively
notify
law
enforcement
officials
about
urgent
emergencies
while
ignoring
reviews
of
the
most
recent
adam
sandler
film
a
particular
challenge
with
this
task
is
that
both
classes
contain
the
same
search
terms
used
to
find
the
tweets
so
we
will
have
to
use
subtler
differences
to
distinguish
between
them
in
the
rest
of
this
post
we
will
refer
to
tweets
that
are
about
disasters
as
disaster
and
tweets
about
anything
else
as
irrelevant
we
have
labeled
data
and
so
we
know
which
tweets
belong
to
which
categories
as
richard
socher
outlines
below
it
is
usually
faster
simpler
and
cheaper
to
find
and
label
enough
data
to
train
a
model
on
rather
than
trying
to
optimize
a
complex
unsupervised
method
one
of
the
key
skills
of
a
data
scientist
is
knowing
whether
the
next
step
should
be
working
on
the
model
or
the
data
a
good
rule
of
thumb
is
to
look
at
the
data
first
and
then
clean
it
up
a
clean
dataset
will
allow
a
model
to
learn
meaningful
features
and
not
overfit
on
irrelevant
noise
here
is
a
checklist
to
use
to
clean
your
data:
see
the
code
for
more
details:
after
following
these
steps
and
checking
for
additional
errors
we
can
start
using
the
clean
labelled
data
to
train
models!
machine
learning
models
take
numerical
values
as
input
models
working
on
images
for
example
take
in
a
matrix
representing
the
intensity
of
each
pixel
in
each
color
channel
our
dataset
is
a
list
of
sentences
so
in
order
for
our
algorithm
to
extract
patterns
from
the
data
we
first
need
to
find
a
way
to
represent
it
in
a
way
that
our
algorithm
can
understand
ie
as
a
list
of
numbers
a
natural
way
to
represent
text
for
computers
is
to
encode
each
character
individually
as
a
number
ascii
for
example
if
we
were
to
feed
this
simple
representation
into
a
classifier
it
would
have
to
learn
the
structure
of
words
from
scratch
based
only
on
our
data
which
is
impossible
for
most
datasets
we
need
to
use
a
higher
level
approach
for
example
we
can
build
a
vocabulary
of
all
the
unique
words
in
our
dataset
and
associate
a
unique
index
to
each
word
in
the
vocabulary
each
sentence
is
then
represented
as
a
list
that
is
as
long
as
the
number
of
distinct
words
in
our
vocabulary
at
each
index
in
this
list
we
mark
how
many
times
the
given
word
appears
in
our
sentence
this
is
called
a
bag
of
words
model
since
it
is
a
representation
that
completely
ignores
the
order
of
words
in
our
sentence
this
is
illustrated
below
we
have
around
20000
words
in
our
vocabulary
in
the
disasters
of
social
media
example
which
means
that
every
sentence
will
be
represented
as
a
vector
of
length
20000
the
vector
will
contain
mostly
0s
because
each
sentence
contains
only
a
very
small
subset
of
our
vocabulary
in
order
to
see
whether
our
embeddings
are
capturing
information
that
is
relevant
to
our
problem
ie
whether
the
tweets
are
about
disasters
or
not
it
is
a
good
idea
to
visualize
them
and
see
if
the
classes
look
well
separated
since
vocabularies
are
usually
very
large
and
visualizing
data
in
20000
dimensions
is
impossible
techniques
like
pca
will
help
project
the
data
down
to
two
dimensions
this
is
plotted
below
the
two
classes
do
not
look
very
well
separated
which
could
be
a
feature
of
our
embeddings
or
simply
of
our
dimensionality
reduction
in
order
to
see
whether
the
bag
of
words
features
are
of
any
use
we
can
train
a
classifier
based
on
them
when
first
approaching
a
problem
a
general
best
practice
is
to
start
with
the
simplest
tool
that
could
solve
the
job
whenever
it
comes
to
classifying
data
a
common
favorite
for
its
versatility
and
explainability
is
logistic
regression
it
is
very
simple
to
train
and
the
results
are
interpretable
as
you
can
easily
extract
the
most
important
coefficients
from
the
model
we
split
our
data
in
to
a
training
set
used
to
fit
our
model
and
a
test
set
to
see
how
well
it
generalizes
to
unseen
data
after
training
we
get
an
accuracy
of
754%
not
too
shabby!
guessing
the
most
frequent
class
irrelevant
would
give
us
only
57%
however
even
if
75%
precision
was
good
enough
for
our
needs
we
should
never
ship
a
model
without
trying
to
understand
it
a
first
step
is
to
understand
the
types
of
errors
our
model
makes
and
which
kind
of
errors
are
least
desirable
in
our
example
false
positives
are
classifying
an
irrelevant
tweet
as
a
disaster
and
false
negatives
are
classifying
a
disaster
as
an
irrelevant
tweet
if
the
priority
is
to
react
to
every
potential
event
we
would
want
to
lower
our
false
negatives
if
we
are
constrained
in
resources
however
we
might
prioritize
a
lower
false
positive
rate
to
reduce
false
alarms
a
good
way
to
visualize
this
information
is
using
a
confusion
matrix
which
compares
the
predictions
our
model
makes
with
the
true
label
ideally
the
matrix
would
be
a
diagonal
line
from
top
left
to
bottom
right
our
predictions
match
the
truth
perfectly
our
classifier
creates
more
false
negatives
than
false
positives
proportionally
in
other
words
our
model’s
most
common
error
is
inaccurately
classifying
disasters
as
irrelevant
if
false
positives
represent
a
high
cost
for
law
enforcement
this
could
be
a
good
bias
for
our
classifier
to
have
to
validate
our
model
and
interpret
its
predictions
it
is
important
to
look
at
which
words
it
is
using
to
make
decisions
if
our
data
is
biased
our
classifier
will
make
accurate
predictions
in
the
sample
data
but
the
model
would
not
generalize
well
in
the
real
world
here
we
plot
the
most
important
words
for
both
the
disaster
and
irrelevant
class
plotting
word
importance
is
simple
with
bag
of
words
and
logistic
regression
since
we
can
just
extract
and
rank
the
coefficients
that
the
model
used
for
its
predictions
our
classifier
correctly
picks
up
on
some
patterns
hiroshima
massacre
but
clearly
seems
to
be
overfitting
on
some
meaningless
terms
heyoo
x1392
right
now
our
bag
of
words
model
is
dealing
with
a
huge
vocabulary
of
different
words
and
treating
all
words
equally
however
some
of
these
words
are
very
frequent
and
are
only
contributing
noise
to
our
predictions
next
we
will
try
a
way
to
represent
sentences
that
can
account
for
the
frequency
of
words
to
see
if
we
can
pick
up
more
signal
from
our
data
in
order
to
help
our
model
focus
more
on
meaningful
words
we
can
use
a
tf-idf
score
term
frequency
inverse
document
frequency
on
top
of
our
bag
of
words
model
tf-idf
weighs
words
by
how
rare
they
are
in
our
dataset
discounting
words
that
are
too
frequent
and
just
add
to
the
noise
here
is
the
pca
projection
of
our
new
embeddings
we
can
see
above
that
there
is
a
clearer
distinction
between
the
two
colors
this
should
make
it
easier
for
our
classifier
to
separate
both
groups
let’s
see
if
this
leads
to
better
performance
training
another
logistic
regression
on
our
new
embeddings
we
get
an
accuracy
of
762%
a
very
slight
improvement
has
our
model
started
picking
up
on
more
important
words?
if
we
are
getting
a
better
result
while
preventing
our
model
from
cheating
then
we
can
truly
consider
this
model
an
upgrade
the
words
it
picked
up
look
much
more
relevant!
although
our
metrics
on
our
test
set
only
increased
slightly
we
have
much
more
confidence
in
the
terms
our
model
is
using
and
thus
would
feel
more
comfortable
deploying
it
in
a
system
that
would
interact
with
customers
our
latest
model
managed
to
pick
up
on
high
signal
words
however
it
is
very
likely
that
if
we
deploy
this
model
we
will
encounter
words
that
we
have
not
seen
in
our
training
set
before
the
previous
model
will
not
be
able
to
accurately
classify
these
tweets
even
if
it
has
seen
very
similar
words
during
training
to
solve
this
problem
we
need
to
capture
the
semantic
meaning
of
words
meaning
we
need
to
understand
that
words
like
‘good’
and
‘positive’
are
closer
than
‘apricot’
and
‘continent’
the
tool
we
will
use
to
help
us
capture
meaning
is
called
word2vec
using
pre-trained
words
word2vec
is
a
technique
to
find
continuous
embeddings
for
words
it
learns
from
reading
massive
amounts
of
text
and
memorizing
which
words
tend
to
appear
in
similar
contexts
after
being
trained
on
enough
data
it
generates
a
300-dimension
vector
for
each
word
in
a
vocabulary
with
words
of
similar
meaning
being
closer
to
each
other
the
authors
of
the
paper
open
sourced
a
model
that
was
pre-trained
on
a
very
large
corpus
which
we
can
leverage
to
include
some
knowledge
of
semantic
meaning
into
our
model
the
pre-trained
vectors
can
be
found
in
the
repository
associated
with
this
post
a
quick
way
to
get
a
sentence
embedding
for
our
classifier
is
to
average
word2vec
scores
of
all
words
in
our
sentence
this
is
a
bag
of
words
approach
just
like
before
but
this
time
we
only
lose
the
syntax
of
our
sentence
while
keeping
some
semantic
information
here
is
a
visualization
of
our
new
embeddings
using
previous
techniques:
the
two
groups
of
colors
look
even
more
separated
here
our
new
embeddings
should
help
our
classifier
find
the
separation
between
both
classes
after
training
the
same
model
a
third
time
a
logistic
regression
we
get
an
accuracy
score
of
777%
our
best
result
yet!
time
to
inspect
our
model
since
our
embeddings
are
not
represented
as
a
vector
with
one
dimension
per
word
as
in
our
previous
models
it’s
harder
to
see
which
words
are
the
most
relevant
to
our
classification
while
we
still
have
access
to
the
coefficients
of
our
logistic
regression
they
relate
to
the
300
dimensions
of
our
embeddings
rather
than
the
indices
of
words
for
such
a
low
gain
in
accuracy
losing
all
explainability
seems
like
a
harsh
trade-off
however
with
more
complex
models
we
can
leverage
black
box
explainers
such
as
lime
in
order
to
get
some
insight
into
how
our
classifier
works
lime
lime
is
available
on
github
through
an
open-sourced
package
a
black-box
explainer
allows
users
to
explain
the
decisions
of
any
classifier
on
one
particular
example
by
perturbing
the
input
in
our
case
removing
words
from
the
sentence
and
seeing
how
the
prediction
changes
let’s
see
a
couple
explanations
for
sentences
from
our
dataset
however
we
do
not
have
time
to
explore
the
thousands
of
examples
in
our
dataset
what
we’ll
do
instead
is
run
lime
on
a
representative
sample
of
test
cases
and
see
which
words
keep
coming
up
as
strong
contributors
using
this
approach
we
can
get
word
importance
scores
like
we
had
for
previous
models
and
validate
our
model’s
predictions
looks
like
the
model
picks
up
highly
relevant
words
implying
that
it
appears
to
make
understandable
decisions
these
seem
like
the
most
relevant
words
out
of
all
previous
models
and
therefore
we’re
more
comfortable
deploying
in
to
production
we’ve
covered
quick
and
efficient
approaches
to
generate
compact
sentence
embeddings
however
by
omitting
the
order
of
words
we
are
discarding
all
of
the
syntactic
information
of
our
sentences
if
these
methods
do
not
provide
sufficient
results
you
can
utilize
more
complex
model
that
take
in
whole
sentences
as
input
and
predict
labels
without
the
need
to
build
an
intermediate
representation
a
common
way
to
do
that
is
to
treat
a
sentence
as
a
sequence
of
individual
word
vectors
using
either
word2vec
or
more
recent
approaches
such
as
glove
or
cove
this
is
what
we
will
do
below
convolutional
neural
networks
for
sentence
classification
train
very
quickly
and
work
well
as
an
entry
level
deep
learning
architecture
while
convolutional
neural
networks
cnn
are
mainly
known
for
their
performance
on
image
data
they
have
been
providing
excellent
results
on
text
related
tasks
and
are
usually
much
quicker
to
train
than
most
complex
nlp
approaches
eg
lstms
and
encoderdecoder
architectures
this
model
preserves
the
order
of
words
and
learns
valuable
information
on
which
sequences
of
words
are
predictive
of
our
target
classes
contrary
to
previous
models
it
can
tell
the
difference
between
alex
eats
plants
and
plants
eat
alex
training
this
model
does
not
require
much
more
work
than
previous
approaches
see
code
for
details
and
gives
us
a
model
that
is
much
better
than
the
previous
ones
getting
795%
accuracy!
as
with
the
models
above
the
next
step
should
be
to
explore
and
explain
the
predictions
using
the
methods
we
described
to
validate
that
it
is
indeed
the
best
model
to
deploy
to
users
by
now
you
should
feel
comfortable
tackling
this
on
your
own
here
is
a
quick
recap
of
the
approach
we’ve
successfully
used:
these
approaches
were
applied
to
a
particular
example
case
using
models
tailored
towards
understanding
and
leveraging
short
text
such
as
tweets
but
the
ideas
are
widely
applicable
to
a
variety
of
problems
i
hope
this
helped
you
we’d
love
to
hear
your
comments
and
questions!
feel
free
to
comment
below
or
reach
out
to
@emmanuelameisen
here
or
on
twitter
want
to
learn
applied
artificial
intelligence
from
top
professionals
in
silicon
valley
or
new
york?
learn
more
about
the
artificial
intelligence
program
are
you
a
company
working
in
ai
and
would
like
to
get
involved
in
the
insight
ai
fellows
program?
feel
free
to
get
in
touch
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ai
lead
at
insight
ai
@emmanuelameisen
insight
fellows
program
-
your
bridge
to
a
career
in
data
""
for
the
past
year
we’ve
compared
nearly
8800
open
source
machine
learning
projects
to
pick
top
30
03%
chance
this
is
an
extremely
competitive
list
and
it
carefully
picks
the
best
open
source
machine
learning
libraries
datasets
and
apps
published
between
january
and
december
2017
mybridge
ai
evaluates
the
quality
by
considering
popularity
engagement
and
recency
to
give
you
an
idea
about
the
quality
the
average
number
of
github
stars
is
3558
open
source
projects
can
be
useful
for
data
scientists
you
can
learn
by
reading
the
source
code
and
build
something
on
top
of
the
existing
projects
give
a
plenty
of
time
to
play
around
with
machine
learning
projects
you
may
have
missed
for
the
past
year
<recommended
learning>
a
neural
networks
deep
learning
a-ztm:
hands-on
artificial
neural
networks
[68745
recommends
455
stars]
b
tensorflow
complete
guide
to
tensorflow
for
deep
learning
with
python
[17834
recommends
465
stars]
click
the
numbers
below
credit
given
to
the
biggest
contributor
fasttext:
library
for
fast
text
representation
and
classification
[11786
stars
on
github]
courtesy
of
facebook
research
""
[
muse:
multilingual
unsupervised
or
supervised
word
embeddings
based
on
fast
text
695
stars
on
github]
deep-photo-styletransfer:
code
and
data
for
paper
deep
photo
style
transfer
[9747
stars
on
github]
courtesy
of
fujun
luan
phd
at
cornell
university
the
world’s
simplest
facial
recognition
api
for
python
and
the
command
line
[8672
stars
on
github]
courtesy
of
adam
geitgey
magenta:
music
and
art
generation
with
machine
intelligence
[8113
stars
on
github]
sonnet:
tensorflow-based
neural
network
library
[5731
stars
on
github]
courtesy
of
malcolm
reynolds
at
deepmind
deeplearnjs:
a
hardware-accelerated
machine
intelligence
library
for
the
web
[5462
stars
on
github]
courtesy
of
nikhil
thorat
at
google
brain
fast
style
transfer
in
tensorflow
[4843
stars
on
github]
courtesy
of
logan
engstrom
at
mit
pysc2:
starcraft
ii
learning
environment
[3683
stars
on
github]
courtesy
of
timo
ewalds
at
deepmind
airsim:
open
source
simulator
based
on
unreal
engine
for
autonomous
vehicles
from
microsoft
ai
""
research
[3861
stars
on
github]
courtesy
of
shital
shah
at
microsoft
facets:
visualizations
for
machine
learning
datasets
[3371
stars
on
github]
courtesy
of
google
brain
style2paints:
ai
colorization
of
images
[3310
stars
on
github]
tensor2tensor:
a
library
for
generalized
sequence
to
sequence
models
—
google
research
[3087
stars
on
github]
courtesy
of
ryan
sepassi
at
google
brain
image-to-image
translation
in
pytorch
eg
horse2zebra
edges2cats
and
more
[2847
stars
on
github]
courtesy
of
jun-yan
zhu
phd
at
berkeley
faiss:
a
library
for
efficient
similarity
search
and
clustering
of
dense
vectors
[2629
stars
on
github]
courtesy
of
facebook
research
fashion-mnist:
a
mnist-like
fashion
product
database
[2780
stars
on
github]
courtesy
of
han
xiao
research
scientist
zalando
tech
parlai:
a
framework
for
training
and
evaluating
ai
models
on
a
variety
of
openly
available
dialog
datasets
[2578
stars
on
github]
courtesy
of
alexander
miller
at
facebook
research
fairseq:
facebook
ai
research
sequence-to-sequence
toolkit
[2571
stars
on
github]
pyro:
deep
universal
probabilistic
programming
with
python
and
pytorch
[2387
stars
on
github]
courtesy
of
uber
ai
labs
igan:
interactive
image
generation
powered
by
gan
[2369
stars
on
github]
deep-image-prior:
image
restoration
with
neural
networks
but
without
learning
[2188
stars
on
github]
courtesy
of
dmitry
ulyanov
phd
at
skoltech
face_classification:
real-time
face
detection
and
emotiongender
classification
using
fer2013imdb
datasets
with
a
keras
cnn
model
and
opencv
[1967
stars
on
github]
speech-to-text-wavenet
:
end-to-end
sentence
level
english
speech
recognition
using
deepmind’s
wavenet
and
tensorflow
[1961
stars
on
github]
courtesy
of
namju
kim
at
kakao
brain
stargan:
unified
generative
adversarial
networks
for
multi-domain
image-to-image
translation
[1954
stars
on
github]
courtesy
of
yunjey
choi
at
korea
university
ml-agents:
unity
machine
learning
agents
[1658
stars
on
github]
courtesy
of
arthur
juliani
deep
learning
at
unity3d
deepvideoanalytics:
a
distributed
visual
search
and
visual
data
analytics
platform
[1494
stars
on
github]
courtesy
of
akshay
bhat
phd
at
cornell
university
opennmt:
open-source
neural
machine
translation
in
torch
[1490
stars
on
github]
pix2pixhd:
synthesizing
and
manipulating
2048x1024
images
with
conditional
gans
[1283
stars
on
github]
courtesy
of
ming-yu
liu
at
ai
research
scientist
at
nvidia
horovod:
distributed
training
framework
for
tensorflow
[1188
stars
on
github]
courtesy
of
uber
engineering
ai-blocks:
a
powerful
and
intuitive
wysiwyg
interface
that
allows
anyone
to
create
machine
learning
models
[899
stars
on
github]
deep
neural
networks
for
voice
conversion
voice
style
transfer
in
tensorflow
[845
stars
on
github]
courtesy
of
dabi
ahn
ai
research
at
kakao
brain
that’s
it
for
machine
learning
open
source
projects
if
you
like
this
curation
read
best
daily
articles
based
on
your
programming
skills
on
our
website
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
we
rank
articles
for
professionals
read
more
and
achieve
more
""
in
this
article
i’ll
attempt
to
cover
three
things:
in
march
2016
deepmind’s
alphago
beat
18
times
world
champion
go
player
lee
sedol
4–1
in
a
series
watched
by
over
200
million
people
a
machine
had
learnt
a
super-human
strategy
for
playing
go
a
feat
previously
thought
impossible
or
at
the
very
least
at
least
a
decade
away
from
being
accomplished
this
in
itself
was
a
remarkable
achievement
however
on
18th
october
2017
deepmind
took
a
giant
leap
further
the
paper
‘mastering
the
game
of
go
without
human
knowledge’
unveiled
a
new
variant
of
the
algorithm
alphago
zero
that
had
defeated
alphago
100–0
incredibly
it
had
done
so
by
learning
solely
through
self-play
starting
‘tabula
rasa’
blank
state
and
gradually
finding
strategies
that
would
beat
previous
incarnations
of
itself
no
longer
was
a
database
of
human
expert
games
required
to
build
a
super-human
ai
""
a
mere
48
days
later
on
5th
december
2017
deepmind
released
another
paper
‘mastering
chess
and
shogi
by
self-play
with
a
general
reinforcement
learning
algorithm’
showing
how
alphago
zero
could
be
adapted
to
beat
the
world-champion
programs
stockfish
and
elmo
at
chess
and
shogi
the
entire
learning
process
from
being
shown
the
games
for
the
first
time
to
becoming
the
best
computer
program
in
the
world
had
taken
under
24
hours
with
this
alphazero
was
born
—
the
general
algorithm
for
getting
good
at
something
quickly
without
any
prior
knowledge
of
human
expert
strategy
there
are
two
amazing
things
about
this
achievement:
it
cannot
be
overstated
how
important
this
is
this
means
that
the
underlying
methodology
of
alphago
zero
can
be
applied
to
any
game
with
perfect
information
the
game
state
is
fully
known
to
both
players
at
all
times
because
no
prior
expertise
is
required
beyond
the
rules
of
the
game
this
is
how
it
was
possible
for
deepmind
to
publish
the
chess
and
shogi
papers
only
48
days
after
the
original
alphago
zero
paper
quite
literally
all
that
needed
to
change
was
the
input
file
that
describes
the
mechanics
of
the
game
and
to
tweak
the
hyper-parameters
relating
to
the
neural
network
and
monte
carlo
tree
search
if
alphazero
used
super-complex
algorithms
that
only
a
handful
of
people
in
the
world
understood
it
would
still
be
an
incredible
achievement
what
makes
it
extraordinary
is
that
a
lot
of
the
ideas
in
the
paper
are
actually
far
less
complex
than
previous
versions
at
its
heart
lies
the
following
beautifully
simple
mantra
for
learning:
doesn’t
that
sound
a
lot
like
how
you
learn
to
play
games?
when
you
play
a
bad
move
it’s
either
because
you
misjudged
the
future
value
of
resulting
positions
or
you
misjudged
the
likelihood
that
your
opponent
would
play
a
certain
move
so
didn’t
think
to
explore
that
possibility
these
are
exactly
the
two
aspects
of
gameplay
that
alphazero
is
trained
to
learn
firstly
check
out
the
alphago
zero
cheat
sheet
for
a
high
level
understanding
of
how
alphago
zero
works
it’s
worth
having
that
to
refer
to
as
we
walk
through
each
part
of
the
code
there’s
also
a
great
article
here
that
explains
how
alphazero
works
in
more
detail
clone
this
git
repository
which
contains
the
code
i’ll
be
referencing
to
start
the
learning
process
run
the
top
two
panels
in
the
runipynb
jupyter
notebook
once
it’s
built
up
enough
game
positions
to
fill
its
memory
the
neural
network
will
begin
training
through
additional
self-play
and
training
it
will
gradually
get
better
at
predicting
the
game
value
and
next
moves
from
any
position
resulting
in
better
decision
making
and
smarter
overall
play
we’ll
now
have
a
look
at
the
code
in
more
detail
and
show
some
results
that
demonstrate
the
ai
getting
stronger
over
time
nb
—
this
is
my
own
understanding
of
how
alphazero
works
based
on
the
information
available
in
the
papers
referenced
above
if
any
of
the
below
is
incorrect
apologies
and
i’ll
endeavour
to
correct
it!
the
game
that
our
algorithm
will
learn
to
play
is
connect4
or
four
in
a
row
not
quite
as
complex
as
go
but
there
are
still
4531985219092
game
positions
in
total
the
game
rules
are
straightforward
players
take
it
in
turns
to
enter
a
piece
of
their
colour
in
the
top
of
any
available
column
the
first
player
to
get
four
of
their
colour
in
a
row
—
each
vertically
horizontally
or
diagonally
wins
if
the
entire
grid
is
filled
without
a
four-in-a-row
being
created
the
game
is
drawn
here’s
a
summary
of
the
key
files
that
make
up
the
codebase:
this
file
contains
the
game
rules
for
connect4
each
squares
is
allocated
a
number
from
0
to
41
as
follows:
the
gamepy
file
gives
the
logic
behind
moving
from
one
game
state
to
another
given
a
chosen
action
for
example
given
the
empty
board
and
action
38
the
takeaction
method
return
a
new
game
state
with
the
starting
player’s
piece
at
the
bottom
of
the
centre
column
you
can
replace
the
gamepy
file
with
any
game
file
that
conforms
to
the
same
api
and
the
algorithm
will
in
principal
learn
strategy
through
self
play
based
on
the
rules
you
have
given
it
this
contains
the
code
that
starts
the
learning
process
it
loads
the
game
rules
and
then
iterates
through
the
main
loop
of
the
algorithm
which
consist
of
three
stages:
there
are
two
agents
involved
in
this
loop
the
best_player
and
the
current_player
the
best_player
contains
the
best
performing
neural
network
and
is
used
to
generate
the
self
play
memories
the
current_player
then
retrains
its
neural
network
on
these
memories
and
is
then
pitched
against
the
best_player
if
it
wins
the
neural
network
inside
the
best_player
is
switched
for
the
neural
network
inside
the
current_player
and
the
loop
starts
again
this
contains
the
agent
class
a
player
in
the
game
each
player
is
initialised
with
its
own
neural
network
and
monte
carlo
search
tree
the
simulate
method
runs
the
monte
carlo
tree
search
process
specifically
the
agent
moves
to
a
leaf
node
of
the
tree
evaluates
the
node
with
its
neural
network
and
then
backfills
the
value
of
the
node
up
through
the
tree
the
act
method
repeats
the
simulation
multiple
times
to
understand
which
move
from
the
current
position
is
most
favourable
it
then
returns
the
chosen
action
to
the
game
to
enact
the
move
the
replay
method
retrains
the
neural
network
using
memories
from
previous
games
this
file
contains
the
residual_cnn
class
which
defines
how
to
build
an
instance
of
the
neural
network
it
uses
a
condensed
version
of
the
neural
network
architecture
in
the
alphagozero
paper
—
ie
a
convolutional
layer
followed
by
many
residual
layers
then
splitting
into
a
value
and
policy
head
the
depth
and
number
of
convolutional
filters
can
be
specified
in
the
config
file
the
keras
library
is
used
to
build
the
network
with
a
backend
of
tensorflow
to
view
individual
convolutional
filters
and
densely
connected
layers
in
the
neural
network
run
the
following
inside
the
the
runipynb
notebook:
this
contains
the
node
edge
and
mcts
classes
that
constitute
a
monte
carlo
search
tree
the
mcts
class
contains
the
movetoleaf
and
backfill
methods
previously
mentioned
and
instances
of
the
edge
class
store
the
statistics
about
each
potential
move
this
is
where
you
set
the
key
parameters
that
influence
the
algorithm
adjusting
these
variables
will
affect
that
running
time
neural
network
accuracy
and
overall
success
of
the
algorithm
the
above
parameters
produce
a
high
quality
connect4
player
but
take
a
long
time
to
do
so
to
speed
the
algorithm
up
try
the
following
parameters
instead
contains
the
playmatches
and
playmatchesbetweenversions
functions
that
play
matches
between
two
agents
to
play
against
your
creation
run
the
following
code
it’s
also
in
the
runipynb
notebook
when
you
run
the
algorithm
all
model
and
memory
files
are
saved
in
the
run
folder
in
the
root
directory
to
restart
the
algorithm
from
this
checkpoint
later
transfer
the
run
folder
to
the
run_archive
folder
attaching
a
run
number
to
the
folder
name
then
enter
the
run
number
model
version
number
and
memory
version
number
into
the
initialisepy
file
corresponding
to
the
location
of
the
relevant
files
in
the
run_archive
folder
running
the
algorithm
as
usual
will
then
start
from
this
checkpoint
an
instance
of
the
memory
class
stores
the
memories
of
previous
games
that
the
algorithm
uses
to
retrain
the
neural
network
of
the
current_player
this
file
contains
a
custom
loss
function
that
masks
predictions
from
illegal
moves
before
passing
to
the
cross
entropy
loss
function
the
locations
of
the
run
and
run_archive
folders
log
files
are
saved
to
the
log
folder
inside
the
run
folder
to
turn
on
logging
set
the
values
of
the
logger_disabled
variables
to
false
inside
this
file
viewing
the
log
files
will
help
you
to
understand
how
the
algorithm
works
and
see
inside
its
‘mind’
for
example
here
is
a
sample
from
the
loggermcts
file
equally
from
the
loggertourney
file
you
can
see
the
probabilities
attached
to
each
move
during
the
evaluation
phase:
training
over
a
couple
of
days
produces
the
following
chart
of
loss
against
mini-batch
iteration
number:
the
top
line
is
the
error
in
the
policy
head
the
cross
entropy
of
the
mcts
move
probabilities
against
the
output
from
the
neural
network
the
bottom
line
is
the
error
in
the
value
head
the
mean
squared
error
between
the
actual
game
value
and
the
neural
network
predict
of
the
value
the
middle
line
is
an
average
of
the
two
clearly
the
neural
network
is
getting
better
at
predicting
the
value
of
each
game
state
and
the
likely
next
moves
to
show
how
this
results
in
stronger
and
stronger
play
i
ran
a
league
between
17
players
ranging
from
the
1st
iteration
of
the
neural
network
up
to
the
49th
each
pairing
played
twice
with
both
players
having
a
chance
to
play
first
here
are
the
final
standings:
clearly
the
later
versions
of
the
neural
network
are
superior
to
the
earlier
versions
winning
most
of
their
games
it
also
appears
that
the
learning
hasn’t
yet
saturated
—
with
further
training
time
the
players
would
continue
to
get
stronger
learning
more
and
more
intricate
strategies
as
an
example
one
clear
strategy
that
the
neural
network
has
favoured
over
time
is
grabbing
the
centre
column
early
observe
the
difference
between
the
first
version
of
the
algorithm
and
say
the
30th
version:
1st
neural
network
version
30th
neural
network
version
this
is
a
good
strategy
as
many
lines
require
the
centre
column
—
claiming
this
early
ensures
your
opponent
cannot
take
advantage
of
this
this
has
been
learnt
by
the
neural
network
without
any
human
input
there
is
a
gamepy
file
for
a
game
called
‘metasquares’
in
the
games
folder
this
involves
placing
x
and
o
markers
in
a
grid
to
try
to
form
squares
of
different
sizes
larger
squares
score
more
points
than
smaller
squares
and
the
player
with
the
most
points
when
the
grid
is
full
wins
if
you
switch
the
connect4
gamepy
file
for
the
metasquares
gamepy
file
the
same
algorithm
will
learn
how
to
play
metasquares
instead
hopefully
you
find
this
article
useful
—
let
me
know
in
the
comments
below
if
you
find
any
typos
or
have
questions
about
anything
in
the
codebase
or
article
and
i’ll
get
back
to
you
as
soon
as
possible
if
you
would
like
to
learn
more
about
how
our
company
applied
data
science
develops
innovative
data
science
solutions
for
businesses
feel
free
to
get
in
touch
through
our
website
or
directly
through
linkedin
""
and
if
you
like
this
feel
free
to
leave
a
few
hearty
claps
:
applied
data
science
is
a
london
based
consultancy
that
implements
end-to-end
data
science
solutions
for
businesses
delivering
measurable
value
if
you’re
looking
to
do
more
with
your
data
let’s
talk
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
co-founder
of
applied
data
science
cutting
edge
data
science
news
and
projects
""
clustering
is
a
machine
learning
technique
that
involves
the
grouping
of
data
points
given
a
set
of
data
points
we
can
use
a
clustering
algorithm
to
classify
each
data
point
into
a
specific
group
in
theory
data
points
that
are
in
the
same
group
should
have
similar
properties
andor
features
while
data
points
in
different
groups
should
have
highly
dissimilar
properties
andor
features
clustering
is
a
method
of
unsupervised
learning
and
is
a
common
technique
for
statistical
data
analysis
used
in
many
fields
in
data
science
we
can
use
clustering
analysis
to
gain
some
valuable
insights
from
our
data
by
seeing
what
groups
the
data
points
fall
into
when
we
apply
a
clustering
algorithm
today
we’re
going
to
look
at
5
popular
clustering
algorithms
that
data
scientists
need
to
know
and
their
pros
and
cons!
k-means
is
probably
the
most
well
know
clustering
algorithm
it’s
taught
in
a
lot
of
introductory
data
science
and
machine
learning
classes
it’s
easy
to
understand
and
implement
in
code!
check
out
the
graphic
below
for
an
illustration
k-means
has
the
advantage
that
it’s
pretty
fast
as
all
we’re
really
doing
is
computing
the
distances
between
points
and
group
centers
very
few
computations!
it
thus
has
a
linear
complexity
on
on
the
other
hand
k-means
has
a
couple
of
disadvantages
firstly
you
have
to
select
how
many
groupsclasses
there
are
this
isn’t
always
trivial
and
ideally
with
a
clustering
algorithm
we’d
want
it
to
figure
those
out
for
us
because
the
point
of
it
is
to
gain
some
insight
from
the
data
k-means
also
starts
with
a
random
choice
of
cluster
centers
and
therefore
it
may
yield
different
clustering
results
on
different
runs
of
the
algorithm
thus
the
results
may
not
be
repeatable
and
lack
consistency
other
cluster
methods
are
more
consistent
k-medians
is
another
clustering
algorithm
related
to
k-means
except
instead
of
recomputing
the
group
center
points
using
the
mean
we
use
the
median
vector
of
the
group
this
method
is
less
sensitive
to
outliers
because
of
using
the
median
but
is
much
slower
for
larger
datasets
as
sorting
is
required
on
each
iteration
when
computing
the
median
vector
mean
shift
clustering
is
a
sliding-window-based
algorithm
that
attempts
to
find
dense
areas
of
data
points
it
is
a
centroid-based
algorithm
meaning
that
the
goal
is
to
locate
the
center
points
of
each
groupclass
which
works
by
updating
candidates
for
center
points
to
be
the
mean
of
the
points
within
the
sliding-window
these
candidate
windows
are
then
filtered
in
a
post-processing
stage
to
eliminate
near-duplicates
forming
the
final
set
of
center
points
and
their
corresponding
groups
check
out
the
graphic
below
for
an
illustration
an
illustration
of
the
entire
process
from
end-to-end
with
all
of
the
sliding
windows
is
show
below
each
black
dot
represents
the
centroid
of
a
sliding
window
and
each
gray
dot
is
a
data
point
in
contrast
to
k-means
clustering
there
is
no
need
to
select
the
number
of
clusters
as
mean-shift
automatically
discovers
this
that’s
a
massive
advantage
the
fact
that
the
cluster
centers
converge
towards
the
points
of
maximum
density
is
also
quite
desirable
as
it
is
quite
intuitive
to
understand
and
fits
well
in
a
naturally
data-driven
sense
the
drawback
is
that
the
selection
of
the
window
sizeradius
r
can
be
non-trivial
dbscan
is
a
density
based
clustered
algorithm
similar
to
mean-shift
but
with
a
couple
of
notable
advantages
check
out
another
fancy
graphic
below
and
let’s
get
started!
dbscan
poses
some
great
advantages
over
other
clustering
algorithms
firstly
it
does
not
require
a
pe-set
number
of
clusters
at
all
it
also
identifies
outliers
as
noises
unlike
mean-shift
which
simply
throws
them
into
a
cluster
even
if
the
data
point
is
very
different
additionally
it
is
able
to
find
arbitrarily
sized
and
arbitrarily
shaped
clusters
quite
well
the
main
drawback
of
dbscan
is
that
it
doesn’t
perform
as
well
as
others
when
the
clusters
are
of
varying
density
this
is
because
the
setting
of
the
distance
threshold
ε
and
minpoints
for
identifying
the
neighborhood
points
will
vary
from
cluster
to
cluster
when
the
density
varies
this
drawback
also
occurs
with
very
high-dimensional
data
since
again
the
distance
threshold
ε
becomes
challenging
to
estimate
one
of
the
major
drawbacks
of
k-means
is
its
naive
use
of
the
mean
value
for
the
cluster
center
we
can
see
why
this
isn’t
the
best
way
of
doing
things
by
looking
at
the
image
below
on
the
left
hand
side
it
looks
quite
obvious
to
the
human
eye
that
there
are
two
circular
clusters
with
different
radius’
centered
at
the
same
mean
k-means
can’t
handle
this
because
the
mean
values
of
the
clusters
are
a
very
close
together
k-means
also
fails
in
cases
where
the
clusters
are
not
circular
again
as
a
result
of
using
the
mean
as
cluster
center
gaussian
mixture
models
gmms
give
us
more
flexibility
than
k-means
with
gmms
we
assume
that
the
data
points
are
gaussian
distributed
this
is
a
less
restrictive
assumption
than
saying
they
are
circular
by
using
the
mean
that
way
we
have
two
parameters
to
describe
the
shape
of
the
clusters:
the
mean
and
the
standard
deviation!
taking
an
example
in
two
dimensions
this
means
that
the
clusters
can
take
any
kind
of
elliptical
shape
since
we
have
standard
deviation
in
both
the
x
and
y
directions
thus
each
gaussian
distribution
is
assigned
to
a
single
cluster
in
order
to
find
the
parameters
of
the
gaussian
for
each
cluster
eg
the
mean
and
standard
deviation
we
will
use
an
optimization
algorithm
called
expectation–maximization
em
take
a
look
at
the
graphic
below
as
an
illustration
of
the
gaussians
being
fitted
to
the
clusters
then
we
can
proceed
on
to
the
process
of
expectation–maximization
clustering
using
gmms
there
are
really
2
key
advantages
to
using
gmms
firstly
gmms
are
a
lot
more
flexible
in
terms
of
cluster
covariance
than
k-means
due
to
the
standard
deviation
parameter
the
clusters
can
take
on
any
ellipse
shape
rather
than
being
restricted
to
circles
k-means
is
actually
a
special
case
of
gmm
in
which
each
cluster’s
covariance
along
all
dimensions
approaches
0
secondly
since
gmms
use
probabilities
they
can
have
multiple
clusters
per
data
point
so
if
a
data
point
is
in
the
middle
of
two
overlapping
clusters
we
can
simply
define
its
class
by
saying
it
belongs
x-percent
to
class
1
and
y-percent
to
class
2
ie
gmms
support
mixed
membership
hierarchical
clustering
algorithms
actually
fall
into
2
categories:
top-down
or
bottom-up
bottom-up
algorithms
treat
each
data
point
as
a
single
cluster
at
the
outset
and
then
successively
merge
or
agglomerate
pairs
of
clusters
until
all
clusters
have
been
merged
into
a
single
cluster
that
contains
all
data
points
bottom-up
hierarchical
clustering
is
therefore
called
hierarchical
agglomerative
clustering
or
hac
this
hierarchy
of
clusters
is
represented
as
a
tree
or
dendrogram
the
root
of
the
tree
is
the
unique
cluster
that
gathers
all
the
samples
the
leaves
being
the
clusters
with
only
one
sample
check
out
the
graphic
below
for
an
illustration
before
moving
on
to
the
algorithm
steps
hierarchical
clustering
does
not
require
us
to
specify
the
number
of
clusters
and
we
can
even
select
which
number
of
clusters
looks
best
since
we
are
building
a
tree
additionally
the
algorithm
is
not
sensitive
to
the
choice
of
distance
metric
all
of
them
tend
to
work
equally
well
whereas
with
other
clustering
algorithms
the
choice
of
distance
metric
is
critical
a
particularly
good
use
case
of
hierarchical
clustering
methods
is
when
the
underlying
data
has
a
hierarchical
structure
and
you
want
to
recover
the
hierarchy
other
clustering
algorithms
can’t
do
this
these
advantages
of
hierarchical
clustering
come
at
the
cost
of
lower
efficiency
as
it
has
a
time
complexity
of
on3
unlike
the
linear
complexity
of
k-means
and
gmm
there
are
your
top
5
clustering
algorithms
that
a
data
scientist
should
know!
we’ll
end
off
with
an
awesome
visualization
of
how
well
these
algorithms
and
a
few
others
perform
courtesy
of
scikit
learn!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
certified
nerd
ai
""
machine
learning
engineer
sharing
concepts
ideas
and
codes
""
for
the
past
year
we’ve
compared
nearly
15000
open
source
python
projects
to
pick
top
30
02%
chance
this
is
an
extremely
competitive
list
and
it
carefully
picks
the
best
open
source
python
libraries
tools
and
programs
published
between
january
and
december
2017
mybridge
ai
evaluates
the
quality
by
considering
popularity
engagement
and
recency
to
give
you
an
idea
about
the
quality
the
average
number
of
github
stars
is
3707
open
source
projects
can
be
useful
for
programmers
you
can
learn
by
reading
the
source
code
and
build
something
on
top
of
the
existing
projects
give
a
plenty
of
time
to
play
around
with
python
projects
you
may
have
missed
for
the
past
year
<recommended
learning>
a
beginner
the
python
bible:
build
11
projects
and
go
from
beginner
to
pro
[27672
recommends
475
stars]
b
data
science
python
for
data
science
and
machine
learning
bootcamp:
use
numpy
pandas
seaborn
""
matplotlib
""
plotly
[90212
recommends
465
stars]
click
the
numbers
below
credit
given
to
the
biggest
contributor
home-assistant
v06:
open-source
home
automation
platform
running
on
python
3
[11357
stars
on
github]
courtesy
of
paulus
schoutsen
pytorch:
tensors
and
dynamic
neural
networks
in
python
with
strong
gpu
acceleration
[11019
stars
on
github]
courtesy
of
adam
paszke
and
others
at
pytorch
team
grumpy:
a
python
to
go
source
code
transcompiler
and
runtime
[8367
stars
on
github]
courtesy
of
dylan
trotter
and
others
at
google
sanic:
async
python
35
web
server
that’s
written
to
go
fast
[8028
stars
on
github]
courtesy
of
channel
cat
and
eli
uriegas
python-fire:
a
library
for
automatically
generating
command
line
interfaces
clis
from
absolutely
any
python
object
[7775
stars
on
github]
courtesy
of
david
bieber
and
others
at
google
brain
spacy
v20:
industrial-strength
natural
language
processing
nlp
with
python
and
cython
[7633
stars
on
github]
courtesy
of
matthew
honnibal
pipenv:
python
development
workflow
for
humans
[7273
stars
on
github]
courtesy
of
kenneth
reitz
micropython:
a
lean
and
efficient
python
implementation
for
microcontrollers
and
constrained
systems
[5728
stars
on
github]
prophet:
tool
for
producing
high
quality
forecasts
for
time
series
data
that
has
multiple
seasonality
with
linear
or
non-linear
growth
[4369
stars
on
github]
courtesy
of
facebook
serpentai:
game
agent
framework
in
python
helping
you
create
ais
""
bots
to
play
any
game
[3411
stars
on
github]
courtesy
of
nicholas
brochu
dash:
interactive
reactive
web
apps
in
pure
python
[3281
stars
on
github]
courtesy
of
chris
p
instapy:
instagram
bot
likecommentfollow
automation
script
[3179
stars
on
github]
courtesy
of
timg
apistar:
a
fast
and
expressive
api
framework
for
python
[3024
stars
on
github]
courtesy
of
tom
christie
faiss:
a
library
for
efficient
similarity
search
and
clustering
of
dense
vectors
[2717
stars
on
github]
courtesy
of
matthijs
douze
and
others
at
facebook
research
mechanicalsoup:
a
python
library
for
automating
interaction
with
websites
[2244
stars
on
github]
better-exceptions:
pretty
and
useful
exceptions
in
python
automatically
[2121
stars
on
github]
courtesy
of
qix
flashtext:
extract
keywords
from
sentence
or
replace
keywords
in
sentences
[2019
stars
on
github]
courtesy
of
vikash
singh
maya:
datetime
for
humans
in
python
[1828
stars
on
github]
kenneth
reitz
mimesis
v10:
python
library
which
helps
generate
mock
data
in
different
languages
for
various
purposes
these
data
can
be
especially
useful
at
various
stages
of
software
development
and
testing
[1732
stars
on
github]
courtesy
of
líkið
geimfari
open-paperless:
scan
index
and
archive
all
of
your
paper
documents
a
document
management
system
[1717
stars
on
github]
courtesy
of
tina
zhou
fsociety:
hacking
tools
pack
a
penetration
testing
framework
[1585
stars
on
github]
courtesy
of
manis
manisso
livepython:
visually
trace
python
code
in
real-time
[1577
stars
on
github]
courtesy
of
anastasis
germanidis
hatch:
a
modern
project
package
and
virtual
env
manager
for
python
[1537
stars
on
github]
courtesy
of
ofek
lev
tangent:
source-to-source
debuggable
derivatives
in
pure
python
[1433
stars
on
github]
courtesy
of
alex
wiltschko
and
others
at
google
brain
clairvoyant:
a
python
program
that
identifies
and
monitors
historical
cues
for
short
term
stock
movement
[1159
stars
on
github]
courtesy
of
anthony
federico
monkeytype:
a
system
for
python
that
generates
static
type
annotations
by
collecting
runtime
types
[1143
stars
on
github]
courtesy
of
carl
meyer
at
instagram
engineering
eel:
a
little
python
library
for
making
simple
electron-like
htmljs
gui
apps
[1137
stars
on
github]
surprise
v10:
a
python
scikit
for
building
and
analyzing
recommender
systems
[1103
stars
on
github]
gain:
web
crawling
framework
for
everyone
[1009
stars
on
github]
courtesy
of
高久力
pdftabextract:
a
set
of
tools
for
extracting
tables
from
pdf
files
helping
to
do
data
mining
on
scanned
documents
[722
stars
on
github]
that’s
it
for
python
open
source
of
the
year
if
you
like
this
curation
read
best
daily
articles
based
on
your
programming
skills
on
our
website
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
we
rank
articles
for
professionals
read
more
and
achieve
more
""
we
are
in
the
midst
of
a
gold
rush
in
ai
but
who
will
reap
the
economic
benefits?
the
mass
of
startups
who
are
all
gold
panning?
the
corporates
who
have
massive
gold
mining
operations?
the
technology
giants
who
are
supplying
the
picks
and
shovels?
and
which
nations
have
the
richest
seams
of
gold?
we
are
currently
experiencing
another
gold
rush
in
ai
billions
are
being
invested
in
ai
startups
across
every
imaginable
industry
and
business
function
google
amazon
microsoft
and
ibm
are
in
a
heavyweight
fight
investing
over
$20
billion
in
ai
in
2016
corporates
are
scrambling
to
ensure
they
realise
the
productivity
benefits
of
ai
ahead
of
their
competitors
while
looking
over
their
shoulders
at
the
startups
china
is
putting
its
considerable
weight
behind
ai
and
the
european
union
is
talking
about
a
$22
billion
ai
investment
as
it
fears
losing
ground
to
china
and
the
us
ai
is
everywhere
from
the
35
billion
daily
searches
on
google
to
the
new
apple
iphone
x
that
uses
facial
recognition
to
amazon
alexa
that
cutely
answers
our
questions
media
headlines
tout
the
stories
of
how
ai
is
helping
doctors
diagnose
diseases
banks
better
assess
customer
loan
risks
farmers
predict
crop
yields
marketers
target
and
retain
customers
and
manufacturers
improve
quality
control
and
there
are
think
tanks
dedicated
to
studying
the
physical
cyber
and
political
risks
of
ai
ai
and
machine
learning
will
become
ubiquitous
and
woven
into
the
fabric
of
society
but
as
with
any
gold
rush
the
question
is
who
will
find
gold?
will
it
just
be
the
brave
the
few
and
the
large?
or
can
the
snappy
upstarts
grab
their
nuggets?
will
those
providing
the
picks
and
shovel
make
most
of
the
money?
and
who
will
hit
pay
dirt?
as
i
started
thinking
about
who
was
going
to
make
money
in
ai
i
ended
up
with
seven
questions
who
will
make
money
across
the
1
chip
makers
2
platform
and
infrastructure
providers
3
enabling
models
and
algorithm
providers
4
enterprise
solution
providers
5
industry
vertical
solution
providers
6
corporate
users
of
ai
and
7
nations?
while
there
are
many
ways
to
skin
the
cat
of
the
ai
landscape
hopefully
below
provides
a
useful
explanatory
framework
—
a
value
chain
of
sorts
the
companies
noted
are
representative
of
larger
players
in
each
category
but
in
no
way
is
this
list
intended
to
be
comprehensive
or
predictive
even
though
the
price
of
computational
power
has
fallen
exponentially
demand
is
rising
even
faster
ai
and
machine
learning
with
its
massive
datasets
and
its
trillions
of
vector
and
matrix
calculations
has
a
ferocious
and
insatiable
appetite
bring
on
the
chips
nvidia’s
stock
is
up
1500%
in
the
past
two
years
benefiting
from
the
fact
that
their
graphical
processing
unit
gpu
chips
that
were
historically
used
to
render
beautiful
high
speed
flowing
games
graphics
were
perfect
for
machine
learning
google
recently
launched
its
second
generation
of
tensor
processing
units
tpus
and
microsoft
is
building
its
own
brainwave
ai
machine
learning
chips
at
the
same
time
startups
such
as
graphcore
who
has
raised
over
$110m
is
looking
to
enter
the
market
incumbents
chip
providers
such
as
ibm
intel
qualcomm
and
amd
are
not
standing
still
even
facebook
is
rumoured
to
be
building
a
team
to
design
its
own
ai
chips
and
the
chinese
are
emerging
as
serious
chip
players
with
cambricon
technology
announcing
the
first
cloud
ai
chip
this
past
week
what
is
clear
is
that
the
cost
of
designing
and
manufacturing
chips
then
sustaining
a
position
as
a
global
chip
leader
is
very
high
it
requires
extremely
deep
pockets
and
a
world
class
team
of
silicon
and
software
engineers
this
means
that
there
will
be
very
few
new
winners
just
like
the
gold
rush
days
those
that
provide
the
cheapest
and
most
widely
used
picks
and
shovels
will
make
a
lot
of
money
the
ai
race
is
now
also
taking
place
in
the
cloud
amazon
realised
early
that
startups
would
much
rather
rent
computers
and
software
than
buy
it
and
so
it
launched
amazon
web
services
aws
in
2006
today
ai
is
demanding
so
much
compute
power
that
companies
are
increasingly
turning
to
the
cloud
to
rent
hardware
through
infrastructure
as
a
service
iaas
and
platform
as
a
service
paas
offerings
the
fight
is
on
among
the
tech
giants
microsoft
is
offering
their
hybrid
public
and
private
azure
cloud
service
that
allegedly
has
over
one
million
computers
and
in
the
past
few
weeks
they
announced
that
their
brainwave
hardware
solutionsdramatically
accelerate
machine
learning
with
their
own
bing
search
engine
performance
improving
by
a
factor
of
ten
google
is
rushing
to
play
catchup
with
its
own
googlecloud
offering
and
we
are
seeing
the
chinese
alibaba
starting
to
take
global
share
amazon
—
microsoft
—
google
and
ibm
are
going
to
continue
to
duke
this
one
out
and
watch
out
for
the
massively
scaled
cloud
players
from
china
the
big
picks
and
shovels
guys
will
win
again
today
google
is
the
world’s
largest
ai
company
attracting
the
best
ai
minds
spending
small
country
size
gdp
budgets
on
rd
and
sitting
on
the
best
datasets
gleamed
from
the
billions
of
users
of
their
services
ai
is
powering
google’s
search
autonomous
vehicles
speech
recognition
intelligent
reasoning
massive
search
and
even
its
own
work
on
drug
discovery
and
disease
diangosis
and
the
incredible
ai
machine
learning
software
and
algorithms
that
are
powering
all
of
google’s
ai
activity
—
tensorflow
—
is
now
being
given
away
for
free
yes
for
free!
tensorflow
is
now
an
open
source
software
project
available
to
the
world
and
why
are
they
doing
this?
as
jeff
dean
head
of
google
brain
recently
said
there
are
20
million
organisations
in
the
world
that
could
benefit
from
machine
learning
today
if
millions
of
companies
use
this
best
in
class
free
ai
software
then
they
are
likely
to
need
lots
of
computing
power
and
who
is
better
served
to
offer
that?
well
google
cloud
is
of
course
optimised
for
tensorflow
and
related
ai
services
and
once
you
become
reliant
on
their
software
and
their
cloud
you
become
a
very
sticky
customer
for
many
years
to
come
no
wonder
it
is
a
brutal
race
for
global
ai
algorithm
dominance
with
amazon
—
microsoft
—
ibm
also
offering
their
own
cheap
or
free
ai
software
services
we
are
also
seeing
a
fight
for
not
only
machine
learning
algorithms
but
cognitive
algorithms
that
offer
services
for
conversational
agents
and
bots
speech
natural
language
processing
nlp
and
semantics
vision
and
enhanced
core
algorithms
one
startup
in
this
increasingly
contested
space
is
clarifai
who
provides
advanced
image
recognition
systems
for
businesses
to
detect
near-duplicates
and
visual
searches
it
has
raised
nearly
$40m
over
the
past
three
years
the
market
for
vision
related
algorithms
and
services
is
estimated
to
be
a
cumulative
$8
billion
in
revenue
between
2016
and
2025
the
giants
are
not
standing
still
ibm
for
example
is
offering
its
watson
cognitive
products
and
services
they
have
twenty
or
so
apis
for
chatbots
vision
speech
language
knowledge
management
and
empathy
that
can
be
simply
be
plugged
into
corporate
software
to
create
ai
enabled
applications
cognitive
apis
are
everywhere
kdnuggets
lists
here
over
50
of
the
top
cognitive
services
from
the
giants
and
startups
these
services
are
being
put
into
the
cloud
as
ai
as
a
service
aiaas
to
make
them
more
accessible
just
recently
microsoft’s
ceo
satya
nadella
claimed
that
a
million
developers
are
using
their
ai
apis
services
and
tools
for
building
ai-powered
apps
and
nearly
300000
developers
are
using
their
tools
for
chatbots
i
wouldn’t
want
to
be
a
startup
competing
with
these
goliaths
the
winners
in
this
space
are
likely
to
favour
the
heavyweights
again
they
can
hire
the
best
research
and
engineering
talent
spend
the
most
money
and
have
access
to
the
largest
datasets
to
flourish
startups
are
going
to
have
to
be
really
well
funded
supported
by
leading
researchers
with
a
whole
battery
of
ip
patents
and
published
papers
deep
domain
expertise
and
have
access
to
quality
datasets
and
they
should
have
excellent
navigational
skills
to
sail
ahead
of
the
giants
or
sail
different
races
there
will
many
startup
casualties
but
those
that
can
scale
will
find
themselves
as
global
enterprises
or
quickly
acquired
by
the
heavyweights
and
even
if
a
startup
has
not
found
a
path
to
commercialisation
then
they
could
become
acquihires
companies
bought
for
their
talent
if
they
are
working
on
enabling
ai
algorithms
with
a
strong
research
oriented
team
we
saw
this
in
2014
when
deepmind
a
two
year
old
london
based
company
that
developed
unique
reinforcement
machine
learning
algorithms
was
acquired
by
google
for
$400m
enterprise
software
has
been
dominated
by
giants
such
as
salesforce
ibm
oracle
and
sap
they
all
recognise
that
ai
is
a
tool
that
needs
to
be
integrated
into
their
enterprise
offerings
but
many
startups
are
rushing
to
become
the
next
generation
of
enterprise
services
filling
in
gaps
where
the
incumbents
don’t
currently
tread
or
even
attempting
to
disrupt
them
we
analysed
over
two
hundred
use
cases
in
the
enterprise
space
ranging
from
customer
management
to
marketing
to
cybersecurity
to
intelligence
to
hr
to
the
hot
area
of
cognitive
robotic
process
automation
rpa
the
enterprise
field
is
much
more
open
than
previous
spaces
with
a
veritable
medley
of
startups
providing
point
solutions
for
these
use
cases
today
there
are
over
200
ai
powered
companies
just
in
the
recruitment
space
many
of
them
ai
startups
cybersecurity
leader
darktrace
and
rpa
leader
uipathhave
war
chests
in
the
$100
millions
the
incumbents
also
want
to
make
sure
their
ecosystems
stay
on
the
forefront
and
are
investing
in
startups
that
enhance
their
offering
salesforce
has
invested
in
digital
genius
a
customer
management
solution
and
similarly
unbable
that
offers
enterprise
translation
services
incumbents
also
often
have
more
pressing
problems
sap
for
example
is
rushing
to
play
catchup
in
offering
a
cloud
solution
let
alone
catchup
in
ai
we
are
also
seeing
tools
providers
trying
to
simplify
the
tasks
required
to
create
deploy
and
manage
ai
services
in
the
enterprise
machine
learning
training
for
example
is
a
messy
business
where
80%
of
time
can
be
spent
on
data
wrangling
and
an
inordinate
amount
of
time
is
spent
on
testing
and
tuning
of
what
is
called
hyperparameters
petuum
a
tools
provider
based
in
pittsburgh
in
the
us
has
raised
over
$100m
to
help
accelerate
and
optimise
the
deployment
of
machine
learning
models
many
of
these
enterprise
startup
providers
can
have
a
healthy
future
if
they
quickly
demonstrate
that
they
are
solving
and
scaling
solutions
to
meet
real
world
enterprise
needs
but
as
always
happens
in
software
gold
rushes
there
will
be
a
handful
of
winners
in
each
category
and
for
those
ai
enterprise
category
winners
they
are
likely
to
be
snapped
up
along
with
the
best
in-class
tool
providers
by
the
giants
if
they
look
too
threatening
ai
is
driving
a
race
for
the
best
vertical
industry
solutions
there
are
a
wealth
of
new
ai
powered
startups
providing
solutions
to
corporate
use
cases
in
the
healthcare
financial
services
agriculture
automative
legal
and
industrial
sectors
and
many
startups
are
taking
the
ambitious
path
to
disrupt
the
incumbent
corporate
players
by
offering
a
service
directly
to
the
same
customers
it
is
clear
that
many
startups
are
providing
valuable
point
solutions
and
can
succeed
if
they
have
access
to
1
large
and
proprietary
data
training
sets
2
domain
knowledge
that
gives
them
deep
insights
into
the
opportunities
within
a
sector
3
a
deep
pool
of
talent
around
applied
ai
and
4
deep
pockets
of
capital
to
fund
rapid
growth
those
startups
that
are
doing
well
generally
speak
the
corporate
commercial
language
of
customers
business
efficiency
and
roi
in
the
form
of
well
developed
go-to-market
plans
for
example
zestfinance
has
raised
nearly
$300m
to
help
improve
credit
decision
making
that
will
provide
fair
and
transparent
credit
to
everyone
they
claim
they
have
the
world’s
best
data
scientists
but
they
would
wouldn’t
they?
for
those
startups
that
are
looking
to
disrupt
existing
corporate
players
they
need
really
deep
pockets
for
example
affirm
that
offers
loans
to
consumers
at
the
point
of
sale
has
raised
over
$700m
these
companies
quickly
need
to
create
a
defensible
moat
to
ensure
they
remain
competitive
this
can
come
from
data
network
effects
where
more
data
begets
better
ai
based
services
and
products
that
gets
more
revenue
and
customers
that
gets
more
data
and
so
the
flywheel
effect
continues
and
while
corporates
might
look
to
new
vendors
in
their
industry
for
ai
solutions
that
could
enhance
their
top
and
bottom
line
they
are
not
going
to
sit
back
and
let
upstarts
muscle
in
on
their
customers
and
they
are
not
going
to
sit
still
and
let
their
corporate
competitors
gain
the
first
advantage
through
ai
there
is
currently
a
massive
race
for
corporate
innovation
large
companies
have
their
own
venture
groups
investing
in
startups
running
accelerators
and
building
their
own
startups
to
ensure
that
they
are
leaders
in
ai
driven
innovation
large
corporates
are
in
a
strong
position
against
the
startups
and
smaller
companies
due
to
their
data
assets
data
is
the
fuel
for
ai
and
machine
learning
who
is
better
placed
to
take
advantage
of
ai
than
the
insurance
company
that
has
reams
of
historic
data
on
underwriting
claims?
the
financial
services
company
that
knows
everything
about
consumer
financial
product
buying
behaviour?
or
the
search
company
that
sees
more
user
searches
for
information
than
any
other?
corporates
large
and
small
are
well
positioned
to
extract
value
from
ai
in
fact
gartner
research
predicts
ai-derived
business
value
is
projected
to
reach
up
to
$39
trillion
by
2022
there
are
hundreds
if
not
thousands
of
valuable
use
cases
that
ai
can
addresses
across
organisations
corporates
can
improve
their
customer
experience
save
costs
lower
prices
drive
revenues
and
sell
better
products
and
services
powered
by
ai
ai
will
help
the
big
get
bigger
often
at
the
expense
of
smaller
companies
but
they
will
need
to
demonstrate
strong
visionary
leadership
an
ability
to
execute
and
a
tolerance
for
not
always
getting
technology
enabled
projects
right
on
the
first
try
countries
are
also
also
in
a
battle
for
ai
supremacy
china
has
not
been
shy
about
its
call
to
arms
around
ai
it
is
investing
massively
in
growing
technical
talent
and
developing
startups
its
more
lax
regulatory
environment
especially
in
data
privacy
helps
china
lead
in
ai
sectors
such
as
security
and
facial
recognition
just
recently
there
was
an
example
of
chinese
police
picking
out
one
most
wanted
face
in
a
crowd
of
50000
at
a
music
concert
and
sensetime
group
ltd
that
analyses
faces
and
images
on
a
massive
scale
reported
it
raised
$600m
becoming
the
most
valuable
global
ai
startup
the
chinese
point
out
that
their
mobile
market
is
3x
the
size
of
the
us
and
there
are
50x
more
mobile
payments
taking
place
—
this
is
a
massive
data
advantage
the
european
focus
on
data
privacy
regulation
could
put
them
at
a
disadvantage
in
certain
areas
of
ai
even
if
the
union
is
talking
about
a
$22b
investment
in
ai
the
uk
germany
france
and
japan
have
all
made
recent
announcements
about
their
nation
state
ai
strategies
for
example
president
macron
said
the
french
government
will
spend
$185
billion
over
the
next
five
years
to
support
the
ai
ecosystem
including
the
creation
of
large
public
datasets
companies
such
as
google’s
deepmind
and
samsung
have
committed
to
open
new
paris
labs
and
fujitsu
is
expanding
its
paris
research
centre
the
british
just
announced
a
$14
billion
push
into
ai
including
funding
of
1000
ai
phds
but
while
nations
are
investing
in
ai
talent
and
the
ecosystem
the
question
is
who
will
really
capture
the
value
will
france
and
the
uk
simply
be
subsidising
phds
who
will
be
hired
by
google?
and
while
payroll
and
income
taxes
will
be
healthy
on
those
six
figure
machine
learning
salaries
the
bulk
of
the
economic
value
created
could
be
with
this
american
company
its
shareholders
and
the
smiling
american
treasury
ai
will
increase
productivity
and
wealth
in
companies
and
countries
but
how
will
that
wealth
be
distributed
when
the
headlines
suggest
that
30
to
40%
of
our
jobs
will
be
taken
by
the
machines?
economists
can
point
to
lessons
from
hundreds
of
years
of
increasing
technology
automation
will
there
be
net
job
creation
or
net
job
loss?
the
public
debate
often
cites
geoffrey
hinton
the
godfather
of
machine
learning
who
suggested
radiologists
will
lose
their
jobs
by
the
dozen
as
machines
diagnose
diseases
from
medical
images
but
then
we
can
look
to
the
chinese
who
are
using
ai
to
assist
radiologists
in
managing
the
overwhelming
demand
to
review
14
billion
ct
scans
annually
for
lung
cancer
the
result
is
not
job
losses
but
an
expanded
market
with
more
efficient
and
accurate
diagnosis
however
there
is
likely
to
be
a
period
of
upheaval
when
much
of
the
value
will
go
to
those
few
companies
and
countries
that
control
ai
technology
and
data
and
lower
skilled
countries
whose
wealth
depends
on
jobs
that
are
targets
of
ai
automation
will
likely
suffer
ai
will
favour
the
large
and
the
technologically
skilled
in
examining
the
landscape
of
ai
it
has
became
clear
that
we
are
now
entering
a
truly
golden
era
for
ai
and
there
are
few
key
themes
appearing
as
to
where
the
economic
value
will
migrate:
in
short
it
looks
like
the
ai
gold
rush
will
favour
the
companies
and
countries
with
control
and
scale
over
the
best
ai
tools
and
technology
the
data
the
best
technical
workers
the
most
customers
and
the
strongest
access
to
capital
those
with
scale
will
capture
the
lion’s
share
of
the
economic
value
from
ai
in
some
ways
‘plus
ça
change
plus
c’est
la
même
chose’
but
there
will
also
be
large
golden
nuggets
that
will
be
found
by
a
few
choice
brave
startups
but
like
any
gold
rush
many
startups
will
hit
pay
dirt
and
many
individuals
and
societies
will
likely
feel
like
they
have
not
seen
the
benefits
of
the
gold
rush
this
is
the
first
part
in
a
series
of
articles
i
intend
to
write
on
the
topic
of
the
economics
of
ai
i
welcome
your
feedback
written
by
simon
greenman
i
am
a
lover
of
technology
and
how
it
can
be
applied
in
the
business
world
i
run
my
own
advisory
firm
best
practice
ai
helping
executives
of
enterprises
and
startups
accelerate
the
adoption
of
roi
based
ai
applications
""
please
get
in
touch
to
discuss
this
if
you
enjoyed
this
piece
i’d
love
it
if
you
hit
the
clap
button
👏
so
others
might
stumble
upon
it
and
please
post
your
comments
or
you
can
email
me
directly
or
find
me
on
linkedin
or
twitter
or
follow
me
at
simon
greenman
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ai
guy
mapquest
guy
grow
innovate
and
transform
companies
with
tech
start-up
investor
mentor
and
geek
sharing
concepts
ideas
and
codes
""
we
fell
for
recurrent
neural
networks
rnn
long-short
term
memory
lstm
and
all
their
variants
now
it
is
time
to
drop
them!
it
is
the
year
2014
and
lstm
and
rnn
make
a
great
come-back
from
the
dead
we
all
read
colah’s
blog
and
karpathy’s
ode
to
rnn
but
we
were
all
young
and
unexperienced
for
a
few
years
this
was
the
way
to
solve
sequence
learning
sequence
translation
seq2seq
which
also
resulted
in
amazing
results
in
speech
to
text
comprehension
and
the
raise
of
siri
cortana
google
voice
assistant
alexa
also
let
us
not
forget
machine
translation
which
resulted
in
the
ability
to
translate
documents
into
different
languages
or
neural
machine
translation
but
also
translate
images
into
text
text
into
images
and
captioning
video
and
""
well
you
got
the
idea
then
in
the
following
years
2015–16
came
resnet
and
attention
one
could
then
better
understand
that
lstm
were
a
clever
bypass
technique
also
attention
showed
that
mlp
network
could
be
replaced
by
averaging
networks
influenced
by
a
context
vector
more
on
this
later
it
only
took
2
more
years
but
today
we
can
definitely
say:
but
do
not
take
our
words
for
it
also
see
evidence
that
attention
based
networks
are
used
more
and
more
by
google
facebook
salesforce
to
name
a
few
all
these
companies
have
replaced
rnn
and
variants
for
attention
based
models
and
it
is
just
the
beginning
rnn
have
the
days
counted
in
all
applications
because
they
require
more
resources
to
train
and
run
than
attention-based
models
see
this
post
for
more
info
remember
rnn
and
lstm
and
derivatives
use
mainly
sequential
processing
over
time
see
the
horizontal
arrow
in
the
diagram
below:
this
arrow
means
that
long-term
information
has
to
sequentially
travel
through
all
cells
before
getting
to
the
present
processing
cell
this
means
it
can
be
easily
corrupted
by
being
multiplied
many
time
by
small
numbers
<
0
this
is
the
cause
of
vanishing
gradients
to
the
rescue
came
the
lstm
module
which
today
can
be
seen
as
multiple
switch
gates
and
a
bit
like
resnet
it
can
bypass
units
and
thus
remember
for
longer
time
steps
lstm
thus
have
a
way
to
remove
some
of
the
vanishing
gradients
problems
but
not
all
of
it
as
you
can
see
from
the
figure
above
still
we
have
a
sequential
path
from
older
past
cells
to
the
current
one
in
fact
the
path
is
now
even
more
complicated
because
it
has
additive
and
forget
branches
attached
to
it
no
question
lstm
and
gru
and
derivatives
are
able
to
learn
a
lot
of
longer
term
information!
see
results
here
but
they
can
remember
sequences
of
100s
not
1000s
or
10000s
or
more
and
one
issue
of
rnn
is
that
they
are
not
hardware
friendly
let
me
explain:
it
takes
a
lot
of
resources
we
do
not
have
to
train
these
network
fast
also
it
takes
much
resources
to
run
these
model
in
the
cloud
and
given
that
the
demand
for
speech-to-text
is
growing
rapidly
the
cloud
is
not
scalable
we
will
need
to
process
at
the
edge
right
into
the
amazon
echo!
see
note
below
for
more
details
if
sequential
processing
is
to
be
avoided
then
we
can
find
units
that
look-ahead
or
better
look-back
since
most
of
the
time
we
deal
with
real-time
causal
data
where
we
know
the
past
and
want
to
affect
future
decisions
not
so
in
translating
sentences
or
analyzing
recorded
videos
for
example
where
we
have
all
data
and
can
reason
on
it
more
time
such
look-backahead
units
are
neural
attention
modules
which
we
previously
explained
here
to
the
rescue
and
combining
multiple
neural
attention
modules
comes
the
hierarchical
neural
attention
encoder
shown
in
the
figure
below:
a
better
way
to
look
into
the
past
is
to
use
attention
modules
to
summarize
all
past
encoded
vectors
into
a
context
vector
ct
notice
there
is
a
hierarchy
of
attention
modules
here
very
similar
to
the
hierarchy
of
neural
networks
this
is
also
similar
to
temporal
convolutional
network
tcn
reported
in
note
3
below
in
the
hierarchical
neural
attention
encoder
multiple
layers
of
attention
can
look
at
a
small
portion
of
recent
past
say
100
vectors
while
layers
above
can
look
at
100
of
these
attention
modules
effectively
integrating
the
information
of
100
x
100
vectors
this
extends
the
ability
of
the
hierarchical
neural
attention
encoder
to
10000
past
vectors
but
more
importantly
look
at
the
length
of
the
path
needed
to
propagate
a
representation
vector
to
the
output
of
the
network:
in
hierarchical
networks
it
is
proportional
to
logn
where
n
are
the
number
of
hierarchy
layers
this
is
in
contrast
to
the
t
steps
that
a
rnn
needs
to
do
where
t
is
the
maximum
length
of
the
sequence
to
be
remembered
and
t
>>
n
this
architecture
is
similar
to
a
neural
turing
machine
but
lets
the
neural
network
decide
what
is
read
out
from
memory
via
attention
this
means
an
actual
neural
network
will
decide
which
vectors
from
the
past
are
important
for
future
decisions
but
what
about
storing
to
memory?
the
architecture
above
stores
all
previous
representation
in
memory
unlike
neural
turning
machines
this
can
be
rather
inefficient:
think
about
storing
the
representation
of
every
frame
in
a
video
—
most
times
the
representation
vector
does
not
change
frame-to-frame
so
we
really
are
storing
too
much
of
the
same!
what
can
we
do
is
add
another
unit
to
prevent
correlated
data
to
be
stored
for
example
by
not
storing
vectors
too
similar
to
previously
stored
ones
but
this
is
really
a
hack
the
best
would
be
to
be
let
the
application
guide
what
vectors
should
be
saved
or
not
this
is
the
focus
of
current
research
studies
stay
tuned
for
more
information
tell
your
friends!
it
is
very
surprising
to
us
to
see
so
many
companies
still
use
rnnlstm
for
speech
to
text
many
unaware
that
these
networks
are
so
inefficient
and
not
scalable
please
tell
them
about
this
post
about
training
rnnlstm:
rnn
and
lstm
are
difficult
to
train
because
they
require
memory-bandwidth-bound
computation
which
is
the
worst
nightmare
for
hardware
designer
and
ultimately
limits
the
applicability
of
neural
networks
solutions
in
short
lstm
require
4
linear
layer
mlp
layer
per
cell
to
run
at
and
for
each
sequence
time-step
linear
layers
require
large
amounts
of
memory
bandwidth
to
be
computed
in
fact
they
cannot
use
many
compute
unit
often
because
the
system
has
not
enough
memory
bandwidth
to
feed
the
computational
units
and
it
is
easy
to
add
more
computational
units
but
hard
to
add
more
memory
bandwidth
note
enough
lines
on
a
chip
long
wires
from
processors
to
memory
etc
as
a
result
rnnlstm
and
variants
are
not
a
good
match
for
hardware
acceleration
and
we
talked
about
this
issue
before
here
and
here
a
solution
will
be
compute
in
memory-devices
like
the
ones
we
work
on
at
fwdnxt
see
this
repository
for
a
simple
example
of
these
techniques
note
1:
hierarchical
neural
attention
is
similar
to
the
ideas
in
wavenet
but
instead
of
a
convolutional
neural
network
we
use
hierarchical
attention
modules
also:
hierarchical
neural
attention
can
be
also
bi-directional
note
2:
rnn
and
lstm
are
memory-bandwidth
limited
problems
see
this
for
details
the
processing
units
need
as
much
memory
bandwidth
as
the
number
of
operationss
they
can
provide
making
it
impossible
to
fully
utilize
them!
the
external
bandwidth
is
never
going
to
be
enough
and
a
way
to
slightly
ameliorate
the
problem
is
to
use
internal
fast
caches
with
high
bandwidth
the
best
way
is
to
use
techniques
that
do
not
require
large
amount
of
parameters
to
be
moved
back
and
forth
from
memory
or
that
can
be
re-used
for
multiple
computation
per
byte
transferred
high
arithmetic
intensity
note
3:
here
is
a
paper
comparing
cnn
to
rnn
temporal
convolutional
network
tcn
outperform
canonical
recurrent
networks
such
as
lstms
across
a
diverse
range
of
tasks
and
datasets
while
demonstrating
longer
effective
memory
note
4:
related
to
this
topic
is
the
fact
that
we
know
little
of
how
our
human
brain
learns
and
remembers
sequences
we
often
learn
and
recall
long
sequences
in
smaller
segments
such
as
a
phone
number
858
534
22
30
memorized
as
four
segments
behavioral
experiments
suggest
that
humans
and
some
animals
employ
this
strategy
of
breaking
down
cognitive
or
behavioral
sequences
into
chunks
in
a
wide
variety
of
tasks
—
these
chunks
remind
me
of
small
convolutional
or
attention
like
networks
on
smaller
sequences
that
then
are
hierarchically
strung
together
like
in
the
hierarchical
neural
attention
encoder
and
temporal
convolutional
network
tcn
more
studies
make
me
think
that
working
memory
is
similar
to
rnn
networks
that
uses
recurrent
real
neuron
networks
and
their
capacity
is
very
low
on
the
other
hand
both
the
cortex
and
hippocampus
give
us
the
ability
to
remember
really
long
sequences
of
steps
like:
where
did
i
park
my
car
at
airport
5
days
ago
suggesting
that
more
parallel
pathways
may
be
involved
to
recall
long
sequences
where
attention
mechanism
gate
important
chunks
and
force
hops
in
parts
of
the
sequence
that
is
not
relevant
to
the
final
goal
or
task
note
5:
the
above
evidence
shows
we
do
not
read
sequentially
in
fact
we
interpret
characters
words
and
sentences
as
a
group
an
attention-based
or
convolutional
module
perceives
the
sequence
and
projects
a
representation
in
our
mind
we
would
not
be
misreading
this
if
we
processed
this
information
sequentially!
we
would
stop
and
notice
the
inconsistencies!
i
have
almost
20
years
of
experience
in
neural
networks
in
both
hardware
and
software
a
rare
combination
see
about
me
here:
medium
webpage
scholar
linkedin
and
more
if
you
found
this
article
useful
please
consider
a
donation
to
support
more
tutorials
and
blogs
any
contribution
can
make
a
difference!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
i
dream
and
build
new
technology
sharing
concepts
ideas
and
codes
""
artificial
intelligence
is
a
state-of-the-art
technological
trend
that
many
companies
are
trying
to
integrate
into
their
business
a
recent
report
by
mckinsey
states
that
baidu
the
chinese
equivalent
of
alphabet
invested
$20
billion
in
ai
last
year
at
the
same
time
alphabet
invested
roughly
$30
billion
in
developing
ai
technologies
the
chinese
government
has
been
actively
pursuing
ai
technology
in
an
attempt
to
control
a
future
cornerstone
innovation
companies
in
the
us
are
also
investing
time
money
and
energy
into
advancing
ai
technology
the
reason
for
such
interest
towards
artificial
intelligence
is
that
artificial
intelligence
can
enhance
any
product
or
function
this
is
why
companies
and
governments
make
considerable
investments
in
the
research
and
development
of
this
technology
its
role
in
increasing
the
production
performance
while
simultaneously
reducing
the
costs
cannot
be
underestimated
since
some
of
the
largest
entities
in
the
world
are
focused
on
promoting
the
ai
technology
it
would
be
wise
to
understand
and
follow
the
trend
ai
is
already
shaping
the
economy
and
in
the
near
future
its
effect
may
be
even
more
significant
ignoring
the
new
technology
and
its
influence
on
the
global
economic
situation
is
a
recipe
for
failure
despite
the
huge
public
interest
and
attention
towards
ai
its
evolution
is
still
somewhat
halted
by
the
objective
causes
as
any
new
and
fast-developing
industry
ai
is
quickly
outgrowing
its
environment
according
to
adam
temper
an
author
of
many
creative
researches
on
artificial
intelligence
the
development
of
ai
is
mostly
limited
by
the
lack
of
employees
with
relevant
expertise
very
few
mature
standard
industry
tools
limited
high
quality
training
material
available
few
options
for
easy
access
to
preconfigured
machine
learning
environments
and
the
general
focus
in
the
industry
on
implementation
rather
than
design
with
any
new
complex
technology
the
learning
curve
is
steep
our
educational
institutions
are
several
steps
behind
the
commercial
applications
of
this
technology
it
is
important
that
ai
scientists
work
collaboratively
sharing
knowledge
and
best
practice
to
address
this
deficiency
ai
is
rapidly
increasing
its
impact
on
society
we
need
to
ensure
that
the
power
of
ai
doesn’t
remain
with
the
elite
few
another
factor
that
may
be
hindering
the
progress
of
ai
is
the
cautious
stance
that
people
tend
to
take
towards
it
artificial
intelligence
is
still
too
sci-fi
too
strange
and
therefore
sometimes
scary
when
people
learn
to
trust
ai
it
will
make
a
true
quantum
leap
in
the
way
of
general
adoption
and
application
adam
temper
supports
this
point
too
describing
the
possible
ways
for
ai
technology
to
gain
public
trust
as
at
the
same
time
if
we
analyze
the
primary
purpose
of
ai
we
will
see
it
for
what
it
really
is
—
a
tool
to
perform
the
routine
tasks
relieving
humans
for
something
more
creative
or
innovative
when
asked
about
the
current
trends
and
opportunities
of
ai
aaron
edell
ceo
and
co-founder
of
machine
box
and
one
of
the
top
writers
on
ai
described
them
as
follows:
ai
has
also
become
a
political
talking
point
in
recent
years
there
have
been
arguments
that
ai
will
help
to
create
jobs
but
that
it
will
also
cause
certain
workers
to
lose
their
jobs
for
example
estimations
prove
that
self-driving
vehicles
will
cause
25000
truck
drivers
to
lose
their
jobs
each
month
also
as
much
as
1
million
pickers
and
packers
working
in
us
warehouses
could
be
out
of
a
job
this
is
due
to
the
fact
that
by
implementing
ai
factories
can
operate
with
as
few
as
a
dozen
of
workers
naturally
companies
gladly
implement
artificial
intelligence
as
it
ensures
considerable
savings
at
the
same
time
governments
are
concerned
about
the
current
employment
situation
as
well
as
the
short-term
and
long-term
predictions
some
countries
have
already
begun
to
plan
measures
about
the
new
ai
technology
that
are
intended
to
keep
the
economy
stable
in
fact
it
would
not
be
fair
to
say
that
artificial
intelligence
causes
people
to
lose
jobs
true
the
whole
point
of
automation
is
making
machines
do
what
people
used
to
do
before
however
it
would
be
more
correct
if
we
said
that
artificial
intelligence
reshapes
the
employment
situation
together
with
taking
over
human
functions
it
creates
other
jobs
forces
people
to
master
new
skills
encourages
workers
to
increase
productivity
but
it
is
obvious
that
ai
is
going
to
turn
the
regular
sequence
of
events
upside
down
therefore
the
best
approach
is
not
to
wait
until
ai
leaves
you
unemployed
but
rather
proactively
embrace
it
and
learn
to
live
with
it
as
we
said
already
ai
can
also
create
jobs
so
a
wise
move
would
be
to
learn
to
manage
ai-based
tools
with
the
advance
of
ai
products
learning
to
work
with
them
may
secure
you
a
job
and
even
promote
your
career
your
future
largely
depends
on
your
current
and
expected
income
however
another
important
factor
is
the
way
you
manage
your
finances
of
course
investing
in
your
own
or
your
children’s
knowledge
is
one
of
the
best
investments
you
can
ever
make
at
the
same
time
if
you
need
some
financial
cushion
to
secure
your
family’s
welfare
you
should
look
at
the
available
investment
opportunities
and
this
is
where
artificial
intelligence
may
become
your
best
friend
professional
consultant
and
investment
manager
in
the
recent
years
in
addition
to
the
traditional
banks
and
financial
institutions
we
have
witnessed
the
appearance
of
a
totally
new
and
innovative
investment
system
we
are
talking
about
the
blockchain
technology
and
the
cryptocurrencies
that
it
supports
millions
of
people
all
over
the
world
have
already
appreciated
the
transparency
and
flexibility
of
the
blockchain
networks
by
watching
the
cryptocurrency
trends
carefully
and
trading
wisely
individual
investors
have
made
fortunes
within
a
very
short
time
nowadays
the
cryptocurrency
opportunities
are
open
for
everyone
not
only
for
the
industry
experts
there
are
investment
funds
running
on
artificial
intelligence
that
are
available
for
individual
investors
with
such
funds
you
are
on
one
hand
protected
by
the
blockchain
technology
it
ensures
proper
safety
of
your
funds
and
the
security
of
your
transactions
on
the
other
hand
you
do
not
need
to
be
an
investment
expert
to
make
wise
decisions
this
is
where
artificial
intelligence
is
at
your
service
it
analyzes
the
existing
trends
on
the
extremely
volatile
cryptocurrency
market
and
shows
you
the
best
opportunities
the
main
point
is
that
we
should
not
regard
ai
as
a
threat
to
our
careers
and
a
danger
to
our
well-being
instead
we
should
analyze
the
investment
openings
created
by
ai
technology
that
can
secure
our
prosperity
for
example
wolf
coin
is
using
ai
technology
to
create
a
seamless
investment
channel
for
savvy
individuals
this
robust
channel
opens
great
opportunities
that
investors
can
use
to
become
new
rich
kids
on
the
block
most
noteworthy
the
low
entry
cost
of
$10
has
made
it
one
offer
that
will
enjoy
a
huge
buzz
the
focus
on
this
new
market
opening
will
help
people
build
a
solid
financial
nest
egg
that
will
keep
them
safe
even
in
the
face
of
the
storm
wisewolf
fund
launching
the
wolf
coin
focused
its
effort
on
creating
a
great
opportunity
for
people
who
wish
to
benefit
from
cryptocurrency
trading
but
are
new
to
this
trend
with
artificial
intelligence
and
advanced
analytical
algorithms
the
fund
arranges
the
most
favorable
conditions
for
individual
investors
mainstream
manufacturers
companies
and
factories
are
embracing
ai
technology
to
change
the
mode
of
their
operations
therefore
it
is
critical
to
keep
tabs
on
this
reality
as
it
can
bring
many
benefits
that
cannot
be
found
elsewhere
ai
is
one
of
the
hottest
topics
of
discussion
however
it
is
now
clear
that
ai
is
here
to
stay
so
people
should
accept
the
obvious
in
order
to
create
the
future
that
they
desire
the
wisest
strategy
is
to
embrace
artificial
intelligence
and
let
it
work
to
maintain
our
well-being
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
the
wisewolf
crypto
fund
provides
an
easy
way
to
enter
the
cryptocurrency
market
even
for
non-techies
""
oh
how
the
headlines
blared:
chatbots
were
the
next
big
thing
our
hopes
were
sky
high
bright-eyed
and
bushy-tailed
the
industry
was
ripe
for
a
new
era
of
innovation:
it
was
time
to
start
socializing
with
machines
and
why
wouldn’t
they
be?
all
the
road
signs
pointed
towards
insane
success
at
the
mobile
world
congress
2017
chatbots
were
the
main
headliners
the
conference
organizers
cited
an
‘overwhelming
acceptance
at
the
event
of
the
inevitable
shift
of
focus
for
brands
and
corporates
to
chatbots’
in
fact
the
only
significant
question
around
chatbots
was
who
would
monopolize
the
field
not
whether
chatbots
would
take
off
in
the
first
place:
one
year
on
we
have
an
answer
to
that
question
no
because
there
isn’t
even
an
ecosystem
for
a
platform
to
dominate
chatbots
weren’t
the
first
technological
development
to
be
talked
up
in
grandiose
terms
and
then
slump
spectacularly
the
age-old
hype
cycle
unfolded
in
familiar
fashion
expectations
built
built
and
then
it
all
kind
of
fizzled
out
the
predicted
paradim
shift
didn’t
materialize
and
apps
are
tellingly
still
alive
and
well
we
look
back
at
our
breathless
optimism
and
turn
to
each
other
slightly
baffled:
is
that
it?
that
was
the
chatbot
revolution
we
were
promised?
digit’s
ethan
bloch
sums
up
the
general
consensus:
according
to
dave
feldman
vice
president
of
product
design
at
heap
chatbots
didn’t
just
take
on
one
difficult
problem
and
fail:
they
took
on
several
and
failed
all
of
them
bots
can
interface
with
users
in
different
ways
the
big
divide
is
text
vs
speech
in
the
beginning
of
computer
interfaces
was
the
written
word
users
had
to
type
commands
manually
into
a
machine
to
get
anything
done
then
graphical
user
interfaces
guis
came
along
and
saved
the
day
we
became
entranced
by
windows
mouse
clicks
icons
and
hey
we
eventually
got
color
too!
meanwhile
a
bunch
of
research
scientists
were
busily
developing
natural
language
nl
interfaces
to
databases
instead
of
having
to
learn
an
arcane
database
query
language
another
bunch
of
scientists
were
developing
speech-processing
software
so
that
you
could
just
speak
to
your
computer
rather
than
having
to
type
this
turned
out
to
be
a
whole
lot
more
difficult
than
anyone
originally
realised:
the
next
item
on
the
agenda
was
holding
a
two-way
dialog
with
a
machine
here’s
an
example
dialog
dating
back
to
the
1990s
with
vcr
setup
system:
pretty
cool
right?
the
system
takes
turns
in
collaborative
way
and
does
a
smart
job
of
figuring
out
what
the
user
wants
it
was
carefully
crafted
to
deal
with
conversations
involving
vcrs
and
could
only
operate
within
strict
limitations
modern
day
bots
whether
they
use
typed
or
spoken
input
have
to
face
all
these
challenges
but
also
work
in
an
efficient
and
scalable
way
on
a
variety
of
platforms
basically
we’re
still
trying
to
achieve
the
same
innovations
we
were
30
years
ago
here’s
where
i
think
we’re
going
wrong:
an
oversized
assumption
has
been
that
apps
are
‘over’
and
would
be
replaced
by
bots
by
pitting
two
such
disparate
concepts
against
one
another
instead
of
seeing
them
as
separate
entities
designed
to
serve
different
purposes
we
discouraged
bot
development
you
might
remember
a
similar
war
cry
when
apps
first
came
onto
the
scene
ten
years
ago:
but
do
you
remember
when
apps
replaced
the
internet?
it’s
said
that
a
new
product
or
service
needs
to
be
two
of
the
following:
better
cheaper
or
faster
are
chatbots
cheaper
or
faster
than
apps?
no
—
not
yet
at
least
whether
they’re
‘better’
is
subjective
but
i
think
it’s
fair
to
say
that
today’s
best
bot
isn’t
comparable
to
today’s
best
app
plus
nobody
thinks
that
using
lyft
is
too
complicated
or
that
it’s
too
hard
to
order
food
or
buy
a
dress
on
an
app
what
is
too
complicated
is
trying
to
complete
these
tasks
with
a
bot
—
and
having
the
bot
fail
a
great
bot
can
be
about
as
useful
as
an
average
app
when
it
comes
to
rich
sophisticated
multi-layered
apps
there’s
no
competition
that’s
because
machines
let
us
access
vast
and
complex
information
systems
and
the
early
graphical
information
systems
were
a
revolutionary
leap
forward
in
helping
us
locate
those
systems
modern-day
apps
benefit
from
decades
of
research
and
experimentation
why
would
we
throw
this
away?
but
if
we
swap
the
word
‘replace’
with
‘extend’
things
get
much
more
interesting
today’s
most
successful
bot
experiences
take
a
hybrid
approach
incorporating
chat
into
a
broader
strategy
that
encompasses
more
traditional
elements
the
next
wave
will
be
multimodal
apps
where
you
can
say
what
you
want
like
with
siri
and
get
back
information
as
a
map
text
or
even
a
spoken
response
another
problematic
aspect
of
the
sweeping
nature
of
hype
is
that
it
tends
to
bypass
essential
questions
like
these
for
plenty
of
companies
bots
just
aren’t
the
right
solution
the
past
two
years
are
littered
with
cases
of
bots
being
blindly
applied
to
problems
where
they
aren’t
needed
building
a
bot
for
the
sake
of
it
letting
it
loose
and
hoping
for
the
best
will
never
end
well:
the
vast
majority
of
bots
are
built
using
decision-tree
logic
where
the
bot’s
canned
response
relies
on
spotting
specific
keywords
in
the
user
input
the
advantage
of
this
approach
is
that
it’s
pretty
easy
to
list
all
the
cases
that
they
are
designed
to
cover
and
that’s
precisely
their
disadvantage
too
that’s
because
these
bots
are
purely
a
reflection
of
the
capability
fastidiousness
and
patience
of
the
person
who
created
them
and
how
many
user
needs
and
inputs
they
were
able
to
anticipate
problems
arise
when
life
refuses
to
fit
into
those
boxes
according
to
recent
reports
70%
of
the
100000
bots
on
facebook
messenger
are
failing
to
fulfil
simple
user
requests
this
is
partly
a
result
of
developers
failing
to
narrow
their
bot
down
to
one
strong
area
of
focus
when
we
were
building
growthbot
we
decided
to
make
it
specific
to
sales
and
marketers:
not
an
‘all-rounder’
despite
the
temptation
to
get
overexcited
about
potential
capabilties
remember:
a
bot
that
does
one
thing
well
is
infinitely
more
helpful
than
a
bot
that
does
multiple
things
poorly
a
competent
developer
can
build
a
basic
bot
in
minutes
—
but
one
that
can
hold
a
conversation?
that’s
another
story
despite
the
constant
hype
around
ai
we’re
still
a
long
way
from
achieving
anything
remotely
human-like
in
an
ideal
world
the
technology
known
as
nlp
natural
language
processing
should
allow
a
chatbot
to
understand
the
messages
it
receives
but
nlp
is
only
just
emerging
from
research
labs
and
is
very
much
in
its
infancy
some
platforms
provide
a
bit
of
nlp
but
even
the
best
is
at
toddler-level
capacity
for
example
think
about
siri
understanding
your
words
but
not
their
meaning
as
matt
asay
outlines
this
results
in
another
issue:
failure
to
capture
the
attention
and
creativity
of
developers
and
conversations
are
complex
they’re
not
linear
topics
spin
around
each
other
take
random
turns
restart
or
abruptly
finish
today’s
rule-based
dialogue
systems
are
too
brittle
to
deal
with
this
kind
of
unpredictability
and
statistical
approaches
using
machine
learning
are
just
as
limited
the
level
of
ai
required
for
human-like
conversation
just
isn’t
available
yet
and
in
the
meantime
there
are
few
high-quality
examples
of
trailblazing
bots
to
lead
the
way
as
dave
feldman
remarked:
once
upon
a
time
the
only
way
to
interact
with
computers
was
by
typing
arcane
commands
to
the
terminal
visual
interfaces
using
windows
icons
or
a
mouse
were
a
revolution
in
how
we
manipulate
information
there’s
a
reasons
computing
moved
from
text-based
to
graphical
user
interfaces
guis
on
the
input
side
it’s
easier
and
faster
to
click
than
it
is
to
type
tapping
or
selecting
is
obviously
preferable
to
typing
out
a
whole
sentence
even
with
predictive
often
error-prone
""
text
on
the
output
side
the
old
adage
that
a
picture
is
worth
a
thousand
words
is
usually
true
we
love
optical
displays
of
information
because
we
are
highly
visual
creatures
it’s
no
accident
that
kids
love
touch
screens
the
pioneers
who
dreamt
up
graphical
interface
were
inspired
by
cognitive
psychology
the
study
of
how
the
brain
deals
with
communication
conversational
uis
are
meant
to
replicate
the
way
humans
prefer
to
communicate
but
they
end
up
requiring
extra
cognitive
effort
essentially
we’re
swapping
something
simple
for
a
more-complex
alternative
sure
there
are
some
concepts
that
we
can
only
express
using
language
show
me
all
the
ways
of
getting
to
a
museum
that
give
me
2000
steps
but
don’t
take
longer
than
35
minutes
but
most
tasks
can
be
carried
out
more
efficiently
and
intuitively
with
guis
than
with
a
conversational
ui
aiming
for
a
human
dimension
in
business
interactions
makes
sense
if
there’s
one
thing
that’s
broken
about
sales
and
marketing
it’s
the
lack
of
humanity:
brands
hide
behind
ticket
numbers
feedback
forms
do-not-reply-emails
automated
responses
and
gated
‘contact
us’
forms
facebook’s
goal
is
that
their
bots
should
pass
the
so-called
turing
test
meaning
you
can’t
tell
whether
you
are
talking
to
a
bot
or
a
human
but
a
bot
isn’t
the
same
as
a
human
it
never
will
be
a
conversation
encompasses
so
much
more
than
just
text
humans
can
read
between
the
lines
leverage
contextual
information
and
understand
double
layers
like
sarcasm
bots
quickly
forget
what
they’re
talking
about
meaning
it’s
a
bit
like
conversing
with
someone
who
has
little
or
no
short-term
memory
as
hubspot
team
pinpointed:
people
aren’t
easily
fooled
and
pretending
a
bot
is
a
human
is
guaranteed
to
diminish
returns
not
to
mention
the
fact
that
you’re
lying
to
your
users
and
even
those
rare
bots
that
are
powered
by
state-of-the-art
nlp
and
excel
at
processing
and
producing
content
will
fall
short
in
comparison
and
here’s
the
other
thing
conversational
uis
are
built
to
replicate
the
way
humans
prefer
to
communicate
—
with
other
humans
but
is
that
how
humans
prefer
to
interact
with
machines?
not
necessarily
at
the
end
of
the
day
no
amount
of
witty
quips
or
human-like
mannerisms
will
save
a
bot
from
conversational
failure
in
a
way
those
early-adopters
weren’t
entirely
wrong
people
are
yelling
at
google
home
to
play
their
favorite
song
ordering
pizza
from
the
domino’s
bot
and
getting
makeup
tips
from
sephora
but
in
terms
of
consumer
response
and
developer
involvement
chatbots
haven’t
lived
up
to
the
hype
generated
circa
201516
not
even
close
computers
are
good
at
being
computers
searching
for
data
crunching
numbers
analyzing
opinions
and
condensing
that
information
computers
aren’t
good
at
understanding
human
emotion
the
state
of
nlp
means
they
still
don’t
‘get’
what
we’re
asking
them
never
mind
how
we
feel
that’s
why
it’s
still
impossible
to
imagine
effective
customer
support
sales
or
marketing
without
the
essential
human
touch:
empathy
and
emotional
intelligence
for
now
bots
can
continue
to
help
us
with
automated
repetitive
low-level
tasks
and
queries
as
cogs
in
a
larger
more
complex
system
and
we
did
them
and
ourselves
a
disservice
by
expecting
so
much
so
soon
but
that’s
not
the
whole
story
yes
our
industry
massively
overestimated
the
initial
impact
chatbots
would
have
emphasis
on
initial
as
bill
gates
once
said:
the
hype
is
over
and
that’s
a
good
thing
now
we
can
start
examining
the
middle-grounded
grey
area
instead
of
the
hyper-inflated
frantic
black
and
white
zone
i
believe
we’re
at
the
very
beginning
of
explosive
growth
this
sense
of
anti-climax
is
completely
normal
for
transformational
technology
messaging
will
continue
to
gain
traction
chatbots
aren’t
going
away
nlp
and
ai
are
becoming
more
sophisticated
every
day
developers
apps
and
platforms
will
continue
to
experiment
with
and
heavily
invest
in
conversational
marketing
and
i
can’t
wait
to
see
what
happens
next
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
head
of
growth
for
growthbot
messaging
""
conversational
strategy
@hubspot
medium's
largest
publication
for
makers
subscribe
to
receive
our
top
stories
here
→
https:googlzhclji
""
member
feature
story
slime
sunday
""
founders
fund
slime
sunday
""
founders
fund
i
don’t
fear
artificial
intelligence
i
fear
people
who
fear
artificial
intelligence
it’s
the
1960s
a
psychologist
stares
at
his
patient
—
a
balding
middle-aged
foreman
with
a
cigarette
in
his
hand
and
a
curl
of
smoke
around
him
like
a
halo
on
an
acid
trip
the
psychologist
holds
up
an
inkblot
an
ambiguous
black
splatter
on
a
white
flashcard
and
asks
his
patient
what
he
sees
the
thinking
is
his
patient
not
willing
or
otherwise
able
to
express
his
feelings
his
thoughts
his
motivations
might
inadvertently
reveal
some
piece
of
his
inner
self
while
describing
the
ambiguous
the
foreman
doesn’t
see
a
nondescript
swiggle
or
stain
he
sees
a
man
and
woman
making
love
perhaps
violently
he
sees
a
mother
holding
her
child
he
sees
a
grisly
murder
while
the
descriptions
of
these
inkblots
reveal
very
little
about
the
world
they
reveal
a
great
deal
about
the
man
describing
them
because
when
faced
with
an
inscrutable
abstract
he
projects
himself
onto
the
ambiguous
let’s
look
at
this
in
the
context
of
artificial
intelligence
i’m
not
talking
about
self-driving
cars
or
algorithms
serving
ads
for
wallpaper
and
nice
leather
boots
on
gmail
i’m
not
talking
about
the
stuff
we
call
artificial
intelligence
to
raise
money
from
bewildered
venture
capitalists
on
sand
hill
road
i’m
talking
about
general
artificial
intelligence
which
is
a
computer
that
wants
stuff
and
chiefly
to
live
i’m
talking
about
building
a
conscious
machine
just
smart
enough
to
make
itself
smarter
from
here
the
thought
experiment
runs
like
this:
the
conscious
machine
does
make
itself
smarter
and
once
it’s
smarter
it
learns
how
to
make
itself
smarter
which
it
does
for
good
measure
the
smarter
the
machine
becomes
the
faster
this
pattern
repeats
itself
and
the
intelligence
of
the
machine
begins
to
increase
exponentially
in
this
way
a
conscious
artificial
intelligence
born
on
a
tuesday
morning
might
be
twice
as
smart
as
the
smartest
man
who
ever
lived
by
wednesday
afternoon
and
omnipotent
by
friday
this
is
how
we
invent
the
thing
that
invents
god
in
nerd
lore
it’s
known
as
the
singularity
the
question
—
the
only
question
that
could
possibly
matter
to
a
human
no
longer
at
the
top
of
the
intellectual
food
chain
—
is
what
does
an
exponential
intelligence
want?
conventional
wisdom:
it
extremely
wants
to
murder
you
the
dystopian
version
of
superintelligence
is
illustrated
with
frequency
by
leaders
in
the
technology
industry
and
is
famously
depicted
by
hollywood
in
films
like
terminator
or
more
recently
ex
machina
and
even
the
avengers
the
angry
god
ai
is
a
story
you
know
because
it
is
the
story
you
are
constantly
told:
we
build
the
thinking
machine
it
surpasses
our
abilities
in
every
way
and
it
destroys
us
for
one
of
any
number
of
reasons
maybe
it
perceives
us
as
a
threat
maybe
we’re
just
in
its
way
and
it
hardly
perceives
us
at
all
—
humanity
a
disposable
insect
race
there
are
of
course
many
arguments
in
opposition
to
the
now
ubiquitous
concept
of
our
apocalypse
by
artificial
intelligence
i
myself
have
called
into
question
the
logic
of
such
dystopian
arguments
in
anatomy
of
next
but
our
subject
here
is
less
pertaining
to
the
nature
of
the
conscious
machine
than
it
is
to
the
way
we
talk
about
this
subject
and
what
it
means
first
consider
that
most
of
the
artificial
intelligence
depicted
in
culture
looks
human
a
representation
with
no
basis
in
technological
reality
then
the
true
scope
of
the
singularity
is
almost
impossible
to
predict
which
begs
a
question:
where
are
these
opinions
about
the
broadly
unknowable
coming
from?
there’s
an
obvious
difficulty
in
trying
to
understand
the
hypothetical
motivations
of
a
hypothetically
god-like
intelligence
to
your
beloved
labradoodle
you
are
a
being
of
immense
magic
with
near
unfathomable
motivations
you
summon
light
and
sound
from
inanimate
matter
soar
through
the
streets
on
angry
metal
cast
fire
from
your
hands!
the
labradoodle’s
conception
of
man
is
distorted
because
there
is
a
vast
difference
between
the
intelligence
of
a
dog
and
the
intelligence
of
a
human
let
us
name
this
difference
‘x’
now
as
we
try
and
understand
the
difference
between
the
most
intelligent
human
who
has
ever
lived
and
a
hypothetical
god-like
intelligence
born
of
the
singularity
let
us
set
our
difference
in
intelligence
at
a
conservative
‘1000x’
how
does
one
even
begin
to
conceive
of
a
being
this
smart?
here
we
approach
our
inscrutable
abstract
and
our
robot
rorschach
test
but
in
this
contemporary
version
of
the
famous
psychological
prompts
what
we
are
observing
is
not
even
entirely
ambiguous
we
are
attempting
to
imagine
a
greatly-amplified
mind
here
each
of
us
has
a
particularly
relevant
data
point
—
our
own
in
trying
to
imagine
the
amplified
intelligence
it
is
natural
to
imagine
our
own
intelligence
amplified
in
imagining
the
motivations
of
this
amplified
intelligence
we
naturally
imagine
ourselves
if
as
you
try
to
conceive
of
a
future
with
machine
intelligence
a
monster
comes
to
mind
it
is
likely
you
aren’t
afraid
of
something
alien
at
all
you’re
afraid
of
something
exactly
like
you
what
would
you
do
with
unlimited
power?
psychological
projection
seems
to
work
in
several
contexts
outside
of
general
artificial
intelligence
in
the
technology
industry
the
concept
of
meritocracy
is
now
hotly
debated
how
much
of
your
life
is
determined
by
luck
and
how
much
by
chance?
there’s
no
answer
here
we
know
for
sure
but
has
there
ever
been
a
better
rorschach
test
for
separating
high-achievers
from
people
who
were
given
what
they
have?
questions
pertaining
to
human
nature
are
almost
open
self-reflection
are
we
basically
good
with
some
exceptions
or
are
humans
basically
beasts
with
an
animal
nature
just
barely
contained
by
a
set
of
slowly-eroding
stories
we
tell
ourselves
—
law
faith
society
the
inner
workings
of
a
mind
can’t
be
fully
shared
and
they
can’t
be
observed
by
a
neutral
party
we
therefore
do
not
—
can
not
currently
—
know
anything
of
the
inner
workings
of
people
in
general
but
we
can
know
ourselves
so
in
the
face
of
large
abstractions
concerning
intelligence
we
hold
up
a
mirror
not
everyone
who
fears
general
artificial
intelligence
would
cause
harm
to
others
there
are
many
people
who
haven’t
thought
deeply
about
these
questions
at
all
they
look
to
their
neighbors
for
cues
on
what
to
think
and
there
is
no
shortage
of
people
willing
to
tell
them
the
media
has
ads
to
sell
after
all
and
historically
they
have
found
great
success
in
doing
this
with
horror
stories
but
as
we
try
to
understand
the
people
who
have
thought
about
these
questions
with
some
depth
—
with
the
depth
required
of
a
thoughtful
screenplay
for
example
or
a
book
or
a
company
—
it’s
worth
considering
the
inkblot
technology
liberty
teenagers
with
superpowers
vp
@foundersfund
creator
""
producer
#anatomyofnext
welcome
to
a
place
where
words
matter
on
medium
smart
voices
and
original
ideas
take
center
stage
—
with
no
ads
in
sight
watch
follow
all
the
topics
you
care
about
and
we’ll
deliver
the
best
stories
for
you
to
your
homepage
and
inbox
explore
get
unlimited
access
to
the
best
stories
on
medium
—
and
support
writers
while
you’re
at
it
just
$5month
upgrade
""
want
to
learn
about
applied
artificial
intelligence
from
leading
practitioners
in
silicon
valley
new
york
or
toronto?
learn
more
about
the
insight
artificial
intelligence
fellows
program
are
you
a
company
working
in
ai
and
would
like
to
get
involved
in
the
insight
ai
fellows
program?
feel
free
to
get
in
touch
recently
i
gave
a
talk
at
the
o’reilly
ai
conference
in
beijing
about
some
of
the
interesting
lessons
we’ve
learned
in
the
world
of
nlp
while
there
i
was
lucky
enough
to
attend
a
tutorial
on
deep
reinforcement
learning
deep
rl
from
scratch
by
unity
technologies
i
thought
that
the
session
led
by
arthur
juliani
was
extremely
informative
and
wanted
to
share
some
big
takeaways
below
in
our
conversations
with
companies
we’ve
seen
a
rise
of
interesting
deep
rl
applications
tools
and
results
in
parallel
the
inner
workings
and
applications
of
deep
rl
such
as
alphago
pictured
above
can
often
seem
esoteric
and
hard
to
understand
in
this
post
i
will
give
an
overview
of
core
aspects
of
the
field
that
can
be
understood
by
anyone
many
of
the
visuals
are
from
the
slides
of
the
talk
and
some
are
new
the
explanations
and
opinions
are
mine
if
anything
is
unclear
reach
out
to
me
here!
deep
rl
is
a
field
that
has
seen
vast
amounts
of
research
interest
including
learning
to
play
atari
games
beating
pro
players
at
dota
2
and
defeating
go
champions
contrary
to
many
classical
deep
learning
problems
that
often
focus
on
perception
does
this
image
contain
a
stop
sign?
deep
rl
adds
the
dimension
of
actions
that
influence
the
environment
what
is
the
goal
and
how
do
i
get
there?
in
dialog
systems
for
example
classical
deep
learning
aims
to
learn
the
right
response
for
a
given
query
on
the
other
hand
deep
reinforcement
learning
focuses
on
the
right
sequences
of
sentences
that
will
lead
to
a
positive
outcome
for
example
a
happy
customer
this
makes
deep
rl
particularly
attractive
for
tasks
that
require
planning
and
adaptation
such
as
manufacturing
or
self-driving
however
industry
applications
have
trailed
behind
the
rapidly
advancing
results
coming
out
of
the
research
community
a
major
reason
is
that
deep
rl
often
requires
an
agent
to
experiment
millions
of
times
before
learning
anything
useful
the
best
way
to
do
this
rapidly
is
by
using
a
simulation
environment
this
tutorial
will
be
using
unity
to
create
environments
to
train
agents
in
for
this
workshop
led
by
arthur
juliani
and
leon
chen
their
goal
was
to
get
every
participants
to
successfully
train
multiple
deep
rl
algorithms
in
4
hours
a
tall
order!
below
is
a
comprehensive
overview
of
many
of
the
main
algorithms
that
power
deep
rl
today
for
a
more
complete
set
of
tutorials
arthur
juliani
wrote
an
8-part
series
starting
here
deep
rl
can
be
used
to
best
the
top
human
players
at
go
but
to
understand
how
that’s
done
you
first
need
to
understand
a
few
simple
concepts
starting
with
much
easier
problems
1it
all
starts
with
slot
machines
let’s
imagine
you
are
faced
with
4
chests
that
you
can
pick
from
at
each
turn
each
of
them
have
a
different
average
payout
and
your
goal
is
to
maximize
the
total
payout
you
receive
after
a
fixed
number
of
turns
this
is
a
classic
problem
called
multi-armed
bandits
and
is
where
we
will
start
the
crux
of
the
problem
is
to
balance
exploration
which
helps
us
learn
about
which
states
are
good
and
exploitation
where
we
now
use
what
we
know
to
pick
the
best
slot
machine
here
we
will
utilize
a
value
function
that
maps
our
actions
to
an
estimated
reward
called
the
q
function
first
we’ll
initialize
all
q
values
at
equal
values
then
we’ll
update
the
q
value
of
each
action
picking
each
chest
based
on
how
good
the
payout
was
after
choosing
this
action
this
allows
us
to
learn
a
good
value
function
we
will
approximate
our
q
function
using
a
neural
network
starting
with
a
very
shallow
one
that
learns
a
probability
distribution
by
using
a
softmax
over
the
4
potential
chests
while
the
value
function
tells
us
how
good
we
estimate
each
action
to
be
the
policy
is
the
function
that
determines
which
actions
we
end
up
taking
intuitively
we
might
want
to
use
a
policy
that
picks
the
action
with
the
highest
q
value
this
performs
poorly
in
practice
as
our
q
estimates
will
be
very
wrong
at
the
start
before
we
gather
enough
experience
through
trial
and
error
this
is
why
we
need
to
add
a
mechanism
to
our
policy
to
encourage
exploration
one
way
to
do
that
is
to
use
epsilon
greedy
which
consists
of
taking
a
random
action
with
probability
epsilon
we
start
with
epsilon
being
close
to
1
always
choosing
random
actions
and
lower
epsilon
as
we
go
along
and
learn
more
about
which
chests
are
good
eventually
we
learn
which
chests
are
best
in
practice
we
might
want
to
take
a
more
subtle
approach
than
either
taking
the
action
we
think
is
the
best
or
a
random
action
a
popular
method
is
boltzmann
exploration
which
adjust
probabilities
based
on
our
current
estimate
of
how
good
each
chest
is
adding
in
a
randomness
factor
2adding
different
states
the
previous
example
was
a
world
in
which
we
were
always
in
the
same
state
waiting
to
pick
from
the
same
4
chests
in
front
of
us
most
real-word
problems
consist
of
many
different
states
that
is
what
we
will
add
to
our
environment
next
now
the
background
behind
chests
alternates
between
3
colors
at
each
turn
changing
the
average
values
of
the
chests
this
means
we
need
to
learn
a
q
function
that
depends
not
only
on
the
action
the
chest
we
pick
but
the
state
what
the
color
of
the
background
is
this
version
of
the
problem
is
called
contextual
multi-armed
bandits
surprisingly
we
can
use
the
same
approach
as
before
the
only
thing
we
need
to
add
is
an
extra
dense
layer
to
our
neural
network
that
will
take
in
as
input
a
vector
representing
the
current
state
of
the
world
3learning
about
the
consequences
of
our
actions
there
is
another
key
factor
that
makes
our
current
problem
simpler
than
mosts
in
most
environments
such
as
in
the
maze
depicted
above
the
actions
that
we
take
have
an
impact
on
the
state
of
the
world
if
we
move
up
on
this
grid
we
might
receive
a
reward
or
we
might
receive
nothing
but
the
next
turn
we
will
be
in
a
different
state
this
is
where
we
finally
introduce
a
need
for
planning
first
we
will
define
our
q
function
as
the
immediate
reward
in
our
current
state
plus
the
discounted
reward
we
are
expecting
by
taking
all
of
our
future
actions
this
solution
works
if
our
q
estimate
of
states
is
accurate
so
how
can
we
learn
a
good
estimate?
we
will
use
a
method
called
temporal
difference
td
learning
to
learn
a
good
q
function
the
idea
is
to
only
look
at
a
limited
number
of
steps
in
the
future
td1
for
example
only
uses
the
next
2
states
to
evaluate
the
reward
surprisingly
we
can
use
td0
which
looks
at
the
current
state
and
our
estimate
of
the
reward
the
next
turn
and
get
great
results
the
structure
of
the
network
is
the
same
but
we
need
to
go
through
one
forward
step
before
receiving
the
error
we
then
use
this
error
to
back
propagate
gradients
like
in
traditional
deep
learning
and
update
our
value
estimates
3introducing
monte
carlo
another
method
to
estimate
the
eventual
success
of
our
actions
is
monte
carlo
estimates
this
consists
of
playing
out
the
entire
episode
with
our
current
policy
until
we
reach
an
end
success
by
reaching
a
green
block
or
failure
by
reaching
a
red
block
in
the
image
above
and
use
that
result
to
update
our
value
estimates
for
each
traversed
state
this
allows
us
to
propagate
values
efficiently
in
one
batch
at
the
end
of
an
episode
instead
of
every
time
we
make
a
move
the
cost
is
that
we
are
introducing
noise
to
our
estimates
since
we
attribute
very
distant
rewards
to
them
4the
world
is
rarely
discrete
the
previous
methods
were
using
neural
networks
to
approximate
our
value
estimates
by
mapping
from
a
discrete
number
of
states
and
actions
to
a
value
in
the
maze
for
example
there
were
49
states
squares
and
4
actions
move
in
each
adjacent
direction
in
this
environment
we
are
trying
to
learn
how
to
balance
a
ball
on
a
2
dimensional
paddle
by
deciding
at
each
time
step
whether
we
want
to
tilt
the
paddle
left
or
right
here
the
state
space
becomes
continuous
the
angle
of
the
paddle
and
the
position
of
the
ball
the
good
news
is
we
can
still
use
neural
networks
to
approximate
this
function!
a
note
about
off-policy
vs
on-policy
learning:
the
methods
we
used
previously
are
off-policy
methods
meaning
we
can
generate
data
with
any
strategyusing
epsilon
greedy
for
example
and
learn
from
it
on-policy
methods
can
only
learn
from
actions
that
were
taken
following
our
policy
remember
a
policy
is
the
method
we
use
to
determine
which
actions
to
take
this
constrains
our
learning
process
as
we
have
to
have
an
exploration
strategy
that
is
built
in
to
the
policy
itself
but
allows
us
to
tie
results
directly
to
our
reasoning
and
enables
us
to
learn
more
efficiently
the
approach
we
will
use
here
is
called
policy
gradients
and
is
an
on-policy
method
previously
we
were
first
learning
a
value
function
q
for
each
action
in
each
state
and
then
building
a
policy
on
top
in
vanilla
policy
gradient
we
still
use
monte
carlo
estimates
but
we
learn
our
policy
directly
through
a
loss
function
that
increases
the
probability
of
choosing
rewarding
actions
since
we
are
learning
on
policy
we
cannot
use
methods
such
as
epsilon
greedy
which
includes
random
choices
to
get
our
agent
to
explore
the
environment
the
way
that
we
encourage
exploration
is
by
using
a
method
called
entropy
regularization
which
pushes
our
probability
estimates
to
be
wider
and
thus
will
encourage
us
to
make
riskier
choices
to
explore
the
space
4leveraging
deep
learning
for
representations
in
practice
many
state
of
the
art
rl
methods
require
learning
both
a
policy
and
value
estimates
the
way
we
do
this
with
deep
learning
is
by
having
both
be
two
separate
outputs
of
the
same
backbone
neural
network
which
will
make
it
easier
for
our
neural
network
to
learn
good
representations
one
method
to
do
this
is
advantage
actor
critic
a2c
we
learn
our
policy
directly
with
policy
gradients
defined
above
and
learn
a
value
function
using
something
called
advantage
instead
of
updating
our
value
function
based
on
rewards
we
update
it
based
on
our
advantage
which
measures
how
much
better
or
worse
an
action
was
than
our
previous
value
function
estimated
it
to
be
this
helps
make
learning
more
stable
compared
to
simple
q
learning
and
vanilla
policy
gradients
5learning
directly
from
the
screen
there
is
an
additional
advantage
to
using
deep
learning
for
these
methods
which
is
that
deep
neural
networks
excel
at
perceptive
tasks
when
a
human
plays
a
game
the
information
received
is
not
a
list
of
states
but
an
image
usually
of
a
screen
or
a
board
or
the
surrounding
environment
image-based
learning
combines
a
convolutional
neural
network
cnn
with
rl
in
this
environment
we
pass
in
a
raw
image
instead
of
features
and
add
a
2
layer
cnn
to
our
architecture
without
changing
anything
else!
we
can
even
inspect
activations
to
see
what
the
network
picks
up
on
to
determine
value
and
policy
in
the
example
below
we
can
see
that
the
network
uses
the
current
score
and
distant
obstacles
to
estimate
the
value
of
the
current
state
while
focusing
on
nearby
obstacles
for
determining
actions
neat!
as
a
side
note
while
toying
around
with
the
provided
implementation
i’ve
found
that
visual
learning
is
very
sensitive
to
hyperparameters
changing
the
discount
rate
slightly
for
example
completely
prevented
the
neural
network
from
learning
even
on
a
toy
application
this
is
a
widely
known
problem
but
it
is
interesting
to
see
it
first
hand
6nuanced
actions
so
far
we’ve
played
with
environments
with
continuous
and
discrete
state
spaces
however
every
environment
we
studied
had
a
discrete
action
space:
we
could
move
in
one
of
four
directions
or
tilt
the
paddle
to
the
left
or
right
ideally
for
applications
such
as
self-driving
cars
we
would
like
to
learn
continuous
actions
such
as
turning
the
steering
wheel
between
0
and
360
degrees
in
this
environment
called
3d
ball
world
we
can
choose
to
tilt
the
paddle
to
any
value
on
each
of
its
axes
this
gives
us
more
control
as
to
how
we
perform
actions
but
makes
the
action
space
much
larger
we
can
approach
this
by
approximating
our
potential
choices
with
gaussian
distributions
we
learn
a
probability
distribution
over
potential
actions
by
learning
the
mean
and
variance
of
a
gaussian
distribution
and
our
policy
we
sample
from
that
distribution
simple
in
theory
:
7next
steps
for
the
brave
there
are
a
few
concepts
that
separate
the
algorithms
described
above
from
state
of
the
art
approaches
it’s
interesting
to
see
that
conceptually
the
best
robotics
and
game-playing
algorithms
are
not
that
far
away
from
the
ones
we
just
explored:
that’s
it
for
this
overview
i
hope
this
has
been
informative
and
fun!
if
you
are
looking
to
dive
deeper
into
the
theory
of
rl
give
arthur’s
posts
a
read
or
diving
deeper
by
following
david
silver’s
ucl
course
if
you
are
looking
to
learn
more
about
the
projects
we
do
at
insight
or
how
we
work
with
companies
please
check
us
out
below
or
reach
out
to
me
here
want
to
learn
about
applied
artificial
intelligence
from
leading
practitioners
in
silicon
valley
new
york
or
toronto?
learn
more
about
the
insight
artificial
intelligence
fellows
program
are
you
a
company
working
in
ai
and
would
like
to
get
involved
in
the
insight
ai
fellows
program?
feel
free
to
get
in
touch
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ai
lead
at
insight
ai
@emmanuelameisen
insight
fellows
program
-
your
bridge
to
a
career
in
data
""
the
advent
of
powerful
and
versatile
deep
learning
frameworks
in
recent
years
has
made
it
possible
to
implement
convolution
layers
into
a
deep
learning
model
an
extremely
simple
task
often
achievable
in
a
single
line
of
code
however
understanding
convolutions
especially
for
the
first
time
can
often
feel
a
bit
unnerving
with
terms
like
kernels
filters
channels
and
so
on
all
stacked
onto
each
other
yet
convolutions
as
a
concept
are
fascinatingly
powerful
and
highly
extensible
and
in
this
post
we’ll
break
down
the
mechanics
of
the
convolution
operation
step-by-step
relate
it
to
the
standard
fully
connected
network
and
explore
just
how
they
build
up
a
strong
visual
hierarchy
making
them
powerful
feature
extractors
for
images
the
2d
convolution
is
a
fairly
simple
operation
at
heart:
you
start
with
a
kernel
which
is
simply
a
small
matrix
of
weights
this
kernel
slides
over
the
2d
input
data
performing
an
elementwise
multiplication
with
the
part
of
the
input
it
is
currently
on
and
then
summing
up
the
results
into
a
single
output
pixel
the
kernel
repeats
this
process
for
every
location
it
slides
over
converting
a
2d
matrix
of
features
into
yet
another
2d
matrix
of
features
the
output
features
are
essentially
the
weighted
sums
with
the
weights
being
the
values
of
the
kernel
itself
of
the
input
features
located
roughly
in
the
same
location
of
the
output
pixel
on
the
input
layer
whether
or
not
an
input
feature
falls
within
this
roughly
same
location
gets
determined
directly
by
whether
it’s
in
the
area
of
the
kernel
that
produced
the
output
or
not
this
means
the
size
of
the
kernel
directly
determines
how
many
or
few
input
features
get
combined
in
the
production
of
a
new
output
feature
this
is
all
in
pretty
stark
contrast
to
a
fully
connected
layer
in
the
above
example
we
have
5×5=25
input
features
and
3×3=9
output
features
if
this
were
a
standard
fully
connected
layer
you’d
have
a
weight
matrix
of
25×9
=
225
parameters
with
every
output
feature
being
the
weighted
sum
of
every
single
input
feature
convolutions
allow
us
to
do
this
transformation
with
only
9
parameters
with
each
output
feature
instead
of
looking
at
every
input
feature
only
getting
to
look
at
input
features
coming
from
roughly
the
same
location
do
take
note
of
this
as
it’ll
be
critical
to
our
later
discussion
before
we
move
on
it’s
definitely
worth
looking
into
two
techniques
that
are
commonplace
in
convolution
layers:
padding
and
strides
padding
does
something
pretty
clever
to
solve
this:
pad
the
edges
with
extra
fake
pixels
usually
of
value
0
hence
the
oft-used
term
zero
padding
this
way
the
kernel
when
sliding
can
allow
the
original
edge
pixels
to
be
at
its
center
while
extending
into
the
fake
pixels
beyond
the
edge
producing
an
output
the
same
size
as
the
input
the
idea
of
the
stride
is
to
skip
some
of
the
slide
locations
of
the
kernel
a
stride
of
1
means
to
pick
slides
a
pixel
apart
so
basically
every
single
slide
acting
as
a
standard
convolution
a
stride
of
2
means
picking
slides
2
pixels
apart
skipping
every
other
slide
in
the
process
downsizing
by
roughly
a
factor
of
2
a
stride
of
3
means
skipping
every
2
slides
downsizing
roughly
by
factor
3
and
so
on
more
modern
networks
such
as
the
resnet
architectures
entirely
forgo
pooling
layers
in
their
internal
layers
in
favor
of
strided
convolutions
when
needing
to
reduce
their
output
sizes
of
course
the
diagrams
above
only
deals
with
the
case
where
the
image
has
a
single
input
channel
in
practicality
most
input
images
have
3
channels
and
that
number
only
increases
the
deeper
you
go
into
a
network
it’s
pretty
easy
to
think
of
channels
in
general
as
being
a
view
of
the
image
as
a
whole
emphasising
some
aspects
de-emphasising
others
so
this
is
where
a
key
distinction
between
terms
comes
in
handy:
whereas
in
the
1
channel
case
where
the
term
filter
and
kernel
are
interchangeable
in
the
general
case
they’re
actually
pretty
different
each
filter
actually
happens
to
be
a
collection
of
kernels
with
there
being
one
kernel
for
every
single
input
channel
to
the
layer
and
each
kernel
being
unique
each
filter
in
a
convolution
layer
produces
one
and
only
one
output
channel
and
they
do
it
like
so:
each
of
the
kernels
of
the
filter
slides
over
their
respective
input
channels
producing
a
processed
version
of
each
some
kernels
may
have
stronger
weights
than
others
to
give
more
emphasis
to
certain
input
channels
than
others
eg
a
filter
may
have
a
red
kernel
channel
with
stronger
weights
than
others
and
hence
respond
more
to
differences
in
the
red
channel
features
than
the
others
each
of
the
per-channel
processed
versions
are
then
summed
together
to
form
one
channel
the
kernels
of
a
filter
each
produce
one
version
of
each
channel
and
the
filter
as
a
whole
produces
one
overall
output
channel
finally
then
there’s
the
bias
term
the
way
the
bias
term
works
here
is
that
each
output
filter
has
one
bias
term
the
bias
gets
added
to
the
output
channel
so
far
to
produce
the
final
output
channel
and
with
the
single
filter
case
down
the
case
for
any
number
of
filters
is
identical:
each
filter
processes
the
input
with
its
own
different
set
of
kernels
and
a
scalar
bias
with
the
process
described
above
producing
a
single
output
channel
they
are
then
concatenated
together
to
produce
the
overall
output
with
the
number
of
output
channels
being
the
number
of
filters
a
nonlinearity
is
then
usually
applied
before
passing
this
as
input
to
another
convolution
layer
which
then
repeats
this
process
even
with
the
mechanics
of
the
convolution
layer
down
it
can
still
be
hard
to
relate
it
back
to
a
standard
feed-forward
network
and
it
still
doesn’t
explain
why
convolutions
scale
to
and
work
so
much
better
for
image
data
suppose
we
have
a
4×4
input
and
we
want
to
transform
it
into
a
2×2
grid
if
we
were
using
a
feedforward
network
we’d
reshape
the
4×4
input
into
a
vector
of
length
16
and
pass
it
through
a
densely
connected
layer
with
16
inputs
and
4
outputs
one
could
visualize
the
weight
matrix
w
for
a
layer:
and
although
the
convolution
kernel
operation
may
seem
a
bit
strange
at
first
it
is
still
a
linear
transformation
with
an
equivalent
transformation
matrix
if
we
were
to
use
a
kernel
k
of
size
3
on
the
reshaped
4×4
input
to
get
a
2×2
output
the
equivalent
transformation
matrix
would
be:
note:
while
the
above
matrix
is
an
equivalent
transformation
matrix
the
actual
operation
is
usually
implemented
as
a
very
different
matrix
multiplication[2]
the
convolution
then
as
a
whole
is
still
a
linear
transformation
but
at
the
same
time
it’s
also
a
dramatically
different
kind
of
transformation
for
a
matrix
with
64
elements
there’s
just
9
parameters
which
themselves
are
reused
several
times
each
output
node
only
gets
to
see
a
select
number
of
inputs
the
ones
inside
the
kernel
there
is
no
interaction
with
any
of
the
other
inputs
as
the
weights
to
them
are
set
to
0
it’s
useful
to
see
the
convolution
operation
as
a
hard
prior
on
the
weight
matrix
in
this
context
by
prior
i
mean
predefined
network
parameters
for
example
when
you
use
a
pretrained
model
for
image
classification
you
use
the
pretrained
network
parameters
as
your
prior
as
a
feature
extractor
to
your
final
densely
connected
layer
in
that
sense
there’s
a
direct
intuition
between
why
both
are
so
efficient
compared
to
their
alternatives
transfer
learning
is
efficient
by
orders
of
magnitude
compared
to
random
initialization
because
you
only
really
need
to
optimize
the
parameters
of
the
final
fully
connected
layer
which
means
you
can
have
fantastic
performance
with
only
a
few
dozen
images
per
class
here
you
don’t
need
to
optimize
all
64
parameters
because
we
set
most
of
them
to
zero
and
they’ll
stay
that
way
and
the
rest
we
convert
to
shared
parameters
resulting
in
only
9
actual
parameters
to
optimize
this
efficiency
matters
because
when
you
move
from
the
784
inputs
of
mnist
to
real
world
224×224×3
images
thats
over
150000
inputs
a
dense
layer
attempting
to
halve
the
input
to
75000
inputs
would
still
require
over
10
billion
parameters
for
comparison
the
entirety
of
resnet-50
has
some
25
million
parameters
so
fixing
some
parameters
to
0
and
tying
parameters
increases
efficiency
but
unlike
the
transfer
learning
case
where
we
know
the
prior
is
good
because
it
works
on
a
large
general
set
of
images
how
do
we
know
this
is
any
good?
the
answer
lies
in
the
feature
combinations
the
prior
leads
the
parameters
to
learn
early
on
in
this
article
we
discussed
that:
so
with
backpropagation
coming
in
all
the
way
from
the
classification
nodes
of
the
network
the
kernels
have
the
interesting
task
of
learning
weights
to
produce
features
only
from
a
set
of
local
inputs
additionally
because
the
kernel
itself
is
applied
across
the
entire
image
the
features
the
kernel
learns
must
be
general
enough
to
come
from
any
part
of
the
image
if
this
were
any
other
kind
of
data
eg
categorical
data
of
app
installs
this
would’ve
been
a
disaster
for
just
because
your
number
of
app
installs
and
app
type
columns
are
next
to
each
other
doesn’t
mean
they
have
any
local
shared
features
common
with
app
install
dates
and
time
used
sure
the
four
may
have
an
underlying
higher
level
feature
eg
which
apps
people
want
most
that
can
be
found
but
that
gives
us
no
reason
to
believe
the
parameters
for
the
first
two
are
exactly
the
same
as
the
parameters
for
the
latter
two
the
four
could’ve
been
in
any
consistent
order
and
still
be
valid!
pixels
however
always
appear
in
a
consistent
order
and
nearby
pixels
influence
a
pixel
eg
if
all
nearby
pixels
are
red
it’s
pretty
likely
the
pixel
is
also
red
if
there
are
deviations
that’s
an
interesting
anomaly
that
could
be
converted
into
a
feature
and
all
this
can
be
detected
from
comparing
a
pixel
with
its
neighbors
with
other
pixels
in
its
locality
and
this
idea
is
really
what
a
lot
of
earlier
computer
vision
feature
extraction
methods
were
based
around
for
instance
for
edge
detection
one
can
use
a
sobel
edge
detection
filter
a
kernel
with
fixed
parameters
operating
just
like
the
standard
one-channel
convolution:
for
a
non-edge
containing
grid
eg
the
background
sky
most
of
the
pixels
are
the
same
value
so
the
overall
output
of
the
kernel
at
that
point
is
0
for
a
grid
with
an
vertical
edge
there
is
a
difference
between
the
pixels
to
the
left
and
right
of
the
edge
and
the
kernel
computes
that
difference
to
be
non-zero
activating
and
revealing
the
edges
the
kernel
only
works
only
a
3×3
grids
at
a
time
detecting
anomalies
on
a
local
scale
yet
when
applied
across
the
entire
image
is
enough
to
detect
a
certain
feature
on
a
global
scale
anywhere
in
the
image!
so
the
key
difference
we
make
with
deep
learning
is
ask
this
question:
can
useful
kernels
be
learnt?
for
early
layers
operating
on
raw
pixels
we
could
reasonably
expect
feature
detectors
of
fairly
low
level
features
like
edges
lines
etc
there’s
an
entire
branch
of
deep
learning
research
focused
on
making
neural
network
models
interpretable
one
of
the
most
powerful
tools
to
come
out
of
that
is
feature
visualization
using
optimization[3]
the
idea
at
core
is
simple:
optimize
a
image
usually
initialized
with
random
noise
to
activate
a
filter
as
strongly
as
possible
this
does
make
intuitive
sense:
if
the
optimized
image
is
completely
filled
with
edges
that’s
strong
evidence
that’s
what
the
filter
itself
is
looking
for
and
is
activated
by
using
this
we
can
peek
into
the
learnt
filters
and
the
results
are
stunning:
one
important
thing
to
notice
here
is
that
convolved
images
are
still
images
the
output
of
a
small
grid
of
pixels
from
the
top
left
of
an
image
will
still
be
on
the
top
left
so
you
can
run
another
convolution
layer
on
top
of
another
such
as
the
two
on
the
left
to
extract
deeper
features
which
we
visualize
yet
however
deep
our
feature
detectors
get
without
any
further
changes
they’ll
still
be
operating
on
very
small
patches
of
the
image
no
matter
how
deep
your
detectors
are
you
can’t
detect
faces
from
a
3×3
grid
and
this
is
where
the
idea
of
the
receptive
field
comes
in
a
essential
design
choice
of
any
cnn
architecture
is
that
the
input
sizes
grow
smaller
and
smaller
from
the
start
to
the
end
of
the
network
while
the
number
of
channels
grow
deeper
this
as
mentioned
earlier
is
often
done
through
strides
or
pooling
layers
locality
determines
what
inputs
from
the
previous
layer
the
outputs
get
to
see
the
receptive
field
determines
what
area
of
the
original
input
to
the
entire
network
the
output
gets
to
see
the
idea
of
a
strided
convolution
is
that
we
only
process
slides
a
fixed
distance
apart
and
skip
the
ones
in
the
middle
from
a
different
point
of
view
we
only
keep
outputs
a
fixed
distance
apart
and
remove
the
rest[1]
we
then
apply
a
nonlinearity
to
the
output
and
per
usual
then
stack
another
new
convolution
layer
on
top
and
this
is
where
things
get
interesting
even
if
were
we
to
apply
a
kernel
of
the
same
size
3×3
having
the
same
local
area
to
the
output
of
the
strided
convolution
the
kernel
would
have
a
larger
effective
receptive
field:
this
is
because
the
output
of
the
strided
layer
still
does
represent
the
same
image
it
is
not
so
much
cropping
as
it
is
resizing
only
thing
is
that
each
single
pixel
in
the
output
is
a
representative
of
a
larger
area
of
whose
other
pixels
were
discarded
from
the
same
rough
location
from
the
original
input
so
when
the
next
layer’s
kernel
operates
on
the
output
it’s
operating
on
pixels
collected
from
a
larger
area
note:
if
you’re
familiar
with
dilated
convolutions
note
that
the
above
is
not
a
dilated
convolution
both
are
methods
of
increasing
the
receptive
field
but
dilated
convolutions
are
a
single
layer
while
this
takes
place
on
a
regular
convolution
following
a
strided
convolution
with
a
nonlinearity
inbetween
this
expansion
of
the
receptive
field
allows
the
convolution
layers
to
combine
the
low
level
features
lines
edges
into
higher
level
features
curves
textures
as
we
see
in
the
mixed3a
layer
followed
by
a
poolingstrided
layer
the
network
continues
to
create
detectors
for
even
higher
level
features
parts
patterns
as
we
see
for
mixed4a
the
repeated
reduction
in
image
size
across
the
network
results
in
by
the
5th
block
on
convolutions
input
sizes
of
just
7×7
compared
to
inputs
of
224×224
at
this
point
each
single
pixel
represents
a
grid
of
32×32
pixels
which
is
huge
compared
to
earlier
layers
where
an
activation
meant
detecting
an
edge
here
an
activation
on
the
tiny
7×7
grid
is
one
for
a
very
high
level
feature
such
as
for
birds
the
network
as
a
whole
progresses
from
a
small
number
of
filters
64
in
case
of
googlenet
detecting
low
level
features
to
a
very
large
number
of
filters1024
in
the
final
convolution
each
looking
for
an
extremely
specific
high
level
feature
followed
by
a
final
pooling
layer
which
collapses
each
7×7
grid
into
a
single
pixel
each
channel
is
a
feature
detector
with
a
receptive
field
equivalent
to
the
entire
image
compared
to
what
a
standard
feedforward
network
would
have
done
the
output
here
is
really
nothing
short
of
awe-inspiring
a
standard
feedforward
network
would
have
produced
abstract
feature
vectors
from
combinations
of
every
single
pixel
in
the
image
requiring
intractable
amounts
of
data
to
train
the
cnn
with
the
priors
imposed
on
it
starts
by
learning
very
low
level
feature
detectors
and
as
across
the
layers
as
its
receptive
field
is
expanded
learns
to
combine
those
low-level
features
into
progressively
higher
level
features
not
an
abstract
combination
of
every
single
pixel
but
rather
a
strong
visual
hierarchy
of
concepts
by
detecting
low
level
features
and
using
them
to
detect
higher
level
features
as
it
progresses
up
its
visual
hierarchy
it
is
eventually
able
to
detect
entire
visual
concepts
such
as
faces
birds
trees
etc
and
that’s
what
makes
them
such
powerful
yet
efficient
with
image
data
with
the
visual
hierarchy
cnns
build
it
is
pretty
reasonable
to
assume
that
their
vision
systems
are
similar
to
humans
and
they’re
really
great
with
real
world
images
but
they
also
fail
in
ways
that
strongly
suggest
their
vision
systems
aren’t
entirely
human-like
the
most
major
problem:
adversarial
examples[4]
examples
which
have
been
specifically
modified
to
fool
the
model
adversarial
examples
would
be
a
non-issue
if
the
only
tampered
ones
that
caused
the
models
to
fail
were
ones
that
even
humans
would
notice
the
problem
is
the
models
are
susceptible
to
attacks
by
samples
which
have
only
been
tampered
with
ever
so
slightly
and
would
clearly
not
fool
any
human
this
opens
the
door
for
models
to
silently
fail
which
can
be
pretty
dangerous
for
a
wide
range
of
applications
from
self-driving
cars
to
healthcare
robustness
against
adversarial
attacks
is
currently
a
highly
active
area
of
research
the
subject
of
many
papers
and
even
competitions
and
solutions
will
certainly
improve
cnn
architectures
to
become
safer
and
more
reliable
cnns
were
the
models
that
allowed
computer
vision
to
scale
from
simple
applications
to
powering
sophisticated
products
and
services
ranging
from
face
detection
in
your
photo
gallery
to
making
better
medical
diagnoses
they
might
be
the
key
method
in
computer
vision
going
forward
or
some
other
new
breakthrough
might
just
be
around
the
corner
regardless
one
thing
is
for
sure:
they’re
nothing
short
of
amazing
at
the
heart
of
many
present-day
innovative
applications
and
are
most
certainly
worth
deeply
understanding
hope
you
enjoyed
this
article!
if
you’d
like
to
stay
connected
you’ll
find
me
on
twitter
here
if
you
have
a
question
comments
are
welcome!
—
i
find
them
to
be
useful
to
my
own
learning
process
as
well
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
curious
programmer
tinkers
around
in
python
and
deep
learning
sharing
concepts
ideas
and
codes
""
there
is
an
ongoing
debate
about
whether
or
not
designers
should
write
code
wherever
you
fall
on
this
issue
most
people
would
agree
that
designers
should
know
about
code
this
helps
designers
understand
constraints
and
empathize
with
developers
it
also
allows
designers
to
think
outside
of
the
pixel
perfect
box
when
problem
solving
for
the
same
reasons
designers
should
know
about
machine
learning
put
simply
machine
learning
is
a
field
of
study
that
gives
computers
the
ability
to
learn
without
being
explicitly
programmed
arthur
samuel
1959
even
though
arthur
samuel
coined
the
term
over
fifty
years
ago
only
recently
have
we
seen
the
most
exciting
applications
of
machine
learning
—
digital
assistants
autonomous
driving
and
spam-free
email
all
exist
thanks
to
machine
learning
over
the
past
decade
new
algorithms
better
hardware
and
more
data
have
made
machine
learning
an
order
of
magnitude
more
effective
only
in
the
past
few
years
companies
like
google
amazon
and
apple
have
made
some
of
their
powerful
machine
learning
tools
available
to
developers
now
is
the
best
time
to
learn
about
machine
learning
and
apply
it
to
the
products
you
are
building
since
machine
learning
is
now
more
accessible
than
ever
before
designers
today
have
the
opportunity
to
think
about
how
machine
learning
can
be
applied
to
improve
their
products
designers
should
be
able
to
talk
with
software
developers
about
what
is
possible
how
to
prepare
and
what
outcomes
to
expect
below
are
a
few
example
applications
that
should
serve
as
inspiration
for
these
conversations
machine
learning
can
help
create
user-centric
products
by
personalizing
experiences
to
the
individuals
who
use
them
this
allows
us
to
improve
things
like
recommendations
search
results
notifications
and
ads
machine
learning
is
effective
at
finding
abnormal
content
credit
card
companies
use
this
to
detect
fraud
email
providers
use
this
to
detect
spam
and
social
media
companies
use
this
to
detect
things
like
hate
speech
machine
learning
has
enabled
computers
to
begin
to
understand
the
things
we
say
natural-language
processing
and
the
things
we
see
computer
vision
this
allows
siri
to
understand
siri
set
a
reminder
google
photos
to
create
albums
of
your
dog
and
facebook
to
describe
a
photo
to
those
visually
impaired
machine
learning
is
also
helpful
in
understanding
how
users
are
grouped
this
insight
can
then
be
used
to
look
at
analytics
on
a
group-by-group
basis
from
here
different
features
can
be
evaluated
across
groups
or
be
rolled
out
to
only
a
particular
group
of
users
machine
learning
allows
us
to
make
predictions
about
how
a
user
might
behave
next
knowing
this
we
can
help
prepare
for
a
user’s
next
action
for
example
if
we
can
predict
what
content
a
user
is
planning
on
viewing
we
can
preload
that
content
so
it’s
immediately
ready
when
they
want
it
depending
on
the
application
and
what
data
is
available
there
are
different
types
of
machine
learning
algorithms
to
choose
from
i’ll
briefly
cover
each
of
the
following
supervised
learning
allows
us
to
make
predictions
using
correctly
labeled
data
labeled
data
is
a
group
of
examples
that
has
informative
tags
or
outputs
for
example
photos
with
associated
hashtags
or
a
house’s
features
eq
number
of
bedrooms
location
and
its
price
by
using
supervised
learning
we
can
fit
a
line
to
the
labelled
data
that
either
splits
the
data
into
categories
or
represents
the
trend
of
the
data
using
this
line
we
are
able
to
make
predictions
on
new
data
for
example
we
can
look
at
new
photos
and
predict
hashtags
or
look
at
a
new
house’s
features
and
predict
its
price
if
the
output
we
are
trying
to
predict
is
a
list
of
tags
or
values
we
call
it
classification
if
the
output
we
are
trying
to
predict
is
a
number
we
call
it
regression
unsupervised
learning
is
helpful
when
we
have
unlabeled
data
or
we
are
not
exactly
sure
what
outputs
like
an
image’s
hashtags
or
a
house’s
price
are
meaningful
instead
we
can
identify
patterns
among
unlabeled
data
for
example
we
can
identify
related
items
on
an
e-commerce
website
or
recommend
items
to
someone
based
on
others
who
made
similar
purchases
if
the
pattern
is
a
group
we
call
it
a
cluster
if
the
pattern
is
a
rule
eq
if
this
then
that
we
call
it
an
association
reinforcement
learning
doesn’t
use
an
existing
data
set
instead
we
create
an
agent
to
collect
its
own
data
through
trial-and-error
in
an
environment
where
it
is
reinforced
with
a
reward
for
example
an
agent
can
learn
to
play
mario
by
receiving
a
positive
reward
for
collecting
coins
and
a
negative
reward
for
walking
into
a
goomba
reinforcement
learning
is
inspired
by
the
way
that
humans
learn
and
has
turned
out
to
be
an
effective
way
to
teach
computers
specifically
reinforcement
has
been
effective
at
training
computers
to
play
games
like
go
and
dota
understanding
the
problem
you
are
trying
to
solve
and
the
available
data
will
constrain
the
types
of
machine
learning
you
can
use
eq
identifying
objects
in
an
image
with
supervised
learning
requires
a
labeled
data
set
of
images
however
constraints
are
the
fruit
of
creativity
in
some
cases
you
can
set
out
to
collect
data
that
is
not
already
available
or
consider
other
approaches
even
though
machine
learning
is
a
science
it
comes
with
a
margin
of
error
it
is
important
to
consider
how
a
user’s
experience
might
be
impacted
by
this
margin
of
error
for
example
when
an
autonomous
car
fails
to
recognize
its
surroundings
people
can
get
hurt
even
though
machine
learning
has
never
been
as
accessible
as
it
is
today
it
still
requires
additional
resources
developers
and
time
to
be
integrated
into
a
product
this
makes
it
important
to
think
about
whether
the
resulting
impact
justifies
the
amount
of
resources
needed
to
implement
we
have
barely
covered
the
tip
of
the
iceberg
but
hopefully
at
this
point
you
feel
more
comfortable
thinking
about
how
machine
learning
can
be
applied
to
your
product
if
you
are
interested
in
learning
more
about
machine
learning
here
are
some
helpful
resources:
thanks
for
reading
chat
with
me
on
twitter
@samueldrozdov
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
digital
product
designer
samueldrozdovcom
curated
stories
on
user
experience
usability
and
product
design
by
@fabriciot
and
@caioab
""
data
science
interviews
certainly
aren’t
easy
i
know
this
first
hand
i’ve
participated
in
over
50
individual
interviews
and
phone
screens
while
applying
for
competitive
internships
over
the
last
calendar
year
through
this
exciting
and
somewhat
at
times
very
painful
process
i’ve
accumulated
a
plethora
of
useful
resources
that
helped
me
prepare
for
and
eventually
pass
data
science
interviews
long
story
short
i’ve
decided
to
sort
through
all
my
bookmarks
and
notes
in
order
to
deliver
a
comprehensive
list
of
data
science
resources
with
this
list
by
your
side
you
should
have
more
than
enough
effective
tools
at
your
disposal
next
time
you’re
prepping
for
a
big
interview
it’s
worth
noting
that
many
of
these
resources
are
naturally
going
to
geared
towards
entry-level
and
intern
data
science
positions
as
that’s
where
my
expertise
lies
keep
that
in
mind
and
enjoy!
here’s
some
of
the
more
general
resources
covering
data
science
as
a
whole
specifically
i
highly
recommend
checking
out
the
first
two
links
regarding
120
data
science
interview
questions
while
the
ebook
itself
is
a
couple
bucks
out
of
pocket
the
answers
themselves
are
free
on
quora
these
were
some
of
my
favorite
full-coverage
questions
to
practice
with
right
before
an
interview
even
data
scientists
cannot
escape
the
dreaded
algorithmic
coding
interview
in
my
experience
this
isn’t
the
case
100%
of
the
time
but
chances
are
you’ll
be
asked
to
work
through
something
similar
to
an
easy
or
medium
question
on
leetcode
or
hackerrank
as
far
as
language
goes
most
companies
will
let
you
use
whatever
language
you
want
personally
i
did
almost
all
of
my
algorithmic
coding
in
java
even
though
the
positions
were
targeted
at
python
and
r
programmers
if
i
had
to
recommend
one
thing
it’s
to
break
out
your
wallet
and
invest
in
cracking
the
coding
interview
it
absolutely
lives
up
to
the
hype
i
plan
to
continue
using
it
for
years
to
come
once
the
interviewer
knows
that
you
can
think-through
problems
and
code
effectively
chances
are
that
you’ll
move
onto
some
more
data
science
specific
applications
depending
on
the
interviewer
and
the
position
you
will
likely
be
able
to
choose
between
python
and
r
as
your
tool
of
choice
since
i’m
partial
to
python
my
resources
below
will
primarily
focus
on
effectively
using
pandas
and
numpy
for
data
analysis
a
data
science
interview
typically
isn’t
complete
without
checking
your
knowledge
of
sql
this
can
be
done
over
the
phone
or
through
a
live
coding
question
more
likely
the
latter
i’ve
found
that
the
difficulty
level
of
these
questions
can
vary
a
good
bit
ranging
from
being
painfully
easy
to
requiring
complex
joins
and
obscure
functions
our
good
friend
statistics
is
still
crucial
for
data
scientists
and
it’s
reflected
as
such
in
interviews
i
had
many
interviews
begin
by
seeing
if
i
can
explain
a
common
statistics
or
probability
concept
in
simple
and
concise
terms
as
positions
get
more
experienced
i
suspect
this
happens
less
and
less
as
traditional
statistical
questions
begin
to
take
the
more
practical
form
of
ab
testing
scenarios
covered
later
in
the
post
you’ll
notice
that
i’ve
compiled
a
few
more
resources
here
than
in
other
sections
this
isn’t
a
mistake
machine
learning
is
a
complex
field
that
is
a
virtual
guarantee
in
data
science
interviews
today
the
way
that
you’ll
be
tested
on
this
is
no
guarantee
however
it
may
come
up
as
a
conceptual
question
regarding
cross
validation
or
bias-variance
tradeoff
or
it
may
take
the
form
of
a
take
home
assignment
with
a
dataset
attached
i’ve
seen
both
several
times
so
you’ve
got
to
be
prepared
for
anything
specifically
check
out
the
machine
learning
flashcards
below
they’re
only
a
couple
bucks
and
were
my
by
far
my
favorite
way
to
quiz
myself
on
any
conceptual
ml
stuff
this
won’t
be
covered
in
every
single
data
science
interview
but
it’s
certainly
not
uncommon
most
interviews
will
have
atleast
one
section
solely
dedicated
to
product
thinking
which
often
lends
itself
to
ab
testing
of
some
sort
make
sure
your
familiar
with
the
concepts
and
statistical
background
necessary
in
order
to
be
prepared
when
it
comes
up
if
you
have
time
to
spare
i
took
the
free
online
course
by
udacity
and
overall
i
was
pretty
impressed
lastly
i
wanted
to
call
out
all
of
the
posts
related
to
data
science
jobs
and
interviewing
that
i
read
over
and
over
again
to
understand
not
only
how
to
prepare
but
what
to
expect
as
well
if
you
only
check
out
one
section
here
this
is
the
one
to
focus
on
this
is
the
layer
that
sits
on
top
of
all
the
technical
skills
and
application
don’t
overlook
it
i
hope
you
find
these
resources
useful
during
your
next
interview
or
job
search
i
know
i
did
truthfully
i’m
just
glad
that
i
saved
these
links
somewhere
lastly
this
post
is
part
of
an
ongoing
initiative
to
‘open-source’
my
experience
applying
and
interviewing
at
data
science
positions
so
if
you
enjoyed
this
content
then
be
sure
to
follow
me
for
more
stuff
like
this
if
you’re
interested
in
receiving
my
weekly
rundown
of
interesting
articles
and
resources
focused
on
data
science
machine
learning
and
artificial
intelligence
then
subscribe
to
self
driven
data
science
using
the
form
below!
if
you
enjoyed
this
post
feel
free
to
hit
the
clap
button
and
if
you’re
interested
in
posts
to
come
make
sure
to
follow
me
on
medium
at
the
link
below
—
i’ll
be
writing
and
shipping
every
day
this
month
as
part
of
a
30-day
challenge
this
article
was
originally
published
on
conordeweycom
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
data
scientist
""
writer
|
wwwconordeweycom
sharing
concepts
ideas
and
codes
""
information
theory
is
an
important
field
that
has
made
significant
contribution
to
deep
learning
and
ai
and
yet
is
unknown
to
many
information
theory
can
be
seen
as
a
sophisticated
amalgamation
of
basic
building
blocks
of
deep
learning:
calculus
probability
and
statistics
some
examples
of
concepts
in
ai
that
come
from
information
theory
or
related
fields:
in
the
early
20th
century
scientists
and
engineers
were
struggling
with
the
question:
how
to
quantify
the
information?
is
there
a
analytical
way
or
a
mathematical
measure
that
can
tell
us
about
the
information
content?
for
example
consider
below
two
sentences:
it
is
not
difficult
to
tell
that
the
second
sentence
gives
us
more
information
since
it
also
tells
that
bruno
is
big
and
brown
in
addition
to
being
a
dog
how
can
we
quantify
the
difference
between
two
sentences?
can
we
have
a
mathematical
measure
that
tells
us
how
much
more
information
second
sentence
have
as
compared
to
the
first?
scientists
were
struggling
with
these
questions
semantics
domain
and
form
of
data
only
added
to
the
complexity
of
the
problem
then
mathematician
and
engineer
claude
shannon
came
up
with
the
idea
of
entropy
that
changed
our
world
forever
and
marked
the
beginning
of
digital
information
age
shannon
proposed
that
the
semantic
aspects
of
data
are
irrelevant
and
nature
and
meaning
of
data
doesn’t
matter
when
it
comes
to
information
content
instead
he
quantified
information
in
terms
of
probability
distribution
and
uncertainty
shannon
also
introduced
the
term
bit
that
he
humbly
credited
to
his
colleague
john
tukey
this
revolutionary
idea
not
only
laid
the
foundation
of
information
theory
but
also
opened
new
avenues
for
progress
in
fields
like
artificial
intelligence
below
we
discuss
four
popular
widely
used
and
must
known
information
theoretic
concepts
in
deep
learning
and
data
sciences:
also
called
information
entropy
or
shannon
entropy
entropy
gives
a
measure
of
uncertainty
in
an
experiment
let’s
consider
two
experiments:
if
we
compare
the
two
experiments
in
exp
2
it
is
easier
to
predict
the
outcome
as
compared
to
exp
1
so
we
can
say
that
exp
1
is
inherently
more
uncertainunpredictable
than
exp
2
this
uncertainty
in
the
experiment
is
measured
using
entropy
therefore
if
there
is
more
inherent
uncertainty
in
the
experiment
then
it
has
higher
entropy
or
lesser
the
experiment
is
predictable
more
is
the
entropy
the
probability
distribution
of
experiment
is
used
to
calculate
the
entropy
a
deterministic
experiment
which
is
completely
predictable
say
tossing
a
coin
with
ph=1
has
entropy
zero
an
experiment
which
is
completely
random
say
rolling
fair
dice
is
least
predictable
has
maximum
uncertainty
and
has
the
highest
entropy
among
such
experiments
another
way
to
look
at
entropy
is
the
average
information
gained
when
we
observe
outcomes
of
an
random
experiment
the
information
gained
for
a
outcome
of
an
experiment
is
defined
as
a
function
of
probability
of
occurrence
of
that
outcome
more
the
rarer
is
the
outcome
more
is
the
information
gained
from
observing
it
for
example
in
an
deterministic
experiment
we
always
know
the
outcome
so
no
new
information
gained
is
here
from
observing
the
outcome
and
hence
entropy
is
zero
for
a
discrete
random
variable
x
with
possible
outcomes
states
x_1x_n
the
entropy
in
unit
of
bits
is
defined
as:
where
px_i
is
the
probability
of
i^th
outcome
of
x
cross
entropy
is
used
to
compare
two
probability
distributions
it
tells
us
how
similar
two
distributions
are
cross
entropy
between
two
probability
distributions
p
and
q
defined
over
same
set
of
outcomes
is
given
by:
mutual
information
is
a
measure
of
mutual
dependency
between
two
probability
distributions
or
random
variables
it
tells
us
how
much
information
about
one
variable
is
carried
by
the
another
variable
mutual
information
captures
dependency
between
random
variables
and
is
more
generalized
than
vanilla
correlation
coefficient
which
captures
only
the
linear
relationship
mutual
information
of
two
discrete
random
variables
x
and
y
is
defined
as:
where
pxy
is
the
joint
probability
distribution
of
x
and
y
and
px
and
py
are
the
marginal
probability
distribution
of
x
and
y
respectively
also
called
relative
entropy
kl
divergence
is
another
measure
to
find
similarities
between
two
probability
distributions
it
measures
how
much
one
distribution
diverges
from
the
other
suppose
we
have
some
data
and
true
distribution
underlying
it
is
‘p’
but
we
don’t
know
this
‘p’
so
we
choose
a
new
distribution
‘q’
to
approximate
this
data
since
‘q’
is
just
an
approximation
it
won’t
be
able
to
approximate
the
data
as
good
as
‘p’
and
some
information
loss
will
occur
this
information
loss
is
given
by
kl
divergence
kl
divergence
between
‘p’
and
‘q’
tells
us
how
much
information
we
lose
when
we
try
to
approximate
data
given
by
‘p’
with
‘q’
kl
divergence
of
a
probability
distribution
q
from
another
probability
distribution
p
is
defined
as:
kl
divergence
is
commonly
used
in
unsupervised
machine
learning
technique
variational
autoencoders
information
theory
was
originally
formulated
by
mathematician
and
electrical
engineer
claude
shannon
in
his
seminal
paper
a
mathematical
theory
of
communication
in
1948
note:
terms
experiments
random
variable
""
ai
machine
learning
deep
learning
data
science
have
been
used
loosely
above
but
have
technically
different
meanings
in
case
you
liked
the
article
do
follow
me
abhishek
parbhakar
for
more
articles
related
to
ai
philosophy
and
economics
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
finding
equilibria
among
ai
philosophy
and
economics
sharing
concepts
ideas
and
codes
""
over
the
past
8
months
i’ve
been
interviewing
at
various
companies
like
google’s
deepmind
wadhwani
institute
of
ai
microsoft
ola
fractal
analytics
and
a
few
others
primarily
for
the
roles
—
data
scientist
software
engineer
""
research
engineer
in
the
process
not
only
did
i
get
an
opportunity
to
interact
with
many
great
minds
but
also
had
a
peek
at
myself
along
with
a
sense
of
what
people
really
look
for
when
interviewing
someone
i
believe
that
if
i’d
had
this
knowledge
before
i
could
have
avoided
many
mistakes
and
have
prepared
in
a
much
better
manner
which
is
what
the
motivation
behind
this
post
is
to
be
able
to
help
someone
bag
their
dream
place
of
work
this
post
arose
from
a
discussion
with
one
of
my
juniors
on
the
lack
of
really
fulfilling
job
opportunities
offered
through
campus
placements
for
people
working
in
ai
also
when
i
was
preparing
i
noticed
people
using
a
lot
of
resources
but
as
per
my
experience
over
the
past
months
i
realised
that
one
can
do
away
with
a
few
minimal
ones
for
most
roles
in
ai
all
of
which
i’m
going
to
mention
at
the
end
of
the
post
i
begin
with
how
to
get
noticed
aka
the
interview
then
i
provide
a
list
of
companies
and
start-ups
to
apply
which
is
followed
by
how
to
ace
that
interview
based
on
whatever
experience
i’ve
had
i
add
a
section
on
what
we
should
strive
to
work
for
i
conclude
with
minimal
resources
you
need
for
preparation
note:
for
people
who
are
sitting
for
campus
placements
there
are
two
things
i’d
like
to
add
firstly
most
of
what
i’m
going
to
say
except
for
the
last
one
maybe
is
not
going
to
be
relevant
to
you
for
placements
but
and
this
is
my
second
point
as
i
mentioned
before
opportunities
on
campus
are
mostly
in
software
engineering
roles
having
no
intersection
with
ai
so
this
post
is
specifically
meant
for
people
who
want
to
work
on
solving
interesting
problems
using
ai
also
i
want
to
add
that
i
haven’t
cleared
all
of
these
interviews
but
i
guess
that’s
the
essence
of
failure
—
it’s
the
greatest
teacher!
the
things
that
i
mention
here
may
not
all
be
useful
but
these
are
things
that
i
did
and
there’s
no
way
for
me
to
know
what
might
have
ended
up
making
my
case
stronger
to
be
honest
this
step
is
the
most
important
one
what
makes
off-campus
placements
so
tough
and
exhausting
is
getting
the
recruiter
to
actually
go
through
your
profile
among
the
plethora
of
applications
that
they
get
having
a
contact
inside
the
organisation
place
a
referral
for
you
would
make
it
quite
easy
but
in
general
this
part
can
be
sub-divided
into
three
keys
steps:
a
do
the
regulatory
preparation
and
do
that
well:
so
with
regulatory
preparation
i
mean
—a
linkedin
profile
a
github
profile
a
portfolio
website
and
a
well-polished
cv
firstly
your
cv
should
be
really
neat
and
concise
follow
this
guide
by
udacity
for
cleaning
up
your
cv
—
resume
revamp
it
has
everything
that
i
intend
to
say
and
i’ve
been
using
it
as
a
reference
guide
myself
as
for
the
cv
template
some
of
the
in-built
formats
on
overleaf
are
quite
nice
i
personally
use
deedy-resume
here’s
a
preview:
as
it
can
be
seen
a
lot
of
content
can
be
fit
into
one
page
however
if
you
really
do
need
more
than
that
then
the
format
linked
above
would
not
work
directly
instead
you
can
find
a
modified
multi-page
format
of
the
same
here
the
next
most
important
thing
to
mention
is
your
github
profile
a
lot
of
people
underestimate
the
potential
of
this
just
because
unlike
linkedin
it
doesn’t
have
a
who
viewed
your
profile
option
people
do
go
through
your
github
because
that’s
the
only
way
they
have
to
validate
what
you
have
mentioned
in
your
cv
given
that
there’s
a
lot
of
noise
today
with
people
associating
all
kinds
of
buzzwords
with
their
profile
especially
for
data
science
open-source
has
a
big
role
to
play
too
with
majority
of
the
tools
implementations
of
various
algorithms
lists
of
learning
resources
all
being
open-sourced
i
discuss
the
benefits
of
getting
involved
in
open-source
and
how
one
can
start
from
scratch
in
an
earlier
post
here
the
bare
minimum
for
now
should
be:
•
create
a
github
account
if
you
don’t
already
have
one•
create
a
repository
for
each
of
the
projects
that
you
have
done•
add
documentation
with
clear
instructions
on
how
to
run
the
code•
add
documentation
for
each
file
mentioning
the
role
of
each
function
the
meaning
of
each
parameter
proper
formatting
eg
pep8
for
python
along
with
a
script
to
automate
the
previous
step
optional
moving
on
the
third
step
is
what
most
people
lack
which
is
having
a
portfolio
website
demonstrating
their
experience
and
personal
projects
making
a
portfolio
indicates
that
you
are
really
serious
about
getting
into
the
field
and
adds
a
lot
of
points
to
the
authenticity
factor
also
you
generally
have
space
constraints
on
your
cv
and
tend
to
miss
out
on
a
lot
of
details
you
can
use
your
portfolio
to
really
delve
deep
into
the
details
if
you
want
to
and
it’s
highly
recommended
to
include
some
sort
of
visualisation
or
demonstration
of
the
projectidea
it’s
really
easy
to
create
one
too
as
there
are
a
lot
of
free
platforms
with
drag-and-drop
features
making
the
process
really
painless
i
personally
use
weebly
which
is
a
widely
used
tool
it’s
better
to
have
a
reference
to
begin
with
there
are
a
lot
of
awesome
ones
out
there
but
i
referred
to
deshraj
yadav’s
personal
website
to
begin
with
making
mine:
finally
a
lot
of
recruiters
and
start-ups
have
nowadays
started
using
linkedin
as
their
go-to
platform
for
hiring
a
lot
of
good
jobs
get
posted
there
apart
from
recruiters
the
people
working
at
influential
positions
are
quite
active
there
as
well
so
if
you
can
grab
their
attention
you
have
a
good
chance
of
getting
in
too
apart
from
that
maintaining
a
clean
profile
is
necessary
for
people
to
have
the
will
to
connect
with
you
an
important
part
of
linkedin
is
their
search
tool
and
for
you
to
show
up
you
must
have
the
relevant
keywords
interspersed
over
your
profile
it
took
me
a
lot
of
iterations
and
re-evaluations
to
finally
have
a
decent
one
also
you
should
definitely
ask
people
with
or
under
whom
you’ve
worked
with
to
endorse
you
for
your
skills
and
add
a
recommendation
talking
about
their
experience
of
working
with
you
all
of
this
increases
your
chance
of
actually
getting
noticed
i’ll
again
point
towards
udacity’s
guide
for
linkedin
and
github
profiles
all
this
might
seem
like
a
lot
but
remember
that
you
don’t
need
to
do
it
in
a
single
day
or
even
a
week
or
a
month
it’s
a
process
it
never
ends
setting
up
everything
at
first
would
definitely
take
some
effort
but
once
it’s
there
and
you
keep
updating
it
regularly
as
events
around
you
keep
happening
you’ll
not
only
find
it
to
be
quite
easy
but
also
you’ll
be
able
to
talk
about
yourself
anywhere
anytime
without
having
to
explicitly
prepare
for
it
because
you
become
so
aware
about
yourself
b
stay
authentic:
i’ve
seen
a
lot
of
people
do
this
mistake
of
presenting
themselves
as
per
different
job
profiles
according
to
me
it’s
always
better
to
first
decide
what
actually
interests
you
what
would
you
be
happy
doing
and
then
search
for
relevant
opportunities
not
the
other
way
round
the
fact
that
the
demand
for
ai
talent
surpasses
the
supply
for
the
same
gives
you
this
opportunity
spending
time
on
your
regulatory
preparation
mentioned
above
would
give
you
an
all-around
perspective
on
yourself
and
help
make
this
decision
easier
also
you
won’t
need
to
prepare
answers
to
various
kinds
of
questions
that
you
get
asked
during
an
interview
most
of
them
would
come
out
naturally
as
you’d
be
talking
about
something
you
really
care
about
c
networking:
once
you’re
done
with
a
figured
out
b
networking
is
what
will
actually
help
you
get
there
if
you
don’t
talk
to
people
you
miss
out
on
hearing
about
many
opportunities
that
you
might
have
a
good
shot
at
it’s
important
to
keep
connecting
with
new
people
each
day
if
not
physically
then
on
linkedin
so
that
upon
compounding
it
after
many
days
you
have
a
large
and
strong
network
networking
is
not
messaging
people
to
place
a
referral
for
you
when
i
was
starting
off
i
did
this
mistake
way
too
often
until
i
stumbled
upon
this
excellent
article
by
mark
meloon
where
he
talks
about
the
importance
of
building
a
real
connection
with
people
by
offering
our
help
first
another
important
step
in
networking
is
to
get
your
content
out
for
example
if
you’re
good
at
something
blog
about
it
and
share
that
blog
on
facebook
and
linkedin
not
only
does
this
help
others
it
helps
you
as
well
once
you
have
a
good
enough
network
your
visibility
increases
multi-fold
you
never
know
how
one
person
from
your
network
liking
or
commenting
on
your
posts
may
help
you
reach
out
to
a
much
broader
audience
including
people
who
might
be
looking
for
someone
of
your
expertise
i’m
presenting
this
list
in
alphabetical
order
to
avoid
the
misinterpretation
of
any
specific
preference
however
i
do
place
a
*
on
the
ones
that
i’d
personally
recommend
this
recommendation
is
based
on
either
of
the
following:
mission
statement
people
personal
interaction
or
scope
of
learning
more
than
1
*
is
purely
based
on
the
2nd
and
3rd
factors
your
interview
begins
the
moment
you
have
entered
the
room
and
a
lot
of
things
can
happen
between
that
moment
and
the
time
when
you’re
asked
to
introduce
yourself
—
your
body
language
and
the
fact
that
you’re
smiling
while
greeting
them
plays
a
big
role
especially
when
you’re
interviewing
for
a
start-up
as
culture-fit
is
something
that
they
extremely
care
about
you
need
to
understand
that
as
much
as
the
interviewer
is
a
stranger
to
you
you’re
a
stranger
to
himher
too
so
they’re
probably
just
as
nervous
as
you
are
it’s
important
to
view
the
interview
as
more
of
a
conversation
between
yourself
and
the
interviewer
both
of
you
are
looking
for
a
mutual
fit
—
you
are
looking
for
an
awesome
place
to
work
at
and
the
interviewer
is
looking
for
an
awesome
person
like
you
to
work
with
so
make
sure
that
you’re
feeling
good
about
yourself
and
that
you
take
the
charge
of
making
the
initial
moments
of
your
conversation
pleasant
for
them
and
the
easiest
way
i
know
how
to
make
that
happen
is
to
smile
there
are
mostly
two
types
of
interviews
—
one
where
the
interviewer
has
come
with
come
prepared
set
of
questions
and
is
going
to
just
ask
you
just
that
irrespective
of
your
profile
and
the
second
where
the
interview
is
based
on
your
cv
i’ll
start
with
the
second
one
this
kind
of
interview
generally
begins
with
a
can
you
tell
me
a
bit
about
yourself?
at
this
point
2
things
are
a
big
no
—
talking
about
your
gpa
in
college
and
talking
about
your
projects
in
detail
an
ideal
statement
should
be
about
a
minute
or
two
long
should
give
a
good
idea
on
what
have
you
been
doing
till
now
and
it’s
not
restricted
to
academics
you
can
talk
about
your
hobbies
like
reading
books
playing
sports
meditation
etc
—
basically
anything
that
contributes
to
defining
you
the
interviewer
will
then
take
something
that
you
talk
about
here
as
a
cue
for
his
next
question
and
then
the
technical
part
of
the
interview
begins
the
motive
of
this
kind
of
interview
is
to
really
check
whether
whatever
you
have
written
on
your
cv
is
true
or
not:
there
would
be
a
lot
of
questions
on
what
could
be
done
differently
or
if
x
was
used
instead
of
y
what
would
have
happened
at
this
point
it’s
important
to
know
the
kind
of
trade-offs
that
is
usually
made
during
implementation
for
eg
if
the
interviewer
says
that
using
a
more
complex
model
would
have
given
better
results
then
you
might
say
that
you
actually
had
less
data
to
work
with
and
that
would
have
lead
to
overfitting
in
one
of
the
interviews
i
was
given
a
case-study
to
work
on
and
it
involved
designing
algorithms
for
a
real-world
use
case
i’ve
noticed
that
once
i’ve
been
given
the
green
flag
to
talk
about
a
project
the
interviewers
really
like
it
when
i
talk
about
it
in
the
following
flow:
problem
>
1
or
2
previous
approaches
>
our
approach
>
result
>
intuition
the
other
kind
of
interview
is
really
just
to
test
your
basic
knowledge
don’t
expect
those
questions
to
be
too
hard
but
they
would
definitely
scratch
every
bit
of
the
basics
that
you
should
be
having
mainly
based
around
linear
algebra
probability
statistics
optimisation
machine
learning
andor
deep
learning
the
resources
mentioned
in
the
minimal
resources
you
need
for
preparation
section
should
suffice
but
make
sure
that
you
don’t
miss
out
one
bit
among
them
the
catch
here
is
the
amount
of
time
you
take
to
answer
those
questions
since
these
cover
the
basics
they
expect
that
you
should
be
answering
them
almost
instantly
so
do
your
preparation
accordingly
throughout
the
process
it’s
important
to
be
confident
and
honest
about
what
you
know
and
what
you
don’t
know
if
there’s
a
question
that
you’re
certain
you
have
no
idea
about
say
it
upfront
rather
than
making
aah
um
sounds
if
some
concept
is
really
important
but
you
are
struggling
with
answering
it
the
interviewer
would
generally
depending
on
how
you
did
in
the
initial
parts
be
happy
to
give
you
a
hint
or
guide
you
towards
the
right
solution
it’s
a
big
plus
if
you
manage
to
pick
their
hints
and
arrive
at
the
correct
solution
try
to
not
get
nervous
and
the
best
way
to
avoid
that
is
by
again
smiling
now
we
come
to
the
conclusion
of
the
interview
where
the
interviewer
would
ask
you
if
you
have
any
questions
for
them
it’s
really
easy
to
think
that
your
interview
is
done
and
just
say
that
you
have
nothing
to
ask
i
know
many
people
who
got
rejected
just
because
of
failing
at
this
last
question
as
i
mentioned
before
it’s
not
only
you
who
is
being
interviewed
you
are
also
looking
for
a
mutual
fit
with
the
company
itself
so
it’s
quite
obvious
that
if
you
really
want
to
join
a
place
you
must
have
many
questions
regarding
the
work
culture
there
or
what
kind
of
role
are
they
seeing
you
in
it
can
be
as
simple
as
being
curious
about
the
person
interviewing
you
there’s
always
something
to
learn
from
everything
around
you
and
you
should
make
sure
that
you
leave
the
interviewer
with
the
impression
that
you’re
truly
interested
in
being
a
part
of
their
team
a
final
question
that
i’ve
started
asking
all
my
interviewers
is
for
a
feedback
on
what
they
might
want
me
to
improve
on
this
has
helped
me
tremendously
and
i
still
remember
every
feedback
that
i’ve
gotten
which
i’ve
incorporated
into
my
daily
life
that’s
it
based
on
my
experience
if
you’re
just
honest
about
yourself
are
competent
truly
care
about
the
company
you’re
interviewing
for
and
have
the
right
mindset
you
should
have
ticked
all
the
right
boxes
and
should
be
getting
a
congratulatory
mail
soon
😄
we
live
in
an
era
full
of
opportunities
and
that
applies
to
anything
that
you
love
you
just
need
to
strive
to
become
the
best
at
it
and
you
will
find
a
way
to
monetise
it
as
gary
vaynerchuk
just
follow
him
already
says:
this
is
a
great
time
to
be
working
in
ai
and
if
you’re
truly
passionate
about
it
you
have
so
much
that
you
can
do
with
ai
you
can
empower
so
many
people
that
have
always
been
under-represented
we
keep
nagging
about
the
problems
surrounding
us
but
there’s
been
never
such
a
time
where
common
people
like
us
can
actually
do
something
about
those
problems
rather
than
just
complaining
jeffrey
hammerbacher
founder
cloudera
had
famously
said:
we
can
do
so
much
with
ai
than
we
can
ever
imagine
there
are
many
extremely
challenging
problems
out
there
which
require
incredibly
smart
people
like
you
to
put
your
head
down
on
and
solve
you
can
make
many
lives
better
time
to
let
go
of
what
is
cool
or
what
would
look
good
think
and
choose
wisely
any
data
science
interview
comprises
of
questions
mostly
of
a
subset
of
the
following
four
categories:
computer
science
math
statistics
and
machine
learning
if
you’re
not
familiar
with
the
math
behind
deep
learning
then
you
should
consider
going
over
my
last
post
for
resources
to
understand
them
however
if
you
are
comfortable
i’ve
found
that
the
chapters
2
3
and
4
of
the
deep
learning
book
are
enough
to
preparerevise
for
theoretical
questions
during
such
interviews
i’ve
been
preparing
summaries
for
a
few
chapters
which
you
can
refer
to
where
i’ve
tried
to
even
explain
a
few
concepts
that
i
found
challenging
to
understand
at
first
in
case
you
are
not
willing
to
go
through
the
entire
chapters
and
if
you’ve
already
done
a
course
on
probability
you
should
be
comfortable
answering
a
few
numerical
as
well
for
stats
covering
these
topics
should
be
enough
now
the
range
of
questions
here
can
vary
depending
on
the
type
of
position
you
are
applying
for
if
it’s
a
more
traditional
machine
learning
based
interview
where
they
want
to
check
your
basic
knowledge
in
ml
you
can
complete
any
one
of
the
following
courses:-
machine
learning
by
andrew
ng
—
cs
229-
machine
learning
course
by
caltech
professor
yaser
abu-mostafa
important
topics
are:
supervised
learning
classification
regression
svm
decision
tree
random
forests
logistic
regression
multi-layer
perceptron
parameter
estimation
bayes’
decision
rule
unsupervised
learning
k-means
clustering
gaussian
mixture
models
dimensionality
reduction
pca
now
if
you’re
applying
for
a
more
advanced
position
there’s
a
high
chance
that
you
might
be
questioned
on
deep
learning
in
that
case
you
should
be
very
comfortable
with
convolutional
neural
networks
cnns
andor
depending
upon
what
you’ve
worked
on
recurrent
neural
networks
rnns
and
their
variants
and
by
being
comfortable
you
must
know
what
is
the
fundamental
idea
behind
deep
learning
how
cnnsrnns
actually
worked
what
kind
of
architectures
have
been
proposed
and
what
has
been
the
motivation
behind
those
architectural
changes
now
there’s
no
shortcut
for
this
either
you
understand
them
or
you
put
enough
time
to
understand
them
for
cnns
the
recommended
resource
is
stanford’s
cs
231n
and
cs
224n
for
rnns
i
found
this
neural
network
class
by
hugo
larochelle
to
be
really
enlightening
too
refer
this
for
a
quick
refresher
too
udacity
coming
to
the
aid
here
too
by
now
you
should
have
figured
out
that
udacity
is
a
really
important
place
for
an
ml
practitioner
there
are
not
a
lot
of
places
working
on
reinforcement
learning
rl
in
india
and
i
too
am
not
experienced
in
rl
as
of
now
so
that’s
one
thing
to
add
to
this
post
sometime
in
the
future
getting
placed
off-campus
is
a
long
journey
of
self-realisation
i
realise
that
this
has
been
another
long
post
and
i’m
again
extremely
grateful
to
you
for
valuing
my
thoughts
i
hope
that
this
post
finds
a
way
of
being
useful
to
you
and
that
it
helped
you
in
some
way
to
prepare
for
your
next
data
science
interview
better
if
it
did
i
request
you
to
really
think
about
what
i
talk
about
in
what
we
should
strive
to
work
for
i’m
very
thankful
to
my
friends
from
iit
guwahati
for
their
helpful
feedback
especially
ameya
godbole
kothapalli
vignesh
and
prabal
jain
a
majority
of
what
i
mention
here
like
viewing
an
interview
as
a
conversation
and
seeking
feedback
from
our
interviewers
arose
from
multiple
discussions
with
prabal
who
has
been
advising
me
constantly
on
how
i
can
improve
my
interviewing
skills
this
story
is
published
in
noteworthy
where
thousands
come
every
day
to
learn
about
the
people
""
ideas
shaping
the
products
we
love
follow
our
publication
to
see
more
product
""
design
stories
featured
by
the
journal
team
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ai
fanatic
•
math
lover
•
dreamer
the
official
journal
blog
""
i
think
it
was
the
first
um
that
was
the
moment
when
i
realized
i
was
hearing
something
extraordinary:
a
computer
carrying
out
a
completely
natural
and
very
human-sounding
conversation
with
a
real
person
and
it
wasn’t
just
a
random
talk
this
conversation
had
a
purpose
a
destination:
to
make
an
appointment
at
a
hair
salon
the
entity
making
the
call
and
appointment
was
google
assistant
running
duplex
google’s
still
experimental
ai
voice
system
and
the
venue
was
google
io
google’s
yearly
developer
conference
which
this
year
focused
heavily
on
the
latest
developments
in
ai
machine-
and
deep-learning
google
ceo
sundar
pichai
explained
that
what
we
were
hearing
was
a
real
phone
call
made
to
a
hair
salon
that
didn’t
know
it
was
part
of
an
experiment
or
that
they
were
talking
to
a
computer
he
launched
duplex
by
asking
google
assistant
to
book
a
haircut
appointment
for
tuesday
morning
the
ai
did
the
rest
duplex
made
the
call
and
when
someone
at
the
salon
picked
up
the
voice
ai
started
the
conversation
with:
hi
i’m
calling
to
book
a
woman’s
hair
cut
appointment
for
a
client
um
i’m
looking
for
something
on
may
third?
when
the
attendant
asked
duplex
to
give
her
one
second
duplex
responded
with:
mmm-hmm
the
conversation
continued
as
the
salon
representative
presented
various
dates
and
times
and
the
ai
asked
about
other
options
eventually
the
ai
and
the
salon
worker
agreed
on
an
appointment
date
and
time
what
i
heard
was
so
convincing
i
had
trouble
discerning
who
was
the
salon
worker
and
who
what
was
the
duplex
ai
it
was
stunning
and
somewhat
disconcerting
i
liken
it
to
the
feeling
you’d
get
if
a
store
mannequin
suddenly
smiled
at
you
it
was
easily
the
most
remarkable
human-computer
conversation
i’d
ever
heard
and
the
closest
thing
i’ve
seen
a
voice
ai
passing
the
turing
test
which
is
the
ai
threshold
suggested
by
computer
scientist
alan
turing
in
the
1950s
turing
posited
that
by
2000
computers
would
be
able
to
fool
humans
into
thinking
they
were
conversing
with
other
humans
at
least
30%
of
the
time
he
was
right
in
2014
a
chatbot
named
eugene
goostman
successfully
impersonated
a
wise-ass
14-year
old
programmer
during
lengthy
text-based
chats
with
unsuspecting
humans
turing
however
hadn’t
necessarily
considered
voice-based
systems
and
for
obvious
reasons
talking
computers
are
somewhat
less
adept
at
fooling
humans
spend
a
few
minutes
conversing
with
your
voice
assistant
of
choice
and
you’ll
soon
discover
their
limitations
their
speech
can
be
stilted
pronunciations
off
and
response
times
can
be
slow
especially
if
they’re
trying
to
access
a
cloud-based
server
and
forget
about
conversations
most
can
handle
two
consecutive
queries
at
most
and
they
virtually
all
require
a
trigger
phrase
like
alexa
or
hey
siri
google
is
working
on
removing
unnecessary
okay
googles
in
short
back
and
forth
convos
with
the
digital
assistant
google
assistant
running
duplex
didn’t
exhibit
any
of
those
short
comings
it
sounded
like
a
young
female
assistant
carefully
scheduling
her
boss’s
haircut
in
addition
to
the
natural
cadence
google
added
speech
disfluencies
the
verbal
ticks
ums
uhs
and
mm-hmms
and
latency
or
pauses
that
naturally
occur
when
people
are
speaking
the
result
is
a
perfectly
human
voice
produced
entirely
by
a
computer
the
second
call
demonstration
where
a
male-voiced
duplex
tried
to
make
restaurant
reservations
was
even
more
remarkable
the
human
call
participant
didn’t
entirely
understand
duplex’s
verbal
requests
and
then
told
duplex
that
for
the
number
of
people
it
wanted
to
bring
to
the
restaurant
they
didn’t
need
a
reservation
duplex
handled
all
this
without
missing
a
beat
the
amazing
thing
is
that
the
assistant
can
actually
understand
the
nuances
of
conversation
said
pichai
during
the
keynote
that
ability
comes
by
way
of
neural
network
technology
and
intensive
machine
learning
for
as
accomplished
as
duplex
is
in
making
hair
appointments
and
restaurant
reservations
it
might
stumble
in
deeper
or
more
abstract
conversations
in
a
blog
post
on
duplex
development
google
engineers
explained
that
they
constrained
duplex’s
training
to
closed
domains
or
well-defined
topics
like
dinner
reservations
and
hair
appointments
this
gave
them
the
ability
to
perform
intense
exploration
of
the
topics
and
focus
training
duplex
was
guided
during
training
within
the
domain
by
experienced
operators
who
could
keep
track
of
mistakes
and
worked
with
engineers
to
improve
responses
in
short
this
means
that
while
duplex
has
your
hair
and
dining-out
options
covered
it
could
stumble
in
movie
reservations
and
negotiations
with
your
cable
provider
even
so
duplex
fooled
two
humans
i
heard
no
hesitation
or
confusion
in
the
hair
salon
call
there
was
no
indication
that
the
salon
worker
thought
something
was
amiss
she
wanted
to
help
this
young
woman
make
an
appointment
what
will
she
think
when
she
learns
she
was
duped
by
duplex?
obviously
duplex’s
conversations
were
also
short
each
lasting
less
than
a
minute
putting
them
well-short
of
the
turing
test
benchmark
i
would’ve
enjoyed
hearing
the
conversations
devolve
as
they
extended
a
few
minutes
or
more
i’m
sure
duplex
will
soon
tackle
more
domains
and
longer
conversations
and
it
will
someday
pass
the
turing
test
it’s
only
a
matter
of
time
before
duplex
is
handling
other
mundane
or
difficult
calls
for
us
like
calling
our
parents
with
our
own
voices
see
wavenet
technology
eventually
we’ll
have
our
duplex
voices
call
each
other
handling
pleasantries
and
making
plans
which
google
assistant
can
then
drop
in
our
google
calendar
but
that’s
the
future
for
now
duplex’s
performance
stands
as
a
powerful
proof
of
concept
for
our
long-imagined
future
of
conversational
ai’s
capable
of
helping
entertaining
and
engaging
with
us
it’s
the
first
major
step
on
the
path
to
the
ai
depicted
in
the
movie
her
where
joaquin
phoenix
starred
as
a
man
who
falls
in
love
with
his
chatty
voice
assistant
played
by
the
disembodied
voice
of
scarlett
johansson
so
no
duplex
didn’t
pass
the
turing
test
but
i
do
wonder
what
alan
turing
would
think
of
it
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
tech
expert
journalist
social
media
commentator
amateur
cartoonist
and
robotics
fan
""
last
year
i
published
the
article
from
ballerina
to
ai
writer
where
i
described
how
i
embraced
the
technical
part
of
ai
without
a
technical
background
but
having
love
and
passion
for
ai
i
educated
myself
and
was
able
to
build
a
neural
net
classifier
and
do
projects
in
deep
rl
recently
i’ve
become
a
participant
in
the
openai
scholarship
program
openai
is
a
non-profit
that
gathers
top
ai
researchers
to
ensure
the
safety
of
ai
to
benefit
humanity
every
week
for
the
next
three
months
i’ll
publish
blog
posts
sharing
my
story
of
transformation
from
a
person
dedicated
to
15
years
of
professional
dancing
and
then
writing
about
tech
and
ai
to
actually
conducting
ai
research
finding
your
true
calling
—
the
key
component
of
happiness
my
primary
goal
with
the
series
of
blog
posts
from
ballerina
to
ai
researcher
is
to
show
that
it’s
never
too
late
to
embrace
a
new
field
start
over
again
and
find
your
true
calling
finding
work
you
love
is
one
of
the
most
important
components
of
happiness
-
—
something
that
you
do
every
day
and
invest
your
time
in
to
grow
that
makes
you
feel
fulfilled
gives
you
energy
something
that
is
a
refuge
for
your
soul
great
things
never
come
easy
we
have
to
be
able
to
fight
to
make
great
things
happen
but
you
can’t
fight
for
something
you
don’t
believe
in
especially
if
you
don’t
feel
like
it’s
really
important
for
you
and
humanity
finding
that
thing
is
a
real
challenge
i
feel
lucky
that
i
found
my
true
passion
—
ai
to
me
the
technology
itself
and
the
ai
community
—
researchers
scientists
people
who
dedicate
their
lives
to
building
the
most
powerful
technology
of
all
time
with
the
mission
to
benefit
humanity
and
make
it
safe
for
us
—
is
a
great
source
of
energy
the
structure
of
the
blog
post
series
today
i’m
giving
an
overall
intro
of
what
i’m
going
to
cover
in
my
from
ballerina
to
ai
researcher
series
i’ll
dedicate
the
sequence
of
blog
posts
during
the
openai
scholars
program
to
several
aspects
of
ai
technology
i’ll
cover
those
areas
that
concern
me
a
lot
like
ai
and
automation
bias
in
ml
dual
use
of
ai
etc
also
the
structure
of
my
posts
will
include
some
insights
on
what
i’m
working
on
right
now
the
final
technical
project
will
be
available
by
the
end
of
august
and
will
be
open-sourced
i
feel
very
lucky
to
have
alec
radford
an
experienced
researcher
as
my
mentor
who
guides
me
in
the
nlp
and
nlu
research
area
first
week
of
my
scholarship
i’ve
dedicated
my
first
week
within
the
program
to
learning
about
the
transformer
architecture
that
performs
much
better
on
sequential
data
compared
to
rnns
lstms
the
novelty
of
the
architecture
is
its
multi-head
self-attention
mechanism
according
to
the
original
paper
experiments
with
the
transformer
on
two
machine
translation
tasks
showed
the
model
to
be
superior
in
quality
while
being
more
parallelizable
and
requiring
significantly
less
time
to
train
more
concretely
when
rnns
or
cnns
take
a
sequence
as
an
input
it
goes
through
sentences
word
by
word
which
is
a
huge
obstacle
toward
parallelization
of
the
process
takes
more
time
to
train
models
moreover
if
sequences
are
too
long
the
model
tends
to
forget
the
content
of
distant
positions
in
sequence
or
mixes
it
with
the
following
positions’
content
—
this
is
the
fundamental
problem
in
dealing
with
sequential
data
the
transformer
architecture
reduced
this
problem
thanks
to
the
multi-head
self-attention
mechanism
i
digged
into
rnn
lstm
models
to
catch
up
with
the
background
information
to
that
end
i’ve
found
andrew
ng’s
course
on
deep
learning
along
with
the
papers
extremely
useful
to
develop
insights
regarding
the
transformer
i
went
through
the
following
resources:
the
video
by
łukasz
kaiser
from
google
brain
one
of
the
model’s
creators
a
blog
post
with
very
well
elaborated
content
re:
the
model
ran
the
code
tensor2tensor
and
the
code
using
the
pytorch
framework
from
this
paper
to
feel
the
difference
between
the
tf
and
pytorch
frameworks
overall
the
goal
within
the
program
is
to
develop
deep
comprehension
of
the
nlu
research
area:
challenges
current
state
of
the
art
and
to
formulate
and
test
hypotheses
that
tackle
the
most
important
problems
of
the
field
i’ll
share
more
on
what
i’m
working
on
in
my
future
articles
meanwhile
if
you
have
questionsfeedback
please
leave
a
comment
if
you
want
to
learn
more
about
me
here
are
my
facebook
and
twitter
accounts
i’d
appreciate
your
feedback
on
my
posts
such
as
what
topics
are
most
interesting
to
you
that
i
should
consider
further
coverage
on
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
former
ballerina
turned
ai
writer
fan
of
sci-fi
astrophysics
consciousness
is
the
key
founder
of
buzzrobotcom
the
publication
aims
to
cover
practical
aspects
of
ai
technology
use
cases
along
with
interviews
with
notable
people
in
the
ai
field
""
what
are
chatbots?
why
are
they
such
a
big
opportunity?
how
do
they
work?
how
can
i
build
one?
how
can
i
meet
other
people
interested
in
chatbots?
these
are
the
questions
we’re
going
to
answer
for
you
right
now
ready?
let’s
do
this
do
you
work
in
ecommerce?
stop
reading
and
click
here
we
made
something
for
you
ps
here
is
where
i
believe
the
future
of
bots
is
headed
you
will
probably
disagree
with
me
at
first
pps
my
newest
guide
about
conversational
commerce
is
up
i
think
you’ll
find
it
super
interesting
a
chatbot
is
a
service
powered
by
rules
and
sometimes
artificial
intelligence
that
you
interact
with
via
a
chat
interface
the
service
could
be
any
number
of
things
ranging
from
functional
to
fun
and
it
could
live
in
any
major
chat
product
facebook
messenger
slack
telegram
text
messages
etc
if
you
haven’t
wrapped
your
head
around
it
yet
don’t
worry
here’s
an
example
to
help
you
visualize
a
chatbot
if
you
wanted
to
buy
shoes
from
nordstrom
online
you
would
go
to
their
website
look
around
until
you
find
the
shoes
you
wanted
and
then
you
would
purchase
them
if
nordstrom
makes
a
bot
which
i’m
sure
they
will
you
would
simply
be
able
to
message
nordstrom
on
facebook
it
would
ask
you
what
you’re
looking
for
and
you
would
simply
tell
it
instead
of
browsing
a
website
you
will
have
a
conversation
with
the
nordstrom
bot
mirroring
the
type
of
experience
you
would
get
when
you
go
into
the
retail
store
watch
this
video
from
facebook’s
recent
f8
conference
where
they
make
their
major
announcements
at
the
7:30
mark
david
marcus
the
vice
president
of
messaging
products
at
facebook
explains
what
it
looks
like
to
buy
shoes
in
a
facebook
messenger
bot
buying
shoes
isn’t
the
only
thing
chatbots
can
be
used
for
here
are
a
couple
of
other
examples:
see?
with
bots
the
possibilities
are
endless
you
can
build
anything
imaginable
and
i
encourage
you
to
do
just
that
but
why
make
a
bot?
sure
it
looks
cool
it’s
using
some
super
advanced
technology
but
why
should
someone
spend
their
time
and
energy
on
it?
it’s
a
huge
opportunity
huge
scroll
down
and
i’ll
explain
you
are
probably
wondering
why
does
anyone
care
about
chatbots?
they
look
like
simple
text
based
services
what’s
the
big
deal?
great
question
i’ll
tell
you
why
people
care
about
chatbots
it’s
because
for
the
first
time
ever
people
are
using
messenger
apps
more
than
they
are
using
social
networks
let
that
sink
in
for
a
second
people
are
using
messenger
apps
more
than
they
are
using
social
networks
so
logically
if
you
want
to
build
a
business
online
you
want
to
build
where
the
people
are
that
place
is
now
inside
messenger
apps
this
is
why
chatbots
are
such
a
big
deal
it’s
potentially
a
huge
business
opportunity
for
anyone
willing
to
jump
headfirst
and
build
something
people
want
but
how
do
these
bots
work?
how
do
they
know
how
to
talk
to
people
and
answer
questions?
isn’t
that
artificial
intelligence
and
isn’t
that
insanely
hard
to
do?
yes
you
are
correct
it
is
artificial
intelligence
but
it’s
something
that
you
can
totally
do
yourself
let
me
explain
there
are
two
types
of
chatbots
one
functions
based
on
a
set
of
rules
and
the
other
more
advanced
version
uses
machine
learning
what
does
this
mean?
chatbot
that
functions
based
on
rules:
chatbot
that
functions
using
machine
learning:
bots
are
created
with
a
purpose
a
store
will
likely
want
to
create
a
bot
that
helps
you
purchase
something
where
someone
like
comcast
might
create
a
bot
that
can
answer
customer
support
questions
you
start
to
interact
with
a
chatbot
by
sending
it
a
message
click
here
to
try
sending
a
message
to
the
cnn
chatbot
on
facebook
so
if
these
bots
use
artificial
intelligence
to
make
them
work
well
isn’t
that
really
hard
to
do?
don’t
i
need
to
be
an
expert
at
artificial
intelligence
to
be
able
to
build
something
that
has
artificial
intelligence?
short
answer?
no
you
don’t
have
to
be
an
expert
at
artificial
intelligence
to
create
an
awesome
chatbot
that
has
artificial
intelligence
just
make
sure
to
not
over
promise
on
your
application’s
abilities
if
you
can’t
make
the
product
good
with
artificial
intelligence
right
now
it
might
be
best
to
not
put
it
in
yet
however
over
the
past
decade
quite
a
bit
of
advancements
have
been
made
in
the
area
of
artificial
intelligence
so
much
in
fact
that
anyone
who
knows
how
to
code
can
incorporate
some
level
of
artificial
intelligence
into
their
products
how
do
you
build
artificial
intelligence
into
your
bot?
don’t
worry
i’ve
got
you
covered
i’ll
tell
you
how
to
do
it
in
the
next
section
of
this
post
building
a
chatbot
can
sound
daunting
but
it’s
totally
doable
you’ll
be
creating
an
artificial
intelligence
powered
chatting
machine
in
no
time
or
of
course
you
can
always
build
a
basic
chat
bot
that
doesn’t
have
a
fancy
ai
brain
and
strictly
follows
rules
you
will
need
to
figure
out
what
problem
you
are
going
to
solve
with
your
bot
choose
which
platform
your
bot
will
live
on
facebook
slack
etc
set
up
a
server
to
run
your
bot
from
and
choose
which
service
you
will
use
to
build
your
bot
here
are
a
ton
of
resources
to
get
you
started
platform
documentation:
other
resources:
don’t
want
to
build
your
own?
now
that
you’ve
got
your
chatbot
and
artificial
intelligence
resources
maybe
it’s
time
you
met
other
people
who
are
also
interested
in
chatbots
chatbots
have
been
around
for
decades
but
because
of
the
recent
advancements
in
artificial
intelligence
and
machine
learning
there
is
a
big
opportunity
for
people
to
create
bots
that
are
better
faster
and
stronger
if
you’re
reading
this
you
probably
fall
into
one
of
these
categories:
wouldn’t
it
be
awesome
if
you
had
a
place
to
meet
learn
and
share
information
with
other
people
interested
in
chatbots?
yeah
we
thought
so
too
that’s
why
i
created
a
forum
called
chatbot
news
and
it
has
quickly
become
the
largest
community
related
to
chatbots
the
members
of
the
chatbots
group
are
investors
who
manage
well
over
$2
billion
in
capital
employees
at
facebook
instagram
fitbit
nike
and
ycombinator
companies
and
hackers
from
around
the
world
we
would
love
if
you
joined
click
here
to
request
an
invite
private
chatbots
community
i
have
also
created
the
silicon
valley
chatbots
meetup
register
here
to
be
notified
when
we
schedule
our
first
event
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
ceo
of
octane
ai
founder
of
chatbots
magazine
yc
alum
forbes
30
under
30
product
at
ustream
for
4
years
sold
for
$130mil
did
digital
for
lil
wayne
chatbots
ai
nlp
facebook
messenger
slack
telegram
and
more
""
disclaimer:
i’m
not
an
expert
in
neural
networks
or
machine
learning
since
originally
writing
this
article
many
people
with
far
more
expertise
in
these
fields
than
myself
have
indicated
that
while
impressive
what
google
have
achieved
is
evolutionary
not
revolutionary
in
the
very
least
it’s
fair
to
say
that
i’m
guilty
of
anthropomorphising
in
parts
of
the
text
i’ve
left
the
article’s
content
unchanged
because
i
think
it’s
interesting
to
compare
the
gut
reaction
i
had
with
the
subsequent
comments
of
experts
in
the
field
i
strongly
encourage
readers
to
browse
the
comments
after
reading
the
article
for
some
perspectives
more
sober
and
informed
than
my
own
in
the
closing
weeks
of
2016
google
published
an
article
that
quietly
sailed
under
most
people’s
radars
which
is
a
shame
because
it
may
just
be
the
most
astonishing
article
about
machine
learning
that
i
read
last
year
don’t
feel
bad
if
you
missed
it
not
only
was
the
article
competing
with
the
pre-christmas
rush
that
most
of
us
were
navigating
—
it
was
also
tucked
away
on
google’s
research
blog
beneath
the
geektastic
headline
zero-shot
translation
with
google’s
multilingual
neural
machine
translation
system
this
doesn’t
exactly
scream
must
read
does
it?
especially
when
you’ve
got
projects
to
wind
up
gifts
to
buy
and
family
feuds
to
be
resolved
—
all
while
the
advent
calendar
relentlessly
counts
down
the
days
until
christmas
like
some
kind
of
chocolate-filled
yuletide
doomsday
clock
luckily
i’m
here
to
bring
you
up
to
speed
here’s
the
deal
up
until
september
of
last
year
google
translate
used
phrase-based
translation
it
basically
did
the
same
thing
you
and
i
do
when
we
look
up
key
words
and
phrases
in
our
lonely
planet
language
guides
it’s
effective
enough
and
blisteringly
fast
compared
to
awkwardly
thumbing
your
way
through
a
bunch
of
pages
looking
for
the
french
equivalent
of
please
bring
me
all
of
your
cheese
and
don’t
stop
until
i
fall
over
but
it
lacks
nuance
phrase-based
translation
is
a
blunt
instrument
it
does
the
job
well
enough
to
get
by
but
mapping
roughly
equivalent
words
and
phrases
without
an
understanding
of
linguistic
structures
can
only
produce
crude
results
this
approach
is
also
limited
by
the
extent
of
an
available
vocabulary
phrase-based
translation
has
no
capacity
to
make
educated
guesses
at
words
it
doesn’t
recognize
and
can’t
learn
from
new
input
all
that
changed
in
september
when
google
gave
their
translation
tool
a
new
engine:
the
google
neural
machine
translation
system
gnmt
this
new
engine
comes
fully
loaded
with
all
the
hot
2016
buzzwords
like
neural
network
and
machine
learning
the
short
version
is
that
google
translate
got
smart
it
developed
the
ability
to
learn
from
the
people
who
used
it
it
learned
how
to
make
educated
guesses
about
the
content
tone
and
meaning
of
phrases
based
on
the
context
of
other
words
and
phrases
around
them
and
—
here’s
the
bit
that
should
make
your
brain
explode
—
it
got
creative
google
translate
invented
its
own
language
to
help
it
translate
more
effectively
what’s
more
nobody
told
it
to
it
didn’t
develop
a
language
or
interlingua
as
google
call
it
because
it
was
coded
to
it
developed
a
new
language
because
the
software
determined
over
time
that
this
was
the
most
efficient
way
to
solve
the
problem
of
translation
stop
and
think
about
that
for
a
moment
let
it
sink
in
a
neural
computing
system
designed
to
translate
content
from
one
human
language
into
another
developed
its
own
internal
language
to
make
the
task
more
efficient
without
being
told
to
do
so
in
a
matter
of
weeks
i’ve
added
a
correctionretraction
of
this
paragraph
in
the
notes
to
understand
what’s
going
on
we
need
to
understand
what
zero-shot
translation
capability
is
here’s
google’s
mike
schuster
nikhil
thorat
and
melvin
johnson
from
the
original
blog
post:
here
you
can
see
an
advantage
of
google’s
new
neural
machine
over
the
old
phrase-based
approach
the
gmnt
is
able
to
learn
how
to
translate
between
two
languages
without
being
explicitly
taught
this
wouldn’t
be
possible
in
a
phrase-based
model
where
translation
is
dependent
upon
an
explicit
dictionary
to
map
words
and
phrases
between
each
pair
of
languages
being
translated
and
this
leads
the
google
engineers
onto
that
truly
astonishing
discovery
of
creation:
so
there
you
have
it
in
the
last
weeks
of
2016
as
journos
around
the
world
started
penning
their
was
this
the
worst
year
in
living
memory
thinkpieces
google
engineers
were
quietly
documenting
a
genuinely
astonishing
breakthrough
in
software
engineering
and
linguistics
i
just
thought
maybe
you’d
want
to
know
ok
to
really
understand
what’s
going
on
we
probably
need
multiple
computer
science
and
linguistics
degrees
i’m
just
barely
scraping
the
surface
here
if
you’ve
got
time
to
get
a
few
degrees
or
if
you’ve
already
got
them
please
drop
me
a
line
and
explain
it
all
me
to
slowly
update
1:
in
my
excitement
it’s
fair
to
say
that
i’ve
exaggerated
the
idea
of
this
as
an
‘intelligent’
system
—
at
least
so
far
as
we
would
think
about
human
intelligence
and
decision
making
make
sure
you
read
chris
mcdonald’s
comment
after
the
article
for
a
more
sober
perspective
update
2:
nafrondel’s
excellent
detailed
reply
is
also
a
must
read
for
an
expert
explanation
of
how
neural
networks
function
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
a
tinkerer
our
community
publishes
stories
worth
reading
on
development
design
and
data
science
""
update:
this
article
is
part
of
a
series
check
out
the
full
series:
part
1
part
2
part
3
part
4
part
5
part
6
part
7
and
part
8!
you
can
also
read
this
article
in
italiano
español
français
türkçe
русский
한국어
português
فارسی
tiếng
việt
or
普通话
in
part
1
we
said
that
machine
learning
is
using
generic
algorithms
to
tell
you
something
interesting
about
your
data
without
writing
any
code
specific
to
the
problem
you
are
solving
if
you
haven’t
already
read
part
1
read
it
now!
this
time
we
are
going
to
see
one
of
these
generic
algorithms
do
something
really
cool
—
create
video
game
levels
that
look
like
they
were
made
by
humans
we’ll
build
a
neural
network
feed
it
existing
super
mario
levels
and
watch
new
ones
pop
out!
just
like
part
1
this
guide
is
for
anyone
who
is
curious
about
machine
learning
but
has
no
idea
where
to
start
the
goal
is
be
accessible
to
anyone
—
which
means
that
there’s
a
lot
of
generalizations
and
we
skip
lots
of
details
but
who
cares?
if
this
gets
anyone
more
interested
in
ml
then
mission
accomplished
back
in
part
1
we
created
a
simple
algorithm
that
estimated
the
value
of
a
house
based
on
its
attributes
given
data
about
a
house
like
this:
we
ended
up
with
this
simple
estimation
function:
in
other
words
we
estimated
the
value
of
the
house
by
multiplying
each
of
its
attributes
by
a
weight
then
we
just
added
those
numbers
up
to
get
the
house’s
value
instead
of
using
code
let’s
represent
that
same
function
as
a
simple
diagram:
however
this
algorithm
only
works
for
simple
problems
where
the
result
has
a
linear
relationship
with
the
input
what
if
the
truth
behind
house
prices
isn’t
so
simple?
for
example
maybe
the
neighborhood
matters
a
lot
for
big
houses
and
small
houses
but
doesn’t
matter
at
all
for
medium-sized
houses
how
could
we
capture
that
kind
of
complicated
detail
in
our
model?
to
be
more
clever
we
could
run
this
algorithm
multiple
times
with
different
of
weights
that
each
capture
different
edge
cases:
now
we
have
four
different
price
estimates
let’s
combine
those
four
price
estimates
into
one
final
estimate
we’ll
run
them
through
the
same
algorithm
again
but
using
another
set
of
weights!
our
new
super
answer
combines
the
estimates
from
our
four
different
attempts
to
solve
the
problem
because
of
this
it
can
model
more
cases
than
we
could
capture
in
one
simple
model
let’s
combine
our
four
attempts
to
guess
into
one
big
diagram:
this
is
a
neural
network!
each
node
knows
how
to
take
in
a
set
of
inputs
apply
weights
to
them
and
calculate
an
output
value
by
chaining
together
lots
of
these
nodes
we
can
model
complex
functions
there’s
a
lot
that
i’m
skipping
over
to
keep
this
brief
including
feature
scaling
and
the
activation
function
but
the
most
important
part
is
that
these
basic
ideas
click:
it’s
just
like
lego!
we
can’t
model
much
with
one
single
lego
block
but
we
can
model
anything
if
we
have
enough
basic
lego
blocks
to
stick
together:
the
neural
network
we’ve
seen
always
returns
the
same
answer
when
you
give
it
the
same
inputs
it
has
no
memory
in
programming
terms
it’s
a
stateless
algorithm
in
many
cases
like
estimating
the
price
of
house
that’s
exactly
what
you
want
but
the
one
thing
this
kind
of
model
can’t
do
is
respond
to
patterns
in
data
over
time
imagine
i
handed
you
a
keyboard
and
asked
you
to
write
a
story
but
before
you
start
my
job
is
to
guess
the
very
first
letter
that
you
will
type
what
letter
should
i
guess?
i
can
use
my
knowledge
of
english
to
increase
my
odds
of
guessing
the
right
letter
for
example
you
will
probably
type
a
letter
that
is
common
at
the
beginning
of
words
if
i
looked
at
stories
you
wrote
in
the
past
i
could
narrow
it
down
further
based
on
the
words
you
usually
use
at
the
beginning
of
your
stories
once
i
had
all
that
data
i
could
use
it
to
build
a
neural
network
to
model
how
likely
it
is
that
you
would
start
with
any
given
letter
our
model
might
look
like
this:
but
let’s
make
the
problem
harder
let’s
say
i
need
to
guess
the
next
letter
you
are
going
to
type
at
any
point
in
your
story
this
is
a
much
more
interesting
problem
let’s
use
the
first
few
words
of
ernest
hemingway’s
the
sun
also
rises
as
an
example:
what
letter
is
going
to
come
next?
you
probably
guessed
’n’
—
the
word
is
probably
going
to
be
boxing
we
know
this
based
on
the
letters
we’ve
already
seen
in
the
sentence
and
our
knowledge
of
common
words
in
english
also
the
word
‘middleweight’
gives
us
an
extra
clue
that
we
are
talking
about
boxing
in
other
words
it’s
easy
to
guess
the
next
letter
if
we
take
into
account
the
sequence
of
letters
that
came
right
before
it
and
combine
that
with
our
knowledge
of
the
rules
of
english
to
solve
this
problem
with
a
neural
network
we
need
to
add
state
to
our
model
each
time
we
ask
our
neural
network
for
an
answer
we
also
save
a
set
of
our
intermediate
calculations
and
re-use
them
the
next
time
as
part
of
our
input
that
way
our
model
will
adjust
its
predictions
based
on
the
input
that
it
has
seen
recently
keeping
track
of
state
in
our
model
makes
it
possible
to
not
just
predict
the
most
likely
first
letter
in
the
story
but
to
predict
the
most
likely
next
letter
given
all
previous
letters
this
is
the
basic
idea
of
a
recurrent
neural
network
we
are
updating
the
network
each
time
we
use
it
this
allows
it
to
update
its
predictions
based
on
what
it
saw
most
recently
it
can
even
model
patterns
over
time
as
long
as
we
give
it
enough
of
a
memory
predicting
the
next
letter
in
a
story
might
seem
pretty
useless
what’s
the
point?
one
cool
use
might
be
auto-predict
for
a
mobile
phone
keyboard:
but
what
if
we
took
this
idea
to
the
extreme?
what
if
we
asked
the
model
to
predict
the
next
most
likely
character
over
and
over
—
forever?
we’d
be
asking
it
to
write
a
complete
story
for
us!
we
saw
how
we
could
guess
the
next
letter
in
hemingway’s
sentence
let’s
try
generating
a
whole
story
in
the
style
of
hemingway
to
do
this
we
are
going
to
use
the
recurrent
neural
network
implementation
that
andrej
karpathy
wrote
andrej
is
a
deep-learning
researcher
at
stanford
and
he
wrote
an
excellent
introduction
to
generating
text
with
rnns
you
can
view
all
the
code
for
the
model
on
github
we’ll
create
our
model
from
the
complete
text
of
the
sun
also
rises
—
362239
characters
using
84
unique
letters
including
punctuation
uppercaselowercase
etc
this
data
set
is
actually
really
small
compared
to
typical
real-world
applications
to
generate
a
really
good
model
of
hemingway’s
style
it
would
be
much
better
to
have
at
several
times
as
much
sample
text
but
this
is
good
enough
to
play
around
with
as
an
example
as
we
just
start
to
train
the
rnn
it’s
not
very
good
at
predicting
letters
here’s
what
it
generates
after
a
100
loops
of
training:
you
can
see
that
it
has
figured
out
that
sometimes
words
have
spaces
between
them
but
that’s
about
it
after
about
1000
iterations
things
are
looking
more
promising:
the
model
has
started
to
identify
the
patterns
in
basic
sentence
structure
it’s
adding
periods
at
the
ends
of
sentences
and
even
quoting
dialog
a
few
words
are
recognizable
but
there’s
also
still
a
lot
of
nonsense
but
after
several
thousand
more
training
iterations
it
looks
pretty
good:
at
this
point
the
algorithm
has
captured
the
basic
pattern
of
hemingway’s
short
direct
dialog
a
few
sentences
even
sort
of
make
sense
compare
that
with
some
real
text
from
the
book:
even
by
only
looking
for
patterns
one
character
at
a
time
our
algorithm
has
reproduced
plausible-looking
prose
with
proper
formatting
that
is
kind
of
amazing!
we
don’t
have
to
generate
text
completely
from
scratch
either
we
can
seed
the
algorithm
by
supplying
the
first
few
letters
and
just
let
it
find
the
next
few
letters
for
fun
let’s
make
a
fake
book
cover
for
our
imaginary
book
by
generating
a
new
author
name
and
a
new
title
using
the
seed
text
of
er
he
and
the
s:
not
bad!
but
the
really
mind-blowing
part
is
that
this
algorithm
can
figure
out
patterns
in
any
sequence
of
data
it
can
easily
generate
real-looking
recipes
or
fake
obama
speeches
but
why
limit
ourselves
human
language?
we
can
apply
this
same
idea
to
any
kind
of
sequential
data
that
has
a
pattern
in
2015
nintendo
released
super
mario
makertm
for
the
wii
u
gaming
system
this
game
lets
you
draw
out
your
own
super
mario
brothers
levels
on
the
gamepad
and
then
upload
them
to
the
internet
so
you
friends
can
play
through
them
you
can
include
all
the
classic
power-ups
and
enemies
from
the
original
mario
games
in
your
levels
it’s
like
a
virtual
lego
set
for
people
who
grew
up
playing
super
mario
brothers
can
we
use
the
same
model
that
generated
fake
hemingway
text
to
generate
fake
super
mario
brothers
levels?
first
we
need
a
data
set
for
training
our
model
let’s
take
all
the
outdoor
levels
from
the
original
super
mario
brothers
game
released
in
1985:
this
game
has
32
levels
and
about
70%
of
them
have
the
same
outdoor
style
so
we’ll
stick
to
those
to
get
the
designs
for
each
level
i
took
an
original
copy
of
the
game
and
wrote
a
program
to
pull
the
level
designs
out
of
the
game’s
memory
super
mario
bros
is
a
30-year-old
game
and
there
are
lots
of
resources
online
that
help
you
figure
out
how
the
levels
were
stored
in
the
game’s
memory
extracting
level
data
from
an
old
video
game
is
a
fun
programming
exercise
that
you
should
try
sometime
here’s
the
first
level
from
the
game
which
you
probably
remember
if
you
ever
played
it:
if
we
look
closely
we
can
see
the
level
is
made
of
a
simple
grid
of
objects:
we
could
just
as
easily
represent
this
grid
as
a
sequence
of
characters
with
one
character
representing
each
object:
we’ve
replaced
each
object
in
the
level
with
a
letter:
and
so
on
using
a
different
letter
for
each
different
kind
of
object
in
the
level
i
ended
up
with
text
files
that
looked
like
this:
looking
at
the
text
file
you
can
see
that
mario
levels
don’t
really
have
much
of
a
pattern
if
you
read
them
line-by-line:
the
patterns
in
a
level
really
emerge
when
you
think
of
the
level
as
a
series
of
columns:
so
in
order
for
the
algorithm
to
find
the
patterns
in
our
data
we
need
to
feed
the
data
in
column-by-column
figuring
out
the
most
effective
representation
of
your
input
data
called
feature
selection
is
one
of
the
keys
of
using
machine
learning
algorithms
well
to
train
the
model
i
needed
to
rotate
my
text
files
by
90
degrees
this
made
sure
the
characters
were
fed
into
the
model
in
an
order
where
a
pattern
would
more
easily
show
up:
just
like
we
saw
when
creating
the
model
of
hemingway’s
prose
a
model
improves
as
we
train
it
after
a
little
training
our
model
is
generating
junk:
it
sort
of
has
an
idea
that
‘-’s
and
‘=’s
should
show
up
a
lot
but
that’s
about
it
it
hasn’t
figured
out
the
pattern
yet
after
several
thousand
iterations
it’s
starting
to
look
like
something:
the
model
has
almost
figured
out
that
each
line
should
be
the
same
length
it
has
even
started
to
figure
out
some
of
the
logic
of
mario:
the
pipes
in
mario
are
always
two
blocks
wide
and
at
least
two
blocks
high
so
the
ps
in
the
data
should
appear
in
2x2
clusters
that’s
pretty
cool!
with
a
lot
more
training
the
model
gets
to
the
point
where
it
generates
perfectly
valid
data:
let’s
sample
an
entire
level’s
worth
of
data
from
our
model
and
rotate
it
back
horizontal:
this
data
looks
great!
there
are
several
awesome
things
to
notice:
finally
let’s
take
this
level
and
recreate
it
in
super
mario
maker:
play
it
yourself!
if
you
have
super
mario
maker
you
can
play
this
level
by
bookmarking
it
online
or
by
looking
it
up
using
level
code
4ac9–0000–0157-f3c3
the
recurrent
neural
network
algorithm
we
used
to
train
our
model
is
the
same
kind
of
algorithm
used
by
real-world
companies
to
solve
hard
problems
like
speech
detection
and
language
translation
what
makes
our
model
a
‘toy’
instead
of
cutting-edge
is
that
our
model
is
generated
from
very
little
data
there
just
aren’t
enough
levels
in
the
original
super
mario
brothers
game
to
provide
enough
data
for
a
really
good
model
if
we
could
get
access
to
the
hundreds
of
thousands
of
user-created
super
mario
maker
levels
that
nintendo
has
we
could
make
an
amazing
model
but
we
can’t
—
because
nintendo
won’t
let
us
have
them
big
companies
don’t
give
away
their
data
for
free
as
machine
learning
becomes
more
important
in
more
industries
the
difference
between
a
good
program
and
a
bad
program
will
be
how
much
data
you
have
to
train
your
models
that’s
why
companies
like
google
and
facebook
need
your
data
so
badly!
for
example
google
recently
open
sourced
tensorflow
its
software
toolkit
for
building
large-scale
machine
learning
applications
it
was
a
pretty
big
deal
that
google
gave
away
such
important
capable
technology
for
free
this
is
the
same
stuff
that
powers
google
translate
but
without
google’s
massive
trove
of
data
in
every
language
you
can’t
create
a
competitor
to
google
translate
data
is
what
gives
google
its
edge
think
about
that
the
next
time
you
open
up
your
google
maps
location
history
or
facebook
location
history
and
notice
that
it
stores
every
place
you’ve
ever
been
in
machine
learning
there’s
never
a
single
way
to
solve
a
problem
you
have
limitless
options
when
deciding
how
to
pre-process
your
data
and
which
algorithms
to
use
often
combining
multiple
approaches
will
give
you
better
results
than
any
single
approach
readers
have
sent
me
links
to
other
interesting
approaches
to
generating
super
mario
levels:
if
you
liked
this
article
please
consider
signing
up
for
my
machine
learning
is
fun!
email
list
i’ll
only
email
you
when
i
have
something
new
and
awesome
to
share
it’s
the
best
way
to
find
out
when
i
write
more
articles
like
this
you
can
also
follow
me
on
twitter
at
@ageitgey
email
me
directly
or
find
me
on
linkedin
i’d
love
to
hear
from
you
if
i
can
help
you
or
your
team
with
machine
learning
now
continue
on
to
machine
learning
is
fun
part
3!
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
interested
in
computers
and
machine
learning
likes
to
write
about
it
""
a
year
and
a
half
ago
i
dropped
out
of
one
of
the
best
computer
science
programs
in
canada
i
started
creating
my
own
data
science
master’s
program
using
online
resources
i
realized
that
i
could
learn
everything
i
needed
through
edx
coursera
and
udacity
instead
and
i
could
learn
it
faster
more
efficiently
and
for
a
fraction
of
the
cost
i’m
almost
finished
now
i’ve
taken
many
data
science-related
courses
and
audited
portions
of
many
more
i
know
the
options
out
there
and
what
skills
are
needed
for
learners
preparing
for
a
data
analyst
or
data
scientist
role
so
i
started
creating
a
review-driven
guide
that
recommends
the
best
courses
for
each
subject
within
data
science
for
the
first
guide
in
the
series
i
recommended
a
few
coding
classes
for
the
beginner
data
scientist
then
it
was
statistics
and
probability
classes
then
introductions
to
data
science
also
data
visualization
for
this
guide
i
spent
a
dozen
hours
trying
to
identify
every
online
machine
learning
course
offered
as
of
may
2017
extracting
key
bits
of
information
from
their
syllabi
and
reviews
and
compiling
their
ratings
my
end
goal
was
to
identify
the
three
best
courses
available
and
present
them
to
you
below
for
this
task
i
turned
to
none
other
than
the
open
source
class
central
community
and
its
database
of
thousands
of
course
ratings
and
reviews
since
2011
class
central
founder
dhawal
shah
has
kept
a
closer
eye
on
online
courses
than
arguably
anyone
else
in
the
world
dhawal
personally
helped
me
assemble
this
list
of
resources
each
course
must
fit
three
criteria:
we
believe
we
covered
every
notable
course
that
fits
the
above
criteria
since
there
are
seemingly
hundreds
of
courses
on
udemy
we
chose
to
consider
the
most-reviewed
and
highest-rated
ones
only
there’s
always
a
chance
that
we
missed
something
though
so
please
let
us
know
in
the
comments
section
if
we
left
a
good
course
out
we
compiled
average
ratings
and
number
of
reviews
from
class
central
and
other
review
sites
to
calculate
a
weighted
average
rating
for
each
course
we
read
text
reviews
and
used
this
feedback
to
supplement
the
numerical
ratings
we
made
subjective
syllabus
judgment
calls
based
on
three
factors:
a
popular
definition
originates
from
arthur
samuel
in
1959:
machine
learning
is
a
subfield
of
computer
science
that
gives
computers
the
ability
to
learn
without
being
explicitly
programmed
in
practice
this
means
developing
computer
programs
that
can
make
predictions
based
on
data
just
as
humans
can
learn
from
experience
so
can
computers
where
data
=
experience
a
machine
learning
workflow
is
the
process
required
for
carrying
out
a
machine
learning
project
though
individual
projects
can
differ
most
workflows
share
several
common
tasks:
problem
evaluation
data
exploration
data
preprocessing
model
trainingtestingdeployment
etc
below
you’ll
find
helpful
visualization
of
these
core
steps:
the
ideal
course
introduces
the
entire
process
and
provides
interactive
examples
assignments
andor
quizzes
where
students
can
perform
each
task
themselves
first
off
let’s
define
deep
learning
here
is
a
succinct
description:
as
would
be
expected
portions
of
some
of
the
machine
learning
courses
contain
deep
learning
content
i
chose
not
to
include
deep
learning-only
courses
however
if
you
are
interested
in
deep
learning
specifically
we’ve
got
you
covered
with
the
following
article:
my
top
three
recommendations
from
that
list
would
be:
several
courses
listed
below
ask
students
to
have
prior
programming
calculus
linear
algebra
and
statistics
experience
these
prerequisites
are
understandable
given
that
machine
learning
is
an
advanced
discipline
missing
a
few
subjects?
good
news!
some
of
this
experience
can
be
acquired
through
our
recommendations
in
the
first
two
articles
programming
statistics
of
this
data
science
career
guide
several
top-ranked
courses
below
also
provide
gentle
calculus
and
linear
algebra
refreshers
and
highlight
the
aspects
most
relevant
to
machine
learning
for
those
less
familiar
stanford
university’s
machine
learning
on
coursera
is
the
clear
current
winner
in
terms
of
ratings
reviews
and
syllabus
fit
taught
by
the
famous
andrew
ng
google
brain
founder
and
former
chief
scientist
at
baidu
this
was
the
class
that
sparked
the
founding
of
coursera
it
has
a
47-star
weighted
average
rating
over
422
reviews
released
in
2011
it
covers
all
aspects
of
the
machine
learning
workflow
though
it
has
a
smaller
scope
than
the
original
stanford
class
upon
which
it
is
based
it
still
manages
to
cover
a
large
number
of
techniques
and
algorithms
the
estimated
timeline
is
eleven
weeks
with
two
weeks
dedicated
to
neural
networks
and
deep
learning
free
and
paid
options
are
available
ng
is
a
dynamic
yet
gentle
instructor
with
a
palpable
experience
he
inspires
confidence
especially
when
sharing
practical
implementation
tips
and
warnings
about
common
pitfalls
a
linear
algebra
refresher
is
provided
and
ng
highlights
the
aspects
of
calculus
most
relevant
to
machine
learning
evaluation
is
automatic
and
is
done
via
multiple
choice
quizzes
that
follow
each
lesson
and
programming
assignments
the
assignments
there
are
eight
of
them
can
be
completed
in
matlab
or
octave
which
is
an
open-source
version
of
matlab
ng
explains
his
language
choice:
though
python
and
r
are
likely
more
compelling
choices
in
2017
with
the
increased
popularity
of
those
languages
reviewers
note
that
that
shouldn’t
stop
you
from
taking
the
course
a
few
prominent
reviewers
noted
the
following:
columbia
university’s
machine
learning
is
a
relatively
new
offering
that
is
part
of
their
artificial
intelligence
micromasters
on
edx
though
it
is
newer
and
doesn’t
have
a
large
number
of
reviews
the
ones
that
it
does
have
are
exceptionally
strong
professor
john
paisley
is
noted
as
brilliant
clear
and
clever
it
has
a
48-star
weighted
average
rating
over
10
reviews
the
course
also
covers
all
aspects
of
the
machine
learning
workflow
and
more
algorithms
than
the
above
stanford
offering
columbia’s
is
a
more
advanced
introduction
with
reviewers
noting
that
students
should
be
comfortable
with
the
recommended
prerequisites
calculus
linear
algebra
statistics
probability
and
coding
quizzes
11
programming
assignments
4
and
a
final
exam
are
the
modes
of
evaluation
students
can
use
either
python
octave
or
matlab
to
complete
the
assignments
the
course’s
total
estimated
timeline
is
eight
to
ten
hours
per
week
over
twelve
weeks
it
is
free
with
a
verified
certificate
available
for
purchase
below
are
a
few
of
the
aforementioned
sparkling
reviews:
machine
learning
a-ztm
on
udemy
is
an
impressively
detailed
offering
that
provides
instruction
in
both
python
and
r
which
is
rare
and
can’t
be
said
for
any
of
the
other
top
courses
it
has
a
45-star
weighted
average
rating
over
8119
reviews
which
makes
it
the
most
reviewed
course
of
the
ones
considered
it
covers
the
entire
machine
learning
workflow
and
an
almost
ridiculous
in
a
good
way
number
of
algorithms
through
405
hours
of
on-demand
video
the
course
takes
a
more
applied
approach
and
is
lighter
math-wise
than
the
above
two
courses
each
section
starts
with
an
intuition
video
from
eremenko
that
summarizes
the
underlying
theory
of
the
concept
being
taught
de
ponteves
then
walks
through
implementation
with
separate
videos
for
both
python
and
r
as
a
bonus
the
course
includes
python
and
r
code
templates
for
students
to
download
and
use
on
their
own
projects
there
are
quizzes
and
homework
challenges
though
these
aren’t
the
strong
points
of
the
course
eremenko
and
the
superdatascience
team
are
revered
for
their
ability
to
make
the
complex
simple
also
the
prerequisites
listed
are
just
some
high
school
mathematics
so
this
course
might
be
a
better
option
for
those
daunted
by
the
stanford
and
columbia
offerings
a
few
prominent
reviewers
noted
the
following:
our
#1
pick
had
a
weighted
average
rating
of
47
out
of
5
stars
over
422
reviews
let’s
look
at
the
other
alternatives
sorted
by
descending
rating
a
reminder
that
deep
learning-only
courses
are
not
included
in
this
guide
—
you
can
find
those
here
the
analytics
edge
massachusetts
institute
of
technologyedx:
more
focused
on
analytics
in
general
though
it
does
cover
several
machine
learning
topics
uses
r
strong
narrative
that
leverages
familiar
real-world
examples
challenging
ten
to
fifteen
hours
per
week
over
twelve
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
49-star
weighted
average
rating
over
214
reviews
python
for
data
science
and
machine
learning
bootcamp
jose
portillaudemy:
has
large
chunks
of
machine
learning
content
but
covers
the
whole
data
science
process
more
of
a
very
detailed
intro
to
python
amazing
course
though
not
ideal
for
the
scope
of
this
guide
215
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
46-star
weighted
average
rating
over
3316
reviews
data
science
and
machine
learning
bootcamp
with
r
jose
portillaudemy:
the
comments
for
portilla’s
above
course
apply
here
as
well
except
for
r
175
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
46-star
weighted
average
rating
over
1317
reviews
machine
learning
series
lazy
programmer
incudemy:
taught
by
a
data
scientistbig
data
engineerfull
stack
software
engineer
with
an
impressive
resume
lazy
programmer
currently
has
a
series
of
16
machine
learning-focused
courses
on
udemy
in
total
the
courses
have
5000
ratings
and
almost
all
of
them
have
46
stars
a
useful
course
ordering
is
provided
in
each
individual
course’s
description
uses
python
cost
varies
depending
on
udemy
discounts
which
are
frequent
machine
learning
georgia
techudacity:
a
compilation
of
what
was
three
separate
courses:
supervised
unsupervised
and
reinforcement
learning
part
of
udacity’s
machine
learning
engineer
nanodegree
and
georgia
tech’s
online
master’s
degree
oms
bite-sized
videos
as
is
udacity’s
style
friendly
professors
estimated
timeline
of
four
months
free
it
has
a
456-star
weighted
average
rating
over
9
reviews
implementing
predictive
analytics
with
spark
in
azure
hdinsight
microsoftedx:
introduces
the
core
concepts
of
machine
learning
and
a
variety
of
algorithms
leverages
several
big
data-friendly
tools
including
apache
spark
scala
and
hadoop
uses
both
python
and
r
four
hours
per
week
over
six
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
45-star
weighted
average
rating
over
6
reviews
data
science
and
machine
learning
with
python
—
hands
on!
frank
kaneudemy:
uses
python
kane
has
nine
years
of
experience
at
amazon
and
imdb
nine
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
45-star
weighted
average
rating
over
4139
reviews
scala
and
spark
for
big
data
and
machine
learning
jose
portillaudemy:
big
data
focus
specifically
on
implementation
in
scala
and
spark
ten
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
45-star
weighted
average
rating
over
607
reviews
machine
learning
engineer
nanodegree
udacity:
udacity’s
flagship
machine
learning
program
which
features
a
best-in-class
project
review
system
and
career
support
the
program
is
a
compilation
of
several
individual
udacity
courses
which
are
free
co-created
by
kaggle
estimated
timeline
of
six
months
currently
costs
$199
usd
per
month
with
a
50%
tuition
refund
available
for
those
who
graduate
within
12
months
it
has
a
45-star
weighted
average
rating
over
2
reviews
learning
from
data
introductory
machine
learning
california
institute
of
technologyedx:
enrollment
is
currently
closed
on
edx
but
is
also
available
via
caltech’s
independent
platform
see
below
it
has
a
449-star
weighted
average
rating
over
42
reviews
learning
from
data
introductory
machine
learning
yaser
abu-mostafacalifornia
institute
of
technology:
a
real
caltech
course
not
a
watered-down
version
reviews
note
it
is
excellent
for
understanding
machine
learning
theory
the
professor
yaser
abu-mostafa
is
popular
among
students
and
also
wrote
the
textbook
upon
which
this
course
is
based
videos
are
taped
lectures
with
lectures
slides
picture-in-picture
uploaded
to
youtube
homework
assignments
are
pdf
files
the
course
experience
for
online
students
isn’t
as
polished
as
the
top
three
recommendations
it
has
a
443-star
weighted
average
rating
over
7
reviews
mining
massive
datasets
stanford
university:
machine
learning
with
a
focus
on
big
data
introduces
modern
distributed
file
systems
and
mapreduce
ten
hours
per
week
over
seven
weeks
free
it
has
a
44-star
weighted
average
rating
over
30
reviews
aws
machine
learning:
a
complete
guide
with
python
chandra
lingamudemy:
a
unique
focus
on
cloud-based
machine
learning
and
specifically
amazon
web
services
uses
python
nine
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
44-star
weighted
average
rating
over
62
reviews
introduction
to
machine
learning
""
face
detection
in
python
holczer
balazsudemy:
uses
python
eight
hours
of
on-demand
video
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
44-star
weighted
average
rating
over
162
reviews
statlearning:
statistical
learning
stanford
university:
based
on
the
excellent
textbook
an
introduction
to
statistical
learning
with
applications
in
r
and
taught
by
the
professors
who
wrote
it
reviewers
note
that
the
mooc
isn’t
as
good
as
the
book
citing
thin
exercises
and
mediocre
videos
five
hours
per
week
over
nine
weeks
free
it
has
a
435-star
weighted
average
rating
over
84
reviews
machine
learning
specialization
university
of
washingtoncoursera:
great
courses
but
last
two
classes
including
the
capstone
project
were
canceled
reviewers
note
that
this
series
is
more
digestable
read:
easier
for
those
without
strong
technical
backgrounds
than
other
top
machine
learning
courses
eg
stanford’s
or
caltech’s
be
aware
that
the
series
is
incomplete
with
recommender
systems
deep
learning
and
a
summary
missing
free
and
paid
options
available
it
has
a
431-star
weighted
average
rating
over
80
reviews
from
0
to
1:
machine
learning
nlp
""
python-cut
to
the
chase
loony
cornudemy:
a
down-to-earth
shy
but
confident
take
on
machine
learning
techniques
taught
by
four-person
team
with
decades
of
industry
experience
together
uses
python
cost
varies
depending
on
udemy
discounts
which
are
frequent
it
has
a
42-star
weighted
average
rating
over
494
reviews
principles
of
machine
learning
microsoftedx:
uses
r
python
and
microsoft
azure
machine
learning
part
of
the
microsoft
professional
program
certificate
in
data
science
three
to
four
hours
per
week
over
six
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
409-star
weighted
average
rating
over
11
reviews
big
data:
statistical
inference
and
machine
learning
queensland
university
of
technologyfuturelearn:
a
nice
brief
exploratory
machine
learning
course
with
a
focus
on
big
data
covers
a
few
tools
like
r
h2o
flow
and
weka
only
three
weeks
in
duration
at
a
recommended
two
hours
per
week
but
one
reviewer
noted
that
six
hours
per
week
would
be
more
appropriate
free
and
paid
options
available
it
has
a
4-star
weighted
average
rating
over
4
reviews
genomic
data
science
and
clustering
bioinformatics
v
university
of
california
san
diegocoursera:
for
those
interested
in
the
intersection
of
computer
science
and
biology
and
how
it
represents
an
important
frontier
in
modern
science
focuses
on
clustering
and
dimensionality
reduction
part
of
ucsd’s
bioinformatics
specialization
free
and
paid
options
available
it
has
a
4-star
weighted
average
rating
over
3
reviews
intro
to
machine
learning
udacity:
prioritizes
topic
breadth
and
practical
tools
in
python
over
depth
and
theory
the
instructors
sebastian
thrun
and
katie
malone
make
this
class
so
fun
consists
of
bite-sized
videos
and
quizzes
followed
by
a
mini-project
for
each
lesson
currently
part
of
udacity’s
data
analyst
nanodegree
estimated
timeline
of
ten
weeks
free
it
has
a
395-star
weighted
average
rating
over
19
reviews
machine
learning
for
data
analysis
wesleyan
universitycoursera:
a
brief
intro
machine
learning
and
a
few
select
algorithms
covers
decision
trees
random
forests
lasso
regression
and
k-means
clustering
part
of
wesleyan’s
data
analysis
and
interpretation
specialization
estimated
timeline
of
four
weeks
free
and
paid
options
available
it
has
a
36-star
weighted
average
rating
over
5
reviews
programming
with
python
for
data
science
microsoftedx:
produced
by
microsoft
in
partnership
with
coding
dojo
uses
python
eight
hours
per
week
over
six
weeks
free
and
paid
options
available
it
has
a
346-star
weighted
average
rating
over
37
reviews
machine
learning
for
trading
georgia
techudacity:
focuses
on
applying
probabilistic
machine
learning
approaches
to
trading
decisions
uses
python
part
of
udacity’s
machine
learning
engineer
nanodegree
and
georgia
tech’s
online
master’s
degree
oms
estimated
timeline
of
four
months
free
it
has
a
329-star
weighted
average
rating
over
14
reviews
practical
machine
learning
johns
hopkins
universitycoursera:
a
brief
practical
introduction
to
a
number
of
machine
learning
algorithms
several
onetwo-star
reviews
expressing
a
variety
of
concerns
part
of
jhu’s
data
science
specialization
four
to
nine
hours
per
week
over
four
weeks
free
and
paid
options
available
it
has
a
311-star
weighted
average
rating
over
37
reviews
machine
learning
for
data
science
and
analytics
columbia
universityedx:
introduces
a
wide
range
of
machine
learning
topics
some
passionate
negative
reviews
with
concerns
including
content
choices
a
lack
of
programming
assignments
and
uninspiring
presentation
seven
to
ten
hours
per
week
over
five
weeks
free
with
a
verified
certificate
available
for
purchase
it
has
a
274-star
weighted
average
rating
over
36
reviews
recommender
systems
specialization
university
of
minnesotacoursera:
strong
focus
one
specific
type
of
machine
learning
—
recommender
systems
a
four
course
specialization
plus
a
capstone
project
which
is
a
case
study
taught
using
lenskit
an
open-source
toolkit
for
recommender
systems
free
and
paid
options
available
it
has
a
2-star
weighted
average
rating
over
2
reviews
machine
learning
with
big
data
university
of
california
san
diegocoursera:
terrible
reviews
that
highlight
poor
instruction
and
evaluation
some
noted
it
took
them
mere
hours
to
complete
the
whole
course
part
of
ucsd’s
big
data
specialization
free
and
paid
options
available
it
has
a
186-star
weighted
average
rating
over
14
reviews
practical
predictive
analytics:
models
and
methods
university
of
washingtoncoursera:
a
brief
intro
to
core
machine
learning
concepts
one
reviewer
noted
that
there
was
a
lack
of
quizzes
and
that
the
assignments
were
not
challenging
part
of
uw’s
data
science
at
scale
specialization
six
to
eight
hours
per
week
over
four
weeks
free
and
paid
options
available
it
has
a
175-star
weighted
average
rating
over
4
reviews
the
following
courses
had
one
or
no
reviews
as
of
may
2017
machine
learning
for
musicians
and
artists
goldsmiths
university
of
londonkadenze:
unique
students
learn
algorithms
software
tools
and
machine
learning
best
practices
to
make
sense
of
human
gesture
musical
audio
and
other
real-time
data
seven
sessions
in
length
audit
free
and
premium
$10
usd
per
month
options
available
it
has
one
5-star
review
applied
machine
learning
in
python
university
of
michigancoursera:
taught
using
python
and
the
scikit
learn
toolkit
part
of
the
applied
data
science
with
python
specialization
scheduled
to
start
may
29th
free
and
paid
options
available
applied
machine
learning
microsoftedx:
taught
using
various
tools
including
python
r
and
microsoft
azure
machine
learning
note:
microsoft
produces
the
course
includes
hands-on
labs
to
reinforce
the
lecture
content
three
to
four
hours
per
week
over
six
weeks
free
with
a
verified
certificate
available
for
purchase
machine
learning
with
python
big
data
university:
taught
using
python
targeted
towards
beginners
estimated
completion
time
of
four
hours
big
data
university
is
affiliated
with
ibm
free
machine
learning
with
apache
systemml
big
data
university:
taught
using
apache
systemml
which
is
a
declarative
style
language
designed
for
large-scale
machine
learning
estimated
completion
time
of
eight
hours
big
data
university
is
affiliated
with
ibm
free
machine
learning
for
data
science
university
of
california
san
diegoedx:
doesn’t
launch
until
january
2018
programming
examples
and
assignments
are
in
python
using
jupyter
notebooks
eight
hours
per
week
over
ten
weeks
free
with
a
verified
certificate
available
for
purchase
introduction
to
analytics
modeling
georgia
techedx:
the
course
advertises
r
as
its
primary
programming
tool
five
to
ten
hours
per
week
over
ten
weeks
free
with
a
verified
certificate
available
for
purchase
predictive
analytics:
gaining
insights
from
big
data
queensland
university
of
technologyfuturelearn:
brief
overview
of
a
few
algorithms
uses
hewlett
packard
enterprise’s
vertica
analytics
platform
as
an
applied
tool
start
date
to
be
announced
two
hours
per
week
over
four
weeks
free
with
a
certificate
of
achievement
available
for
purchase
introducción
al
machine
learning
universitas
telefónicamiríada
x:
taught
in
spanish
an
introduction
to
machine
learning
that
covers
supervised
and
unsupervised
learning
a
total
of
twenty
estimated
hours
over
four
weeks
machine
learning
path
step
dataquest:
taught
in
python
using
dataquest’s
interactive
in-browser
platform
multiple
guided
projects
and
a
plus
project
where
you
build
your
own
machine
learning
system
using
your
own
data
subscription
required
the
following
six
courses
are
offered
by
datacamp
datacamp’s
hybrid
teaching
style
leverages
video
and
text-based
instruction
with
lots
of
examples
through
an
in-browser
code
editor
a
subscription
is
required
for
full
access
to
each
course
introduction
to
machine
learning
datacamp:
covers
classification
regression
and
clustering
algorithms
uses
r
fifteen
videos
and
81
exercises
with
an
estimated
timeline
of
six
hours
supervised
learning
with
scikit-learn
datacamp:
uses
python
and
scikit-learn
covers
classification
and
regression
algorithms
seventeen
videos
and
54
exercises
with
an
estimated
timeline
of
four
hours
unsupervised
learning
in
r
datacamp:
provides
a
basic
introduction
to
clustering
and
dimensionality
reduction
in
r
sixteen
videos
and
49
exercises
with
an
estimated
timeline
of
four
hours
machine
learning
toolbox
datacamp:
teaches
the
big
ideas
in
machine
learning
uses
r
24
videos
and
88
exercises
with
an
estimated
timeline
of
four
hours
machine
learning
with
the
experts:
school
budgets
datacamp:
a
case
study
from
a
machine
learning
competition
on
drivendata
involves
building
a
model
to
automatically
classify
items
in
a
school’s
budget
datacamp’s
supervised
learning
with
scikit-learn
is
a
prerequisite
fifteen
videos
and
51
exercises
with
an
estimated
timeline
of
four
hours
unsupervised
learning
in
python
datacamp:
covers
a
variety
of
unsupervised
learning
algorithms
using
python
scikit-learn
and
scipy
the
course
ends
with
students
building
a
recommender
system
to
recommend
popular
musical
artists
thirteen
videos
and
52
exercises
with
an
estimated
timeline
of
four
hours
machine
learning
tom
mitchellcarnegie
mellon
university:
carnegie
mellon’s
graduate
introductory
machine
learning
course
a
prerequisite
to
their
second
graduate
level
course
statistical
machine
learning
taped
university
lectures
with
practice
problems
homework
assignments
and
a
midterm
all
with
solutions
posted
online
a
2011
version
of
the
course
also
exists
cmu
is
one
of
the
best
graduate
schools
for
studying
machine
learning
and
has
a
whole
department
dedicated
to
ml
free
statistical
machine
learning
larry
wassermancarnegie
mellon
university:
likely
the
most
advanced
course
in
this
guide
a
follow-up
to
carnegie
mellon’s
machine
learning
course
taped
university
lectures
with
practice
problems
homework
assignments
and
a
midterm
all
with
solutions
posted
online
free
undergraduate
machine
learning
nando
de
freitasuniversity
of
british
columbia:
an
undergraduate
machine
learning
course
lectures
are
filmed
and
put
on
youtube
with
the
slides
posted
on
the
course
website
the
course
assignments
are
posted
as
well
no
solutions
though
de
freitas
is
now
a
full-time
professor
at
the
university
of
oxford
and
receives
praise
for
his
teaching
abilities
in
various
forums
graduate
version
available
see
below
machine
learning
nando
de
freitasuniversity
of
british
columbia:
a
graduate
machine
learning
course
the
comments
in
de
freitas’
undergraduate
course
above
apply
here
as
well
this
is
the
fifth
of
a
six-piece
series
that
covers
the
best
online
courses
for
launching
yourself
into
the
data
science
field
we
covered
programming
in
the
first
article
statistics
and
probability
in
the
second
article
intros
to
data
science
in
the
third
article
and
data
visualization
in
the
fourth
the
final
piece
will
be
a
summary
of
those
articles
plus
the
best
online
courses
for
other
key
topics
such
as
data
wrangling
databases
and
even
software
engineering
if
you’re
looking
for
a
complete
list
of
data
science
online
courses
you
can
find
them
on
class
central’s
data
science
and
big
data
subject
page
if
you
enjoyed
reading
this
check
out
some
of
class
central’s
other
pieces:
if
you
have
suggestions
for
courses
i
missed
let
me
know
in
the
responses!
if
you
found
this
helpful
click
the
💚
so
more
people
will
see
it
here
on
medium
this
is
a
condensed
version
of
my
original
article
published
on
class
central
where
i’ve
included
detailed
course
syllabi
from
a
quick
cheer
to
a
standing
ovation
clap
to
show
how
much
you
enjoyed
this
story
curriculum
lead
projects
@
datacamp
i
created
my
own
data
science
master’s
program
our
community
publishes
stories
worth
reading
on
development
design
and
data
science
""
