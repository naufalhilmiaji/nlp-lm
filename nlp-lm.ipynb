{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUGAS LANGUAGE MODELING NLP - SFY\n",
      "SILAKAN MASUKKAN IDENTITAS ANDA\n",
      "\n",
      "\n",
      "TUGAS 1. TAMPILKAN 5 BARIS PERTAMA DARI DATASET\n",
      "\n",
      "HASIL : \n",
      "                                                text\n",
      "0  Oh, how the headlines blared: Chatbots were Th...\n",
      "1  If you’ve ever found yourself looking up the s...\n",
      "2  Machine learning is increasingly moving from h...\n",
      "3  If your understanding of A.I. and Machine Lear...\n",
      "4  Want to learn about applied Artificial Intelli...\n",
      "\n",
      "TUGAS 2. BUAT MODEL UNIGRAM\n",
      "\n",
      "HASIL : \n",
      "             text\n",
      "0              oh\n",
      "1             how\n",
      "2             the\n",
      "3       headlines\n",
      "4         blared:\n",
      "...           ...\n",
      "190784     design\n",
      "190785        and\n",
      "190786       data\n",
      "190787    science\n",
      "190788        NaN\n",
      "\n",
      "[190789 rows x 1 columns]\n",
      "\n",
      "TUGAS 3. BUAT MODEL BIGRAM\n",
      "\n",
      "HASIL : \n",
      "              W1         W2      Prob\n",
      "1             oh        how  0.400000\n",
      "2            how        the  0.049844\n",
      "3            the  headlines  0.000332\n",
      "4      headlines    blared:  0.500000\n",
      "5        blared:   chatbots  1.000000\n",
      "...          ...        ...       ...\n",
      "79468      wayne   chatbots  1.000000\n",
      "79469   chatbots         ai  0.029412\n",
      "79470         ai        nlp  0.002558\n",
      "79471        nlp   facebook  0.024390\n",
      "79472   telegram        and  0.500000\n",
      "\n",
      "[79472 rows x 3 columns]\n",
      "\n",
      "TUGAS 4. MENAMPILKAN NEXT BEST WORD\n",
      "\n",
      "HASIL : \n",
      "Next best words of \"of\" is: \n",
      "\n",
      "(0.056534138567354474, 'the')\n",
      "---------\n",
      "Next best words of \"update\" is: \n",
      "\n",
      "(0.0004742708086317288, 'our')\n",
      "---------\n",
      "Next best words of \"hopes\" is: \n",
      "\n",
      "(0.0002376425855513308, 'were')\n",
      "---------\n",
      "SELAMAT Naufal HIlmiaji dengan NIM  1301174314  ANDA SUDAH MENYELESAIKAN TUGAS LANGUAGE MODELING NLP-SFY\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def readData(file):\n",
    "    with open(file, encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "def buildUnigramModel(text):                                \n",
    "    data = []\n",
    "    \n",
    "    content_lower = text.lower()                            #mengecilkan huruf pada teks\n",
    "    for word in content_lower.split():                      #perulangan sebanyak teks yang sudah dikecilkan dan di split menjadi token\n",
    "        cleanContent = re.sub('[\"()&+,./;“”\\\"]', '', word)  #menghilangkan punctuation\n",
    "        data.append(cleanContent)                           #menyimpan unigram ke list 'data[]'\n",
    "    \n",
    "    pd.DataFrame(data).to_csv('files/unigramModel.csv', index=False, header=False) #menyimpan model unigram ke csv agar memudahkan penghitungan\n",
    "    return data\n",
    "\n",
    "def buildBigramModel(data):                                 #fungsi bigram modelnya\n",
    "    key = []                                                #key merupakan list yang akan menyimpan kumpulan bigram\n",
    "    countBigram = {}                                        #countBigram dan countUnigram, array yang menyimpan jumlah bigram dan unigram yang ada\n",
    "    countUnigram = {}\n",
    "\n",
    "    i = 1\n",
    "    for i in range(len(data)):\n",
    "        if i < len(data) - 1:\n",
    "            key.append((data[i], data[i + 1]))              #menyimpan bigram(i, i+1) ke list key[]\n",
    "            if (data[i], data[i + 1]) in countBigram:\n",
    "                countBigram[(data[i], data[i + 1])] += 1\n",
    "            else:\n",
    "                countBigram[(data[i], data[i + 1])] = 1\n",
    "        if data[i] in countUnigram:\n",
    "            countUnigram[data[i]] += 1                       \n",
    "        else:                                              \n",
    "            countUnigram[data[i]] = 1\n",
    "\n",
    "    return key, countBigram, countUnigram\n",
    "\n",
    "def probBigram(key, countBigram, countUnigram):\n",
    "    valueBigram = {}                                                                     \n",
    "    for bigramNew in key:\n",
    "        word1 = bigramNew[0]                                                              #men-assign nilai Ci\n",
    "        valueBigram[bigramNew] = (countBigram.get(bigramNew) / (countUnigram.get(word1))) #P() = P(Ci,Ci+1)/P(Ci)\n",
    "\n",
    "    pd.Series(valueBigram).to_csv('files/bigramModel.csv', header=False)                        #menyimpan model bigram dan probabilitasnya ke csv agar memudahkan penghitungan\n",
    "\n",
    "    return valueBigram\n",
    "\n",
    "def laplaceSmoothing(key, countBigram, countUnigram):       #untuk menghindari \"Zero Probability\"\n",
    "    valueBigram = {}\n",
    "    cStar = {}\n",
    "\n",
    "    for bigram in key:\n",
    "        word1 = bigram[0]\n",
    "        valueBigram[bigram] = (countBigram.get(bigram) + 1) / (countUnigram.get(word1) + len(countUnigram))          #perhitungan P = (Ci + 1)/N + V\n",
    "        cStar[bigram] = (countBigram[bigram] + 1) * countUnigram[word1] / (countUnigram[word1] + len(countUnigram))  #cStar = (Ci + 1)*(N/N+V)\n",
    "                                                                                                                     #N merupakan countUnigram dan V merupakan vocab dari unigram\n",
    "    pd.Series(valueBigram).to_csv('files/laplaceSmoothingResult.csv', header=False)\n",
    "\n",
    "    return valueBigram, cStar\n",
    "\n",
    "def nextBestWord():\n",
    "\n",
    "    dataUnigram = pd.read_csv('files/unigramModel.csv', names=['words'])['words'].tolist()         #membaca data model unigram dari csv\n",
    "    dataBigram = pd.read_csv('files/laplaceSmoothingResult.csv', header=None).values.tolist()      #membaca data model bigram dari csv\n",
    "\n",
    "    word1 = 'of'\n",
    "    word2 = 'update'\n",
    "    word3 = 'hopes'\n",
    "    while (word1 == 'of'):\n",
    "\n",
    "        if (word1 in dataUnigram):\n",
    "            nextBest = []                                    #variable untuk menyimpan list kata selanjutnya\n",
    "\n",
    "            for row in dataBigram:\n",
    "                if word1 == row[0]:                          #ketika kata yang diinputkan sama dengan kata pada baris 1 di file csv\n",
    "                    nextBest.append((row[2], row[1]))        #memasukan nilai ke list nextBest\n",
    "            \n",
    "            nextBest.sort(reverse = True)                    #sort berdasar value tertinggi\n",
    "            \n",
    "            print('Next best words of \"' + word1 + '\" is: \\n')\n",
    "            print(nextBest[0])\n",
    "            print('---------')\n",
    "            break\n",
    "        else:\n",
    "            print('No data in dataframe.')\n",
    "    \n",
    "    while (word2 == 'update'):\n",
    "\n",
    "        if (word2 in dataUnigram):\n",
    "            nextBest = []\n",
    "\n",
    "            for row in dataBigram:\n",
    "                if word2 == row[0]:\n",
    "                    nextBest.append((row[2], row[1]))\n",
    "            \n",
    "            nextBest.sort(reverse = True)                      #sort berdasar value tertinggi\n",
    "            \n",
    "            print('Next best words of \"' + word2 + '\" is: \\n')\n",
    "            print(nextBest[0])\n",
    "            print('---------')\n",
    "            break\n",
    "        else:\n",
    "            print('No data in dataframes.')\n",
    "    \n",
    "    while (word3 == 'hopes'):\n",
    "\n",
    "        if (word3 in dataUnigram):\n",
    "            nextBest = []\n",
    "\n",
    "            for row in dataBigram:\n",
    "                if word3 == row[0]:\n",
    "                    nextBest.append((row[2], row[1]))\n",
    "            \n",
    "            nextBest.sort(reverse = True)                       #sort berdasar value tertinggi\n",
    "            \n",
    "            print('Next best words of \"' + word3 + '\" is: \\n')\n",
    "            print(nextBest[0])\n",
    "            print('---------')\n",
    "            break\n",
    "        else:\n",
    "            print('No data in dataframes.')\n",
    "\n",
    "def nextTenBestWords():\n",
    "    unigram = pd.read_csv('unigramModel.csv', names=['words'])['words'].tolist()             #membaca data dari csv untuk penghitungan\n",
    "    dataBigram = pd.read_csv('laplaceSmoothingResult.csv', header=None).values.tolist()      #membaca data dari csv untuk penghitungan\n",
    "\n",
    "    word1 = 'of'\n",
    "    word2 = 'update'\n",
    "    word3 = 'hopes'\n",
    "    while (word1 == 'of'):\n",
    "        \n",
    "        if (word1 in dataUnigram):\n",
    "            bestTen = []\n",
    "\n",
    "            for row in dataBigram:\n",
    "                if word1 == row[0]:\n",
    "                    bestTen.append((row[2], row[1]))\n",
    "\n",
    "            bestTen.sort(reverse = True)                        #sort berdasar value tertinggi\n",
    "            \n",
    "            print('Next best TEN words of \"' + word1 + '\" is: \\n')\n",
    "            print(bestTen[0:10])\n",
    "            print('---------')\n",
    "            break\n",
    "        else:\n",
    "            print('No data in dataframes.')\n",
    "    \n",
    "    while (word2 == 'update'):\n",
    "        \n",
    "        if (word2 in dataUnigram):\n",
    "            bestTen = []\n",
    "\n",
    "            for row in dataBigram:\n",
    "                if word2 == row[0]:\n",
    "                    bestTen.append((row[2], row[1]))\n",
    "\n",
    "            bestTen.sort(reverse = True)                        #sort berdasar value tertinggi\n",
    "            \n",
    "            print('Next best TEN words of \"' + word2 + '\" is: \\n')\n",
    "            print(bestTen[0:10])\n",
    "            print('---------')\n",
    "            break\n",
    "        else:\n",
    "            print('No data in dataframes.')\n",
    "    \n",
    "    while (word3 == 'hopes'):\n",
    "        \n",
    "        if (word3 in dataUnigram):\n",
    "            bestTen = []\n",
    "\n",
    "            for row in dataBigram:\n",
    "                if word3 == row[0]:\n",
    "                    bestTen.append((row[2], row[1]))\n",
    "\n",
    "            bestTen.sort(reverse = True)                        #sort berdasar value tertinggi\n",
    "            \n",
    "            print('Next best TEN words of \"' + word3 + '\" is: ')\n",
    "            print(bestTen[0:10])\n",
    "            print('---------')\n",
    "            break\n",
    "        else:\n",
    "            print('No data in dataframes.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    data = pd.read_csv(\"files/text.csv\") #khusus no. 1\n",
    "    \n",
    "    text = readData(\"files/text.csv\")    #untuk pendefinisian argumen pada functions\n",
    "    text = buildUnigramModel(text)\n",
    "    \n",
    "    key, countBigram, countUnigram = buildBigramModel(text)                                          \n",
    "    bigramProb = probBigram(key, countBigram, countUnigram)                       \n",
    "    bigramAddOne, addOneCstar = laplaceSmoothing(key,countBigram,countUnigram)\n",
    "    \n",
    "    unigramModel = pd.read_csv(\"files/unigramModel.csv\")\n",
    "    bigramModel = pd.read_csv(\"files/bigramModel.csv\", names=['W1', 'W2', 'Prob'])\n",
    "    \n",
    "    print(\"TUGAS LANGUAGE MODELING NLP - SFY\")\n",
    "    print(\"SILAKAN MASUKKAN IDENTITAS ANDA\\n\")\n",
    "    \n",
    "    Nama = 'Naufal HIlmiaji'\n",
    "    NIM = '1301174314'\n",
    "\n",
    "    os.system(\"pause\")\n",
    "    os.system(\"cls\")\n",
    "    \n",
    "    print(\"\\nTUGAS 1. TAMPILKAN 5 BARIS PERTAMA DARI DATASET\")\n",
    "    print()\n",
    "    print(\"HASIL : \")\n",
    "    print(data.head())\n",
    "\n",
    "    os.system(\"pause\")\n",
    "    os.system(\"cls\")\n",
    "\n",
    "    print(\"\\nTUGAS 2. BUAT MODEL UNIGRAM\")\n",
    "    print()\n",
    "    print(\"HASIL : \")\n",
    "    print(unigramModel)\n",
    "\n",
    "    os.system(\"pause\")\n",
    "    os.system(\"cls\")\n",
    "\n",
    "    print(\"\\nTUGAS 3. BUAT MODEL BIGRAM\")\n",
    "    print()\n",
    "    print(\"HASIL : \")\n",
    "    print(bigramModel[1:])    \n",
    "\n",
    "    os.system(\"pause\")\n",
    "    os.system(\"cls\")\n",
    "    \n",
    "    print(\"\\nTUGAS 4. MENAMPILKAN NEXT BEST WORD\")\n",
    "    print()\n",
    "    print(\"HASIL : \")\n",
    "    nextBestWord()\n",
    "\n",
    "    os.system(\"pause\")\n",
    "    os.system(\"cls\")\n",
    "\n",
    "    print(\"SELAMAT\", Nama , \"dengan NIM \", NIM, \" ANDA SUDAH MENYELESAIKAN TUGAS LANGUAGE MODELING NLP-SFY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
